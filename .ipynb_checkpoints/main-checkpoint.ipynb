{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Capstone: Iceberg Classifier\n",
    "\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Icebergs presents threats to the ships navigation and various offshore activities. Especially, it as actual problem for the area offshore to Newfoundland and Labrador known as Iceberg Alley. The primary iceberg detection method for now is aerial reconnaissance using vessel-based monitoring data. Also, data received though satellites are widely being integrated now onto the monitoring systems greatly reduce monitoring cost. Additionally, Synthetic Aperture Radar (SAR) satellites can still monitor in various weather conditions such as clouds and fog.\n",
    "However, manual visual classification of SAR images to identify iceberg is very time-consuming process. So, C‑CORE company (https://www.c-core.ca/) has developed a computer vision system that analyzes SAR data to automatically detect and classify icebergs and vessels. Now it challenges ML community to build effective classification algorithm for their detection system [1]\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of the project is to build an algorithm which can reliably classify data to identify either it is iceberg or ship, based on given Synthetic Aperture Radar data. Also, the results are clearly measurable using prediction accuracy and it is important to have classifier with higher accuracy (ideally 100%).\n",
    "Additionally, analysis and classification SAR data is interesting problem. Even if it seems like standard image classification task it has some important differences which makes it challengeable to use pre-trained neural networks with transfer learning for the image classification such as VGG [2] or Inception [3]:\n",
    "• SAR data is not a three-channels regular image\n",
    "• Radar detected shapes are different than visually detected shapes.\n",
    "• Data set has additional incidence angle parameter of which the image was taken. So, it is additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "CORE provided dataset of satellite SAR images containing either a ship or an iceberg including 1604 training samples and 8424 test samples (5000 from them are autogenerated) Data was collected from SAR which bounces a signal off an object and records the echo, then that data is translated into an image. Two channels of image are provided: HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). See [4] for more details. Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format:\n",
    "\n",
    "Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format: \n",
    "* Id of the image \n",
    "* band1 – flatten image data of the HH channel (5625 elements, 75x75 image), each element is float value measured in dB. \n",
    "* band2 – flatten image data of the HV channel (5625 elements, 75x75 image) , each element is float value measured in dB. \n",
    "* inc_angle - the incidence angle of which the image was taken \n",
    "* is_iceberg – classification label of the image. 1 is for iceberg, 0 for ship.\n",
    "\n",
    "Training dataset the only dataset which has labels assigned – so it will be used for training and validation. Test dataset does not have labels and will be used for the model evaluation.\n",
    "For the model features we are going to use band1, band2 and inc_angle data as features and is_iceberg field as labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to load data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to find global dataset characteristics (signal strength min/max, angle min/max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_signal_minmax(data):\n",
    "    mins = [min(min(item['band_1']), min(item['band_2'])) for item in data ]\n",
    "    maxes = [max(max(item['band_1']), max(item['band_2'])) for item in data ]\n",
    "\n",
    "    global_min = min(mins)\n",
    "    global_max = max(maxes)\n",
    "    \n",
    "    return global_min, global_max\n",
    "\n",
    "def find_angle_minmax(data):\n",
    "    global_min  = min( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'  ])\n",
    "    global_max = max( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'])\n",
    "  \n",
    "    return global_min, global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to extract and display 75*75 image from raw SAR JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_image(data_item, key, image_size = 75):\n",
    "    image = np.array(data_item[key])\n",
    "    image = image.reshape(image_size, image_size)\n",
    "    return image\n",
    "    \n",
    "def extract_images(data_item, image_size = 75):\n",
    "    hh_image = extract_image(data_item, 'band_1', image_size)\n",
    "    hv_image = extract_image(data_item, 'band_2', image_size)\n",
    "    return hh_image, hv_image\n",
    "     \n",
    "def display_image(image, cmap='gray'):\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image pre-processing and normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_min_max_scale_sar_image(image, global_min, global_max):\n",
    "    image = (image - global_min) / (global_max - global_min)\n",
    "    return image\n",
    "\n",
    "def local_min_max_scale_sar_image(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    \n",
    "    image = (image - img_min) / (img_max - img_min)\n",
    "    return image\n",
    "\n",
    "def local_standard_scale_sar_image(image):\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    \n",
    "    image = (image - np.ones_like(image) * img_mean) / img_std;\n",
    "    \n",
    "    return image\n",
    "\n",
    "def flatten_image(image):\n",
    "    image = image.flatten()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 2-channel Images for CNN from HH and HV SAR channels ignoring angle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_ignore_angles(data, process_func):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        hh_image, hv_image = extract_images(item)\n",
    "        hh_image = process_func(hh_image)\n",
    "        hv_image = process_func(hv_image)\n",
    "        \n",
    "        image = np.dstack((hh_image, hv_image))\n",
    "        X.append(image)\n",
    "        if 'is_iceberg' in item.keys():\n",
    "            labels.append(item['is_iceberg'])\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            \n",
    "        ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3-channel Images for CNN from HH and HV SAR channels + angle data as separate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image)\n",
    "            hv_image = process_func(hv_image)\n",
    "            angle_layer = np.ones_like(hh_image) * angle_processing(angle)\n",
    "            image = np.dstack((hh_image, hv_image, angle_layer))\n",
    "            X.append(image)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create flat features for simple classifiers algoirithms like Logistic Regression, Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_flat_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image).flatten()\n",
    "            hv_image = process_func(hv_image).flatten()\n",
    "            angle_layer = angle_processing(angle)\n",
    "            x_item = np.concatenate((hh_image, hv_image, [angle_layer]))\n",
    "            X.append(x_item)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_image_part(data, margin):\n",
    "    return data[:, margin : -margin, margin : - margin, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1604\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train.json'\n",
    "train_data = read_data(train_file)\n",
    "print('Training dataset size: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum incidence angle = 24.7546, Maximum incidence angle = 45.9375\n",
      "Minimum signal strength (dB)= -45.655499, Maximum signal strength (dB) = 34.574917\n"
     ]
    }
   ],
   "source": [
    "image_size = 75\n",
    "\n",
    "angle_min, angle_max = find_angle_minmax(train_data)\n",
    "global_min, global_max = find_signal_minmax(train_data)\n",
    "\n",
    "print(\"Minimum incidence angle = {}, Maximum incidence angle = {}\".format(angle_min, angle_max))\n",
    "print(\"Minimum signal strength (dB)= {}, Maximum signal strength (dB) = {}\".format(global_min, global_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and run image workflow to  extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    #image = cv2.bilateralFilter(image.astype(np.float32), 5, 80, 80)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "#X_train_initial, y_train_initial, _ = prepare_dataset_ignore_angles(train_data, ptocessing_lambda)\n",
    "X_train_initial, y_train_initial, _ = prepare_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "#X_train, y_train, _ = prepare_dataset_with_angle(train_data, global_min, global_max, angle_min, angle_max)\n",
    "print('Training dataset size: {}'.format(len(X_train_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_channels(data, index):\n",
    "    hh_channel = data[index, :, :, 0]\n",
    "    hv_channel = data[index, :, :, 1]\n",
    "    return hh_channel, hv_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a64b867198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnW2sXld15//LN2+QACFAEhPnxXHiOHZIDBMyoBTUATJi\nGES/IZA66lSV+FJGoOmoU/phpPlQKZ+q9sOoEqJ0isrQMrRoKlRRMS0VM4BCnImTOn5NbCc4JCS8\nQwI4tvd8uM86/j337n/uc+34ce496y9F2T73nH32XnufZ//X2muvFa01FQqF8WHD+W5AoVA4P6iP\nv1AYKerjLxRGivr4C4WRoj7+QmGkqI+/UBgp6uMvFEaKs/r4I+K9EXEgIh6NiN97qRpVKBTOPeJM\nnXwiYkHSQUn3SDom6X5JH26t7X3pmlcoFM4VLjiLZ++S9Ghr7bAkRcRfSvo1Sfbjv/TSS9sVV1wh\nSeKPDssRMZQ3bNiw7PqpU6eGaydOnBjKJ0+eHMoXXHC6W7yfYN0LCwvde44fP76sfj7HsnsP20Kw\nz66eXp2UD+twfX7hhReG8oUXXrji+4lf/vKXy65dfPHFK7aF1ylDyplt5Nj12ugWKL6H/ed1V+b7\n2c9sI+cWnyNYB9vIPrjxdzLv1ccxZLt673/++ed1/PjxfoOX4Gw+/mskfRv/PibpX77YA1dccYU+\n/vGPS5oecAqfwnrlK185lC+66CJJ0s9//vPh2rPPPjuUf/zjHw/lN7zhDUOZ9xOs+9WvfnX3nscf\nf3wo/+QnP5EkveIVrxiuXXrppUP5ueee69bx+te/vnudg8i2PP/88y9a5yWXXDKUOSnyR3VpHU89\n9dRQ3rhx41Cm/PlBc9I99thjy95/0003DWWOFevjR/6d73xnKL/qVa8ayuzbz372s6F81VVXLauT\n/ST4nte97nXd/vBD5AfHHyX2M+fCM888M1yjzImcE9L0h/jGN75xKL/2ta/tPst5xH70+sx5/vTT\nTw/lX/ziF0M559PXvva17vt6OOcGv4j4SETsiohd7gMpFArzx9ms/E9Kuhb/3jS5NoXW2iclfVKS\nrrvuupa/yj/84Q+He9wvNX9Nv/e970ma/kVkHfxV/9GPfjSUyQL4nmQSrFvyKkC2hasXV1iuJARX\neP5SX3bZZUOZfX7Na16z7H7ey1WSK9IPfvCD7nU+S7mwH46CZp+vu+664RrlRqxEYyXpySdPT4/L\nL798KDt2lO8nq6CcObacC1dffXW3Po4Xx4X354rPe8mqnJyvueaa7jspfzJFMizOi56qR/nwXsoi\ny27+9nA2K//9km6OiM0RcZGkD0n627Oor1AozBFnvPK31k5ExEcl/b2kBUmfbq098pK1rFAonFOc\nDe1Xa+3vJP3drPefPHlyMJLQEEcKRppEG0GPVtL4tHv37qFMwxbpFem9M8TQQMW2fP/73x/6kLjy\nyiu77aNq8uijjw5lR99IB0nbkm7SOETjIA1ONFqSDm7atGkos/+kl6yH9Hnr1q2SPNXndY4hja+U\nC2m624Vgu5544olldROUIceCRjFSc95PKk9kW6iWsE2cW73dEGna+Mj3c+44pOwoW44tjYk//elP\nl5XnRfsLhcIaRn38hcJIcVa0f7VorQ2UlHvEpH2koKQwaam+9trTGwxUHWiRdk4RxNGjR7vvZ7v4\n/qTASf+laUrH50gviWPHjg1l0nHSxx41JO0nBaRaQrnRUk3wWe7/s/+kz9kuto9qAamxo8DOL4G7\nGnSiOXjw4LLrlC3VOI4tVSeC6h3lT6s9xyvHnNSdOyycc5Qnx4iyYJnqGOuhjFL+Tp7sP8fCORO9\nGGrlLxRGivr4C4WRYq60PyK69ISUlXSIFDhpP6mjc1elAwsttbQCE6SgtKASSf2oXhDsF8ukdNu2\nbRvKVCnYZ1Lg7BP7w/6TahJUB0g1SfVpTaZ1nu8nNe7VTTqclnlJ2rJly1Du7V4sbRfpO+l27hTQ\n5depVBw3qlQcc44LZd6bI3TacXOLcE5j995771D+6Ec/OpSdU1rKiGoM3XsfeeT0bjrruP766yWV\ntb9QKMyA+vgLhZFi7rQ/6REtlaRJK/mck/YePnx4KNPy6pxS6CBBayrrdE4Z6aDjHFtIO50zBykz\naSfpM514ksKRfpMCU4bOaYpyISUkHab64M4fJNh/jpVzYHIgBecJOjoC9VREvpMOWdzhoGMVwTa6\nU505Ft/97neHa5wTVBHoZEaHtG9+85vd93NecG5TpnnPt799+sCsU+/4/s2bN0uaPr+yEmrlLxRG\nirmu/C+88ELX6Mb9Yv6C89c5f+X4K8lfT+e6yvtpROEvO8F7uPLliuOMc1yluJLwOtvrjjdzdc4y\nV1K2m33jdRrNuIKwHp5ko+x6Phf8+x133NFtKzELq+J+NceZ6NXvglmwbjI8GhZpcHT74jfeeKOk\naT8Qjptzr2Z9O3fuHMo0ELONZDsEv4XevWnYk6YN1S6YzIuhVv5CYaSoj79QGCnmSvtPnTo1UCXu\n0dNI4YJV9ECKSjdO0jTSRFIz0nsazkilSevToMd3ktK6k3kONFyx/z2DH9UIBsRgW+n27AJbcL+Y\nRiQ+S7Us30vaT58Ajg/7QLlQdaLawftJdTn+SXdZB9UFGvB4nWNBak64GIYpO44DXZ7ZPvaTfaPq\n6gKLcOxoxM33cq5ybrM/NDLnXJnl5GCiVv5CYaSoj79QGCnmSvsJ0hPSJBfPLekmKbWLycf6SK9p\nEWc9pHUE6W5ST6dG8J1UDVy4ap4O7O2nsx7eS9rJvvGenousNL2H7vpB63SqBi60uAtp7VygWQ8p\nq3O7TpDq0sLNfX72jaoJ5xnVJ7rUsl0pc7aPahR3DHgPTwxSjeEc4riwvT0/C1J9tzPE+Z8yWo3V\nv1b+QmGkqI+/UBgpzpt7L2kXLaK0oPZcFV0CB0e7SbucMwmtw6Rs7rRhgn0gNaPqwtNu7A8pKGkq\n29Kz3HKX5KGHHhrKdP6gYwspLQNlUNWhpZqWfxcUJEE6TIs15c8+Hzp0aCjTEci5uiYddtl1OFaU\nP9Uo9seNESl7wjmbzZIEhvKkXFx4c7pvZ5/YVucQxPnhgpm8GFZc+SPi0xHxTETswbUrIuIrEXFo\n8v9+NMxCofCyxSy0/79Leu+Sa78n6R9aazdL+ofJvwuFwhrCirS/tfa1iLhhyeVfk/Srk/KfS/on\nSf95pboWFha6vssuRHeP9pLSkl4SpGy9ZJ/StIMKra10/uidMKRV3TkHUdUgqLKwbww+QYqbFJMy\nI3V2uwcsM4CIo/SsZ9++fUP5hhtuWPZ+UlT2gfnubrvttqHsgqPQas56eH/ew52JnmVe8uogaTrH\nmSpbL6S3o/FUY1ymKcqfcM5fLnBI7z379+8fyjxnkfPP7cD0cKYGv6taaxkW5mlJV73YzYVC4eWH\ns7b2t8Wlsp9DWdOJOlcyIBUKhfnhTK39342Ija21pyJio6S+OVLTiTo3btzY/ZEg7SStI03LdNmk\nRb0gCJK36rvsLS4cc099cBl7XAw/0kvSZ0eH2cZ8P3cMKB860+zdu7fbLsqWagzLLmlqyp9UkioN\n20KnFJ4/oFXdBUpxlD2dfzjOHDc6/NCBi9f5HheWvaeKsj9sH+fKLOHCWXZHzalq5nXKmaniM2iH\nNL0ble+ZJWFq4kxX/r+V9BuT8m9I+l9nWE+hUDhPmGWr73OSvinplog4FhG/JeleSfdExCFJ75n8\nu1AorCHMYu3/sPnTu1f7sogYrKI8XkoKSv9nIqksw0/TqkpHIVJtUkBaqntUS5qmj2xXUmA65HDn\ngVSbaocLdU1nHdJRxiVMmsz+kEby/W9605uGMqkuHURchBu+v3fOoRfLcCnY5x6NlvpJOKVp9YXx\n93Jc+Byt6pwLLtoO206qz7nT2zVw8fYIypYqDctUE/h+OitxjqaqR8cjjg9lwedyPMu3v1AorIj6\n+AuFkWLuvv1JsXiMk2WXESVpOinaSiqCNE1vaSl21JAUjzQtHU1Yn6ORrNsl4XSBKHnUM9UUqjSk\n3b2MOkvfwx0TqjTOwk5k26m6sD63dUv1xh21ptWauOWWW4ZyHtmlDEmpOVc4trTCu6SZlCNVuZxf\npOt8Dyk4abdLCMu2sE7OET6b13lWg05wdI5iu1O2rh091MpfKIwU9fEXCiPFeYvkc/vttw9lF6mE\nOwIJF7TR+eqzTPrWC34oeX/xpHu0ZLvMNHTQIE2kesNklmxjz/+f11yOexe00+0UMAqOO0accmHd\nVKmodrnsObM4nZAC96LwUIYcN6pILHNcmLeAbWe72I+k5lRXXKBWzlvWQTWJ7XUqE9+VKiDpO+cn\n5UOHtJz/Lh9BD7XyFwojRX38hcJIMVfav2HDhoES0VLtHEd6tJbOIXyOllRScxfHnGoC6TApLulj\nWvMffvjh4dqdd97ZrZtUm22kk41zPmHbU01hBB7Wd+DAgW59pLcsU3akmu4sRM9yzOdo1XdnItxx\nXDrI8Fkix9/55zvfeqpuVAfYXsp8pSg4VC+pirIOt/NCtWOlKE3SaZXBqRfsP2XLNs6KWvkLhZGi\nPv5CYaSYe5bePO7poqrQiYEUM2m9S+HlgoA60LGGdNA5AjFSTYL01lFd+q07Cy7B60kx+RwddRgB\niEdxSa9JO1k3VSo6v/R2MPgcrfpUl5yfO2XoAlFyLEiHc1xoPXcy5LFj0n72k9dJk/n+jKrD8eR5\nC4JyZht5PsFFTOJ4kdanmuZ2G1gHv5ty8ikUCjNj7ga/XPEZiICrgzvBlfHkaLTir7fzFeCq5vZL\ned2Fet6xY4ckn43GJYRke2nwo4HSGW5yNXFhsVlmW50Bi7LgyuPy3PdYAOtwbtkujhz3wl3C0Qcf\nfHDZc/SJ4FzhitjLeiNNGxN5z0oJN7mqc/XmCs92cwz5TraFZbqJuznVe44GacoiGc4sSWITtfIX\nCiNFffyFwkgxd9rf21MmlSR95b1JvXouv5Knxs69lPeQ9rpEoGloc5Sa+/O9U1rStApAuH3unqxI\nAbnnSxrrDHhs4yz7wrlHThdVp15RjSI1duHSSXVJn9nGHCPWR4MwQUMk27J169ahzD5TBeD4p/rm\nXLTdSVJ38o5wpyCpPuV8pRw4J9luYp6huwuFwhpHffyFwkhx3k71OQs36VYvjDVpLF1XSZ2dxZN0\nkFST++UMykG6mSAVp0unOzHoaDrddEkZ6a+QcunFeJOmqa4LWuFOIVK2tDxTrcl3ucxI7A/Vrl7e\neGl6X5put+wHg3nk/jrrcJl+6P/AOcQ+M84h4//19vmdBd6dxuOc4+4B5wXfQ/WBcy7rp08E5znr\n5timDGfxcUnMEr332oj4akTsjYhHIuJjk+uVrLNQWMOYhfafkPQ7rbXtkt4m6bcjYrsqWWehsKYx\nS+jupyQ9NSn/NCL2SbpGZ5Cs88SJEwPFobMC3URJk0iHkvqRLt10003d99DiSapPxxpSUNJHvp/3\nMJlighSUzzHDCkEKSsrIcs/5iaoGKS2pIVUD3s+6Xd9I6+nGnA5PlBvVEtJ+Z2V2p+dIgd0pzN74\nUqXivYwJ6FRHypZqF6l81s856bJEOccugiqtO73K66nqsN27du1a1j5JuvXWW4ey2zF6MazK4DfJ\n1vtmSfepknUWCmsaM3/8EXGZpL+W9PHW2tRP3osl66xEnYXCyxMzWfsj4kItfvifba39zeTyTMk6\nmajzqquuamlZJmUkrXLUuEeZ+BypI+lwLyyzNG0RpwpCaylpXS+Gn3OaoTrgnIxI30jH+QNJKp9w\nzjG0mK+UgUeapuDcteDZhpSFS3BJ0MJPSzpBuZDWUgXoBavgOPA553D01re+dSj3QqEvRW8cOT5U\nNanecK5w3Fx4b4JyZHaebAtpPOftt771raFM9SbHczVBPWax9oekP5W0r7X2h/hTJessFNYwZln5\n75b07yT9c0Tsnlz7fS0m5/z8JHHn45I+eG6aWCgUzgVmsfb/X0nOYfiMknVK/uioC0aQVmbnWEG4\nMN6kYKRMBC3bRFJM1kE6SppGpxlax0lTuXtAv/yeEwtpJNvnjqtyt4FHZJkFhjsCVE16TiKUOdUl\n5+fujs6SklLVcRb57D/Hec+ePUOZKgrhrOouEAzVvryH11ymHc5b9plz2AUfoSxWOoZLVYO7Gpxb\neT7AxQbsodx7C4WRoj7+QmGkmHuizqSQpIOkY6S4pLVJTWkZ5vFe50PtjuCSvnNHwGX7STrMe0nd\nuMPAswKkdKSjpLc8Q0DLbvqr0yGH4HMuvDWp+QMPPDCU3/GOd3TfSdqf11k3KTDHkNZ+joWLYUha\n7WhyvpcyJ/j+3pkIaXqOcJ6xTsor1Rr2h/2n6sZdDapUVN3YH6pPBM9FZLsoH87hjGglTasdqQK5\ncxg91MpfKIwU9fEXCiPFXGl/a22gVaTmpPeklaSgSZ9JgTdu3DiUSZNItUnZ6LjRixiz9B4iKSOp\nFt9DCz+t7aR0BJ1Cbrzxxm57s40MBU1KyeuOOtM6TXlRtqTPVFkSpKuk1KTGzrff0VDutrAfpM+p\nPlB1YVup9jhLvrOkUwWjzJNic5zZB4ZCp6ycwxffzzHvBQ2VTs9jypbgWLDPlMusqJW/UBgp6uMv\nFEaKudL+U6dOdf2xHU0iBU8aROrqQBpH325HQUlZXRSebCPVEu4GkPaTjtJSSzruIvKQ1pNiJpxV\nn3XT8k16zSOy3G0gfaUTT54LIEXle9hn3kNVw8nTBblkPP+0YLvjz3y/i7DD8eJcIOjklHPEZcYh\nKAsnI8qf88VZ/hPuO2DEIn4L2c9y8ikUCiuiPv5CYaSYK+1fWFgYIqTQCk4K6iK/JJWiY4U7ouuO\nUZIC8llSMJdMMqkhfaupLtCxhRSQag4twtydIFV7+OGHl/WDjirsG9tCCkzKyjJBqk/LN6lk7lqQ\nFrtzE7TYO59/0mE62fDYMVWmpPIcH84b0mHSfraLVJ9qB+XI8Uo5uh0jWtU55hw3qmNUbzhHiZ7D\nEVVhp64y6lLOLTfePdTKXyiMFHNd+S+55JIhNDPj83GF5688V9Y8zcVfVa5IfI4Gp4MHDw5l/tpz\nteGeKldEMoW8350YcyGT3X6u821gMs8M8sAVfhaDpwvR7VyN2RauZnkPjXBsH6/TyMj+cIyYsYir\nGVdHMpgcF84P1s17Cc4bt8K7wB65crJusjfK0/mEkOGQKXDsyHZ76bopH7aVY8Vyfk+18hcKhRVR\nH3+hMFLMfZ8/KamLIcfr3P9O2s17SY1oTFspDp40TdlITXvurUTP5XhpHayb6gUNR7yfVJqGnqR1\nNKY5quvkwuuk19y7dmGsk0qyTSyT6nOsSI3379/fbS/b6Fxw03BHiuwMtQTl7xJbUjUirc/5xza5\n0OK8x/kFMGgL7+G8pCEwVdZHHnmk224XkzHnbZ3qKxQKK6I+/kJhpJgr7T958uRAQ0mBXPAFUpi0\nQtPCSTrIfXPSNBfembTLhdcmfU03TdJF7iHTwkyq6fLTk77Twt6juFu2bBmuuT18qiuk7nQvpTxp\nYT5y5MhQdjEHe31wYaJdcBSOnXND5U5NWs2prrBvVBE5/k4F4tixnl4gGM4P/p3We46hiz1JNZGU\nnXOu5z5MVYzvoarDd6bq8FKH7r4kIr4VEQ9NEnX+18n1StRZKKxhzEL7fynpXa21OyTtlPTeiHib\nKlFnobCmMUvo7iYpuc6Fk/+aziBRJ0E3TlL9lRxh+BxBes2sJky4SHWAtJfUlHTUZRJKkFLy/aR0\ndEpyoHWatDIt6M6xyNFotmvnzp3dZ0kZ2cYMAS2dViW4G0GK6nYPXDATOui497Mf2T/SeI4/1RLS\nYaouLuAF7yc1z/nCv3M8KXOqVM4FmjJylJ3vT/rOPhw9enQoU84MfJJ9c0FVepjJ4BcRC5OEHc9I\n+kprrRJ1FgprHDN9/K21k621nZI2SborIm5b8veZEnU641uhUJg/VmXtb639KCK+Kum9OoNEnddc\nc01LWkIK6LL39GjdgQMHhmvOh5z18bqz4NJq3qP30mm6R2cWF3CBoPWV7XUn9WjtTtCqzf64eIcE\n6SN3J0jNaU1mn5Jik6JSReJJNmYGYrhuyoX9IH2lk0svXDrBsxouYxGfo/ypArJd9LnPvjpnLr6H\nY0WVgu0iqNIwUAuduJhktPceqj3cVcndi5fUtz8i3hARl0/Kr5B0j6T9qkSdhcKaxiwr/0ZJfx4R\nC1r8sfh8a+1LEfFNVaLOQmHNYhZr/8OS3ty5/n2tMlHnyZMnB7pF2k36QosoKXg6opBekY6SOm/b\ntm0o03rtYvU5az/paFJjUtqeT7g0TcddCGa3a8GjzmllJ5VbKbS4NG1hJ9Vl/Dfn875169Zl19jP\n+++/fygzaIfzOecOC6k028j+sf/ZJz7H+UHqzP5w/LmT4JKzUjXIcWSb+E7OP3cmgePPOcz5R9WA\nR7azfv6d6hodsoieurgSyr23UBgp6uMvFEaKuSfqTBpGCycTSN51113dZ5MGkXbdfPPNQ5nUyFk8\nSekdlSQ14/1Jn/l+1kFK3aPO0rQaQTpKysq2p8pCSs93UtXJCElLrxNsO9UBJn/sJcUk7WffaLGn\nGueOlfI62+j873v3Um50/skw35KPnsT+03GKqllv7vAcAMEx5y4EaTxlx3HmmFKOqQ5xTrrdIJ7n\nSHXFZS7qoVb+QmGkqI+/UBgp5n6kNynOfffdd7oRJignfcrTajyLA4Vz7Nm9e/dQpvMLKaCjptlG\ntpWOOrRqE6SUpIakgytFD2LfHKUnNSWNdz7q27dv79ZDC37S6l27dnXf4ygm6TjlTNk51YD9S5pO\nVYROXhw3FyWHTlmkzK7tOS6k4pQ/d2koZ1r+qWrQ8s8y/fW5a5XhuN3OlJv/qYI4Z68eauUvFEaK\n+vgLhZFirrSfeN/73jeUSSVptSdNSycb0lJSOjpq8Bglqaar22VH6fmL8146qjirLq/z/sOHDw9l\nd0Yh2+6cSZxjE6kp/fbZZ+dww7Z87nOf6743wexGVC/YFhdwlGPO/pHi92Lrk66T6rPPpNG836ks\nvJ6U3fnQO2ca9pnORxx/Un0XfDTVpFkSbtLhKvvj5N1DrfyFwkhRH3+hMFLMlfZffPHFQ454OlyQ\nPjpHjKSmpPo9hxTJW3UZQ53UkFSftIkW16TJtOS62PdMJkrrLK22bBedRXrBKunMRKrPdzKAJVUT\nl1uA10n1iXRW2bt373CN6gp3DFwdVIHYRo7dSmmveG6C9Jq++qzP5YRg5Buqg72291K1SdPzw80b\njiFpP+txR517kZ9YH3eP+E7OuVlRK3+hMFLUx18ojBRz9+3vWa7pZ56ZaaVpWkO6n3AOQaSXzimC\ndbNMCkarbdJ00nX6s9Pyysgw7hgp1RuCEXZuv/12ST7qD48rU42hXHjdZcPlPfv27RvK3/jGNyRN\nO7O4o7ukwO6ILikt6bBT+2688cZl/eHfuXtDmk6qTxWQcqRjVc+hhnXTSu/AucC5yuuEy6GQY0FZ\nud0e1p3jOUvA2ESt/IXCSDH3RJ29E1L8BWOwjN6ePg1OLpMLwVWNKxJXPv7Ku3pyRaAbL+twJ/lc\nWG6uTu6deQ9XW64qXDG4wrI+GgUpzyeeeGIo01jkgm8k3vKWtwxlGjyd8Y3GR67wrk66QKexkKch\nyQzJSLjyOyPjLKGzs04yEzI5GmfJvHpZdyTfZ44FV+uUHZkB20ImwzHPb8GxhB5q5S8URor6+AuF\nkWKutP/555/XQw89JGnapZRGNlJw0tqkYzT+8DkXnIN01LlUMux0GpmkaYqVqgn3+dkWl8PeJVzk\naTeqAFR7duzYIWladWA/3ck4lzee+/UMOJEnyaT+/jZpNKmmiwPoEkv2TgwuvSf9QKTTlJ3JS2nY\n5Xs4FjQg002W6qUL0Z7qEMfBZRci6A7swnjznc7nI+EyQFGNIbKNL2miTjRmISIejIgvTf5diToL\nhTWM1dD+j0nah39Xos5CYQ1jJtofEZsk/VtJfyDpP04un1WiTtJRUlmXHSavUy3ohVyWpq2wpJS0\nyJJ20u2XFKv3ftJIZ3lnf5zVeM+ePUOZFI+0N63cfI571U4FcqDV/DOf+cxQvvXWW7t1Jmi9p0zY\nf1Jzvoc7MqS3pO98J+WYZUeduQvi3ItnuYd15rx0OyBU19hnzsVZArtw/pPKs/4E1QiqLr3Q8efC\n2v9Hkn5XEqMbVqLOQmENY5Z0Xe+X9Exr7QF3z6yJOrkiFAqF84tZaP/dkj4QEe+TdImkV0fEX+gM\nEnVeeeWVLWk7LdW0oLLci6NGKykpDuk4qaY7MUZHmEOHDg1lhg7vJXykWkIaR+cP/sjxFBp3D267\nbSrR8QC696bFmX0gvWSfSW9JKenMwz5TlSDYvwRpuTs95pypXNAOF+ePfUo1gY4y3I1xocjdyUsH\nzsUcc1J3/t05ZLlw4Rw7hhenykBVKmVNObA+tsWpF7NixZW/tfaJ1tqm1toNkj4k6R9ba7+uStRZ\nKKxpnI2Tz72S7omIQ5LeM/l3oVBYI1iVk09r7Z+0aNU/o0SdGzZsGKgvaRItmKSMtIInSO9cxhzn\n/ELaPYu1mQ5CvZDZbJ+LFUgLM+9xWWhoEU8nFtI+OrY4336W3S4ALeisk3SzR3FJ3Z2FmzsSrINU\nn5Zq3uOcn3rX6CjGnQdazzm2LLuEsCkXzi2OD2k85xProPzZH/aZ48+2pHwZopxwoduzjt4341Du\nvYXCSFEff6EwUszVt/+CCy4YLPHOB9lRqXQ0cX8nXEJOWocZzIHWZKoPpOBJJamisD6qK3SKoZpC\ni7hzViHyfndE1lFAghSY7yHtd84vSXGdhZv+7wTr5u4Affs5RqS9vD/HmrKlKuYShfJ+Un2OJ63t\npPUZuKN3xkCapv1UL9lnqkYsU15UAXrnBXhcmWoZ53NvN8olqe2hVv5CYaSoj79QGCnOW6JOWqEd\nTSY1zvtJC0kXaVWlasDIP4RLaEg/f1KzpG9uJ4G0j1STFmmqF6R6pHWk4EeOHOm+K0EZsuxCSrts\nLnwnLdUnX8qVAAAW9UlEQVQpf/cco+rw6DKdpqiakKZT7aIKwPFPdYPyJK3l+HM8SdlnOeJKtSbD\ne3MOUV2jLHiGhGHBKX/u/BAcf+5gpFpMuXEeUu2gLM7pkd5CobC+UB9/oTBSzJX2LywsDM4gLlIJ\nVQBaZ9NSTApEJxfSXvrW0wrsEiXSCs76+WxSw1kSgrpoO/RLJ32kNbnn5EOHJLabdJF03YWLpp8/\nKStpf893nA40pJpUS9gWN57u3AYdhHq0lbSbdJzU2B2X5bOcc4y804ukw3ZTLXE0njJ0Y0TVkGPB\nXYCVjuRybnHO51x0amkPtfIXCiNFffyFwkgxV9q/YcOGrq+5S+DII5BpeSZFd84vpP2kY84phnSU\n9ZCa9nYHaOEmjSSl53FN7l7wPbQ206c7Ld8u8CipLut2QR5Jr51jTy+xKC3wfI4Wbr6f95OGkupy\n54N0nGOaz1K2bB9lSCu48293TmGk/Y8++uiyv3P3gHOS76S6yHZRBXPZm3q7Sk8++eRwjXOI87AX\ndWk1qJW/UBgp6uMvFEaKuafrSopDakyaRuePXrQTl/6I1Nkl56RvOct0EGJyTlpeU11xSSBJHUnH\nnF82VZOVwptRDmyTo7ekms7pg/ewTqpAKSPKlucZSOlpsaZqwPdTXlRBKDtS6bSyOws2r/NMhlPv\nSPvZZ6p3PT97F5mH8ufYsp8E1URSeb4zndg4h5yKRtUg1SXnkNVDrfyFwkhRH3+hMFLMlfa31gZ6\n5CKlEKRbzlLaq4PUjJZi0kQXwJK++HRiSepNyzut18zSy/e7yEBsF0FrcqoDpH10OCFcMEdSYFrS\nSeVJU3k9ZerOXrBddKzhjg5VFrbRHWOm81HWSRmSDrPd3HlwDkwcW1J29in76uTpVDSqDiy7Myes\n3x0N7z3ngnnu27dv2d9XQq38hcJIMXeDX64sbu+Sv3z8BU/wOf560oDDXz+uZPyldEEmaDjqGU9Y\nB1cPsgAadgi2iwY3Gjl7SRl7seykaUOZC45BFuLy07N+tj1XHOdnwP6zDq6kBFcwypZyYTndod0J\nTPaZ/XSu3gyLTubH+nvu5wQZJu9x84ksgP137uA5/j2XY2m6b/QPSOaza9eu7nM9zJqu66ikn0o6\nKelEa+3OiLhC0l9JukHSUUkfbK31vUsKhcLLDquh/f+qtbaztXbn5N+VqLNQWMM4G9q/6kSdJ0+e\n7GbQIQWiUYyGnqR4pHGORhM8eceQ0jTEkJq5TEJJ5Ug13X6uM9CRypEa8z29XPA0DlItYn8I1keV\ngqAbq1M7su18Z88VdSnYf5cok2NH9a4XL5A0mvIhXeY9VClc6HCeguSz2a5eFh1pWrbcZ6fBkQY8\njh2f5TzuGUs5bzgnXcam/FZW4/I7651N0v+OiAci4iOTa5Wos1BYw5h15f+V1tqTEXGlpK9ExH7+\nsbXWIsIm6pT0EclvbxUKhfljpo+/tfbk5P/PRMQXJd2lM0zUmdedFZ60hXvUjAvX+7sLGkGqxx8f\nUiZep9W+545LSkfaTwpMauxO2Dmw/pSRi/dHNYLUke7NvcSb0rS1m7Lgvnz2j/10iU9dck66Y1M1\ncwk0e+HAqRawb7S2k8a7YCp8lrEae0k2Xfht58ZLufA9bifLIecO1SLnrux2FWbFLCm6L42IV2VZ\n0r+WtEeVqLNQWNOYZeW/StIXJ7+yF0j6H621L0fE/ZI+HxG/JelxSR88d80sFAovNVb8+FtrhyXd\n0bm+6kSdrbWueySpEa3gpLiZwYR0yOVE74X8lqatzaTMDO/N+0lfcxeCTjDsi6PXvM6dB1ptGSOv\nZxch7aQVmrshLrYcTymSjtLVlTssvYAjfD+pJlUq5y5Mqv/YY4913+My/6RzEeumCuCcZlwmH7fz\n0QPfwz5TjXCh2HtWeGm6z25HKOc0ZU4HIu4qcG5lGHXnENVDufcWCiNFffyFwkgx9xh+SVtJe0ix\n6BRCipX3kw65E26ktKRGtMi6MN60zpOmZ510rHGx52jVZR1UNZyDDlWQVAHYB7fDQfA6+8lYdASp\nMccl1RonZ8KdvKPMObbcVSFN5y5EXneJQt0JNqpmpMGk6VSvesFMXAYmtoV1sC1UV53sKC/Wmac6\nqYr1voOlyDMJK4X+JmrlLxRGivr4C4WR4rwF8yA1pjXV0ZYexaMzDZ02CFImR8HoWEO613MEYn10\nLCE1d77gtE7T2k86yjrTmr5ly5bhGukgzycQPJZMFYjWeVJ9ypEqUI4FnyNdpRpDi/2ePXu693AM\nmYWIMuo5RVEV4LFcOiTxHqo9pP0u/qBz1urBxd7jeHJuu5h6bCPHKGXE4+occ9bHPqTqMIsjUaJW\n/kJhpKiPv1AYKeZK+0+ePDlQNdIT0mTSp97RSD5Huka6TEcdWuRdeG/SJ6ogpF55j4sl6MJ1cyfD\nOQI57NixY9lzzvJLukoKnM4f0jRldzsFlFfuVDjHJ4d0yJKmjw676E0EVa20fFN1YN9YpiMMr/PY\nLXdenAqYag9lznY7Sz5l6xKfUubctaFMUy4uAhNlwfekCuIiEPVQK3+hMFLUx18ojBRzpf0E6Tut\n1qQ4pFVZpg87qRGt16S6BJ+lykAVgPStl0OeVIsUiw4s7AOddmid5jtpbd+8efOytpB2U24ukwxV\nExdVho4wPE/BtiSV5BkG0ljey77t33863IOj2qTSVA3Yrnw/qbZL1EkVjeNPek1Voze3pNPj686H\nUB1wapwL481dG4L1pPOXi/TkAojm7onLbtRDrfyFwkhRH3+hMFLM3cknKRyPJjrLK6lP3kMnEBcW\njFST9J6WfFJW57jDdvWs3KTDfA/r5jFSXidIWXvZi5yFmZSelJYWYdZNeVG2zskp6ShlQvWCcnNn\nFaiOsS2U57Zt27rPporTc2aRpuk4VQC+0znIUL2gFT5VLap3PHLN8WF2JcIlBOXYuUSt2VcXJYh1\n9IK9rga18hcKI0V9/IXCSDFX2r+wsDBFKxOOpveoEWkUfdhpySdNpFWZlJFWfUa7IQXu0TqqHXTs\noVXXxcFn3+kvT8rKPvd2Ldg3p3bwPS6xJKku6TjvT3nRn56gT7o7usxdAI6FO6bMXZO85+DBg933\nuzj4HGfuvLi0Y1Qfcn7RMk9HrV5QV2la5lQv6WTk1L6eKkG1g+oK5yflnCrduYjbXygU1hnmuvJf\neOGFw6rIXy13CouGjjTQcDXkLyJXIa7qDPnN+8kUdu/e3W0vV638RU2XW2mahXDPlSsCjWxc7dkW\n5zKcbIKrumMVLmkp20XWwhV2pdWCqzpXSa5wDlzteHqR48UxZ9vT4Mc6MnmnNC1DskDCJTPl2HJl\nz5WXMncus5yLN99881B2WXoId6o1WQvfyfHhvKWscg6txoV8ppU/Ii6PiC9ExP6I2BcRb4+IKyLi\nKxFxaPL/fmrWQqHwssSstP+PJX25tbZNi5F896kSdRYKaxor0v6IeI2kd0r695LUWjsu6XhErDpR\n56lTpwZDhqOaVAG4F5uUjRTpvvvuG8o0uJB2kfaR6hKktaRppGZJE7/+9a8P1975zncOZfaHe+hH\njhwZytx/Zj9IR0nNk+L3EpZK0xScIB0mZWa73P2Uf6op7j3OXXkW0Cjn/AiSyjs/ACdz0nFSY2fw\nI5Juu8xEBOdT72SeNH3C0MUcpJqY84/0nd8B5yTLOc9faoPfZknPSvqziHgwIj41ydxTiToLhTWM\nWT7+CyS9RdKftNbeLOk5LaH4bXGvyibqjIhdEbHLRR4tFArzxyzW/mOSjrXWkmN/QYsf/6oTdV59\n9dXDDwTdQd2PAilM7m9z35R0lZSK9Jp7yO7EEymrc1NNUEXgaTS+k6cUXXy6XljwpfUkaI3mPi9D\ncVNWLFNlIE0k1aRlma6sVJkSpMOkpvRVcOPJtnO8WA/VjrS4u/iI7oSd24Vw4cW3b98+lNMXgfWR\n0nNseY/LqkQZUr2hzN2pwQR3Hjjn2a4cl1nCrCdWXPlba09L+nZE3DK59G5Je1WJOguFNY1Z9/n/\ng6TPRsRFkg5L+k0t/nBUos5CYY1ipo+/tbZb0p2dP606UWdaQknBXZw1ImkNLfks0/Ls8pnTCusS\nS9I6u5I11VlvmZCSlI4qgFNBeq6hpII8Dcn+uBOOjoKyXaSSdCJKaz/b6sqktC4Udk+eS9vIenIc\nqVLQIcrt8BDuujsdl6C7NncJqBZRXWMGIjqC9ai55FWTnFNOpVltHMgXQ7n3FgojRX38hcJIMVff\n/ogYaCBPirnknHTWSDpKGsksMY6Okho7RxSqGqSdtFonHSOlJQUlTaRF2FlfSQfpl987+UUnD/6d\nZwXYT6f2EFSTelZ96bR1nLSb/eF72AfnaMJnqWq4k5dJjblLwPF3apxzZnJj4eIf9t7JuUL1jhmj\nuGNEGbHP3Pnp7VRRvSCcqpPzsk71FQqFFVEff6EwUsyV9r/wwguDv3YvVpw0TUd5PSkeKRjpMCk6\nqQ/pmLN88z20grPOpFV07CEFdFZYvoeUnVTT+c5nX0lXSRFZtwsXTdCBiGXS+p7zCSmo85snenSU\n9UnTsiBl37p161DOceQugTuizLbQ8u6OwLpkqmnZJ0WnGkFVg9fZB47X0aNHhzIzGbkxyvqp0vIo\nNOVJueRzHMuVUCt/oTBS1MdfKIwUc0/UmRZqWodJX2kppwrQOwpMCkwKNkuOctJH0jTSup6fPY+X\nko6548KOpvM9dHKiqpHWbraPciNcWGiXjYj3UxaklUnfuXvC9+/atWso945fS9NUn3W7rEo933nS\nctbN65QbLexU9QiOC8t5P9vqsh5RLk6927lz51Am1WdEot55Cs4t1u3mU47LakJ418pfKIwU9fEX\nCiPFXGn/iRMnBupDSzktsgR9qvM50j7SLh57JXWk5ZeUjc48zsmlFwXGOQrxXpfDnuoI1RcXYSbp\nOGVFxx5SQFJTgu2lBdtFOOI9SSF7gSIl6dZbbx3KlOcsDkQcC6oSPf93yoph1l0yTYIyJ712vvuJ\nTHwpTatL7L+LxuTOh7BOgiprqi+sg2oc1RhS/JT5S3qkt1AorE/Ux18ojBRzpf2XXXaZ7r777mXX\naaklbSFlSssvnWnccU3mh3dWW6oMdBZh/bS40vkkQQu3C/hI2kv6StWA9JWW6pSFi/1Pes17SFNd\nhhfnA04Knv3nOQxn7SZ1pSy4Y+Ji67NPdG7JZ10EJNJ4toVziPdQNSTY51QB2E/K0KkoBB1t3G4D\n0TveTDXGtds5Oc2KWvkLhZGiPv5CYaSYe7qutMSTSpKaOSqf1lnSQoIWW1qsl74/QccWUnDnfJL3\nkHaSDrqAjKSgbCMDVVLV6OVqd7nsqXa4qEJUDQjSdKpGdFDJPvGIKndSSG8pW5cHgTsPLpgnVZYe\nraZ8aG3nc4cPH172nDTdTyZh5XixnoQ7Uk2wDy6BKuFyNfBdCcqW3wpVkDOJ8FMrf6EwUtTHXyiM\nFLOk67pF0l/h0o2S/oukz0yu3yDpqKQPttZ+uPR54vjx48MRR1ItUllaNqkC5P2ORpEibt68eSiT\nspKyOSssj13SCpuOSHQyIUjBnOWZbWQ/qQ70nF9Ir91RUD5HRxSXUov0kRZ57gKk7Ejj2TcX255y\nY5ZkHq+mdZpz4cEHHxzKOV50JuL4O2cq+s1zJ4GUnuoIaXq2i/dy3vBMxiwBWSlz1uNyG6Q6zPnE\nuXrLLbcse0Y6PbdWY/WfJW7/gdbaztbaTkn/QtLzkr6oStRZKKxprNbg925Jj7XWHj+TRJ0nTpwY\n3EDpvkjQEENDSP6ycmVye9j8heWqxX17rnb8NefKwuu54vI5Gi1dDnuCq3bPjXZpP/JXnCssmQSN\nQ1y9eD9XEPaHxjK65pJlZJl1OEMVx4WrLcHxoksxV0EytVzxaEzj+LjTey7ZK+shO+JqmczOndLk\nSk7mQZ8UsgD6NrC9js1lG2mQ7Z06XNruZI80PK+E1er8H5L0uUm5EnUWCmsYM3/8k2w9H5D0P5f+\nbdZEnW47qlAozB+rof3/RtL/a60lj1l1os4rr7yyJW1ygTBITWkgSlrD51xSTZ6eYn28TmrsTp6R\nyibFJY2kIYpwgTVI72fJ/JP39GIJLn2OdI/GKu7L07BIyu7anhST72d/aBwlvaWMXMw/Z5iisS6N\nwzRaUu2i0ZJj64xplD/vP3jw4FBO+k5aTjWOc85RbOcL4ALFsC3po8DYf/RtobrSe/8sgWwSq6H9\nH9Zpyi9Vos5CYU1jpo8/Ii6VdI+kv8HleyXdExGHJL1n8u9CobBGMGuizuckvW7Jte9rlYk6I2Kg\nnqRyLkZaL5khraBMlEiqR9pLmkqa7Ci4CxPdi2fnaCz389kWtp3X+Sxpeqoj/Dv78NRTTw1l5y7M\nQCmUEftBdYD3ZD0uDh/r2LJly1B2ceRceGta0Nk/yiJx5MiRocy5QqrNdpEmk16zH1T7su20zHP3\nwo25C6LBut09lH/eT1m5E5g99eJc0f5CobCOUB9/oTBSzPVUn3SawpDqOSpHF9ikybzmkhmudDJQ\nmqZ9vO5yyKcF22V94XMuLDktyM5NtBf/jjSWux2k+r0Ek9K0dZqqBuk920JkP0jF2RZSY2f5JqUl\nlXWnDYnsvxsf1k3VkZZ0xnCkYxnb0pM/x9A5dlHV4e6J2z1y6hDvv+mmmyRNU/oDBw4MZVr+e7tE\nRfsLhcKKqI+/UBgp5kr7I2KgjaSamzZtGsrOR7pn2XTJNkmHSC9J00iHSaVIE+lHndSTKgVpHNUI\n+vmzP9wFoBXeOdwkNSW9JdUm7b/++uuHMvtPv/1ZaGcvCw0ptUuq6nZYOG5OfeBcIB1PVYv3cgeI\nMncnJnfs2DGUuWvBcellZmI7qFI5GXLM2WeOkXMyI3KsqQrzOXeqM8/EuDDwPdTKXyiMFPXxFwoj\nRawmn/dZvyziWUnPSeqbptcXXq/q53rCWunn9a21fgqsJZjrxy9JEbGrtXbnXF96HlD9XF9Yj/0s\n2l8ojBT18RcKI8X5+Pg/eR7eeT5Q/VxfWHf9nLvOXygUXh4o2l8ojBRz/fgj4r0RcSAiHo2IdRPq\nOyKujYivRsTeiHgkIj42uX5FRHwlIg5N/v/alep6uSMiFiLiwYj40uTf666PkhQRl0fEFyJif0Ts\ni4i3r7e+zu3jj4gFSf9Ni7EAt0v6cERsn9f7zzFOSPqd1tp2SW+T9NuTvq3H3AYfk7QP/16PfZSk\nP5b05dbaNkl3aLHP66uvrbW5/Cfp7ZL+Hv/+hKRPzOv98/xPi/EM75F0QNLGybWNkg6c77adZb82\naXHSv0vSlybX1lUfJ/14jaQjmtjEcH1d9XWetP8aScx1dWxybV0hIm6Q9GZJ92n95Tb4I0m/K4mH\nxtdbHyVps6RnJf3ZRMX51CSO5brqaxn8XkJExGWS/lrSx1trU9Eq2uJysWa3ViLi/ZKeaa094O5Z\n630ELpD0Fkl/0lp7sxZd0qco/nro6zw//iclXYt/b5pcWxeIiAu1+OF/trWWUY6/O8lpoBfLbbBG\ncLekD0TEUUl/KeldEfEXWl99TByTdKy1dt/k31/Q4o/BuurrPD/++yXdHBGbJ9l/PqTF2P9rHrEY\nw+pPJe1rrf0h/rRuchu01j7RWtvUWrtBi2P3j621X9c66mOitfa0pG9PMlRLi1Gq92qd9XXep/re\np0W9cUHSp1trfzC3l59DRMSvSPo/kv5Zp/Xh39ei3v95SddJelyLacz7WSzXECLiVyX9p9ba+yPi\ndVqffdwp6VOSLpJ0WNJvanGxXDd9LQ+/QmGkKINfoTBS1MdfKIwU9fEXCiNFffyFwkhRH3+hMFLU\nx18ojBT18RcKI0V9/IXCSPH/AULrupEzoGneAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a63246cbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a63253fb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 24)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 25)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 26)\n",
    "display_image(hh_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6325225c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6325229e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnV3MZld13/9rZmyMB2OPPWOMbFODhByhCkyKKIioSgFX\nNEXkDoGUKo0icZNWRE2VhlxU6kUkrqLkooqECGmq0CSUBDVCERFNiNpKEQVK2gQMNaWAx/LH+IsP\n8+WZ2b14n/XMfx6vn5/9jmee4X3P+kuW95x3n/11zn7Of629PmKMoUajsTwcudoDaDQaVwe9+RuN\nhaI3f6OxUPTmbzQWit78jcZC0Zu/0VgoevM3GgvF89r8EfG2iPhyRHwlIn7lcg2q0WhcecSlGvlE\nxFFJ/0fSvZJOS/qMpHePMb54+YbXaDSuFI49j3tfL+krY4yvSlJE/IGkn5aEm//YsWPj2muvlSQd\nPXq0rBMR6/K5c+fW5bNnzz6rrv9w0Y/YkSNHyrLD+/H+vZz3nj9/fn3NywTv09tz+NifeeaZZ133\ntcr1k6Rrrrmm7Mfbo/H6nL2+95XjpXHTWnl73g+tMz2XClTXx+1lGouv835w7NiFLUNr7v34mnud\nbWtH7/a28tmzZ3X+/Pn6gW3OZaYS4HZJD9i/T0v6+891w7XXXqu7775bkvSiF73owiBsQb381FNP\nrctPPPGEpIsX038Qqk2z2Y9vHH9BvvWtb63L/iB8c2U73/ve99bXnn766XWZXoQXvOAFZdnxgx/8\nYF1+5JFH1uUf/vCHkqQTJ06sr91xxx1l2efma+Ft+9h9bb3+jTfe+Kw2aaPSevpz8bX1srfjz6j6\nQfW6L3zhC8u6N91007r84he/uKyT75AknTlzphzvth+ikydPrsvXXXfduuwb/sknn1yX8xlulumd\nz/F6XXrP/dlmHX9/tuH5bP4pRMR7JL1HungzNRqNq4vns/kflHSn/fuO1bWLMMb4gKQPSNLx48dH\n/sr5r5l/Ef0Xz7+g+ZXx+/zL8/3vf39dJprq8Pr0NTl+/Pi6nL/s3qf/Ynt73oZ/Ef1L4XV8zv4D\nmb/y9DXyL7nX8a/Dt7/97bJPh8/fkWtNolMlim32T2vhXy1nUNVY/Hn6fTfccEM5Ln9vfIy+zrT+\n+Ux9Dv5MvH+fj3/5vb6/FyTqViID0Xufj1/Pcc+8+4nno+3/jKRXRsTLI+JaSe+S9CfPo71Go7FD\nXPKXf4xxNiL+uaQ/k3RU0ofGGF+4bCNrNBpXFM9L5h9j/KmkP93PPRWFJcWZUxinzBX879/97nfX\nZdKOX3/99euyUylXPlVjJar3ne98Z112Gunj8rHMUPmKyjkd9Xn6dW+DFEQusjiqcfl8fM6k5KPT\nC6fAPka/t3pG3qeDrtN4vR8fiz+7rE9Un8QFf7Zen+i7t+/jynWhUy+/z59htr0r2t9oNA4wevM3\nGgvFFT/qc0TEmqoQBSWjiLzutMdpp1NgPz1w7bFTM6L6Xsev53jprNbprbfh85kZl88p23Ta5/P0\ntfCzZad+ZBfhZT8Xd2Qdb8PnSacE/my97FSbzvl9/rnWtJ5EwcnIh1Bp08kIiGwCZuZJpwDeZj5r\nF4t8/iRG5lrsy2Bqumaj0ThU6M3faCwUO6X90gV6QoY9pIVOykSGNU7dnBrRKYFTVqdKPq7KFp7o\nGhmi0OkFUTkfV/blpqgOp/pkakqmoW4gQ/Q5r5OW2u+jdfHn5XMjG/lKrKPTAy97e/TM/bqbMfu9\nqfn/5je/WY7P684YEJFoSPfmyRdZw/r7WWn+yZCoQn/5G42Fojd/o7FQ7Jz2J4jqO8VyaprUj2yr\nvQ3XHpNRhLfjdMypdGUv7ve5cYjD6aW3Qbb1PsbKO4woItF7XzcaF9FUv7cSwYj2klel13fPO3KB\nrcbrFNwNwrxt0o6TeOWo5kn29N4eteHzIb8Ib8efRYqGJGp4235Kk3uhjXwajcZW7PTLf+7cufXX\nsjJNlC7+Ndtm0jujNHIWQL74fnZONgeVtxWZdPpX0BU0M+fv7oWXffk1/0o43OffGZFf93HRl8XX\nrlp/b9u/yNVZ9WYbpGT1tXM4C0uQuba34evlz9+fhZt3kx1DBWKbxMK8vs+fbC6y7G37nCkmRK5n\nf/kbjcZW9OZvNBaKndL+I0eOrCkMxXZzWlsp/IjWOI2sTGSli8+iPaQTBW5wypr3Eu0nuwXvk87i\nKZ5ezrU6+5cuDjbiNNHXzev4GlXeg1I9f6fIvm40T2+DFG4kvnh4sRyvt+Fj8edGJr0u0lHQFC9X\ntJrWx+fsIJsToux+vRI7yGTXxZt8zjNxJdftTtdsNBqHCr35G42FYqe0f4xReueRJ5/TmoRrRv2c\n3emViwZOzbw9CkRBMeqSmpGGm8Iye//kyehUtgom4mMi6kxmxCQmuSjhYodT0ByjU2cS0SjgBJ0q\nOL197LHHyuvZjrfhY/F18z59XP6OkJjm5Vxfoui0thQBmoKfuDhWmUNTHEQ/pXLkPCmuYoX+8jca\nC0Vv/kZjodi5tj9pFcVtc5r0+OOPP6sNMo4h7bHTcQogUZ0qbCLpltNlFyMomIfTuxkxpTJy8vac\n9nl7rr2nYBJerkxKN+eR1Nfb9jKZ8fp4vW06HXBUySp83E71HZSNh0Q6CnWe76K/Hw6KoUjzISMv\n0uDnmnr/lMCjOsm4rLQ/Ij4UEY9GxN/atZsj4pMRcf/q/yeeq41Go/Gjhxna/+8lvW3j2q9I+vMx\nxisl/fnq341G4wBhK+0fY/zXiLhr4/JPS/rJVfl3Jf2lpH+9ra0jR46saTPRVKItqU2lQAlOk5wO\nEb3y+k7ByYgjKZu37QEhnJo51XfQWOjkI+mzixqk7XW4OEAJLH1u5NuQFHsmaAfFlnM6TlmN/N7K\nzp0MuGbWkzL5UCadir47dfe1dQ0/nSq4mEKelI4U+0iMonDt2d4ubPtfMsZ4aFV+WNJLLrGdRqNx\nlfC8FX5jjBERGCLVE3XS2Wmj0dg9LnXzPxIRLx1jPBQRL5X0KFX0RJ033HDDSOrlVI+CYjhlSzrk\ndZ1SzcRWm4nnR3SrGhO5rjq22e1LbPyT93pdovE+B3JdJWMZiptYnaBQglWKm0gGLyS+ecCPqk8y\nmiHRwMsuatLpQIoMlPXI14QMcSgbkI+F6HkVgtvFyG0JRncRuvtPJP3sqvyzkv7zJbbTaDSuEmaO\n+n5f0l9JujsiTkfEz0t6v6R7I+J+SW9d/bvRaBwgzGj73w1/est+O4uINd1xyuRacwrHnPedOnVq\nfc1pj1Nq18KS3wC5gBI13RbVhhIv0nyoT6eb1TjIgMjH4kZLXt/HQv4H3k6KI+T+66IG2dk7KM4f\nnQ5kmUSEmVh5lMnJ+3exK09TZk5syCDM17Z6ntLF71MVL5DEpf3Q+m1o895GY6Hozd9oLBQ7d+lN\nCkOZcZw+On1OGkbU3etStBdyr3X6SrboVVSZSizZBIkdlHmo0uDSKYFTU6eRft0pOEUsIlFnm704\nnYzQepL/A+WwT98JSrBKRkb+XsyIPdUJBrko0zOkYLIzvgBVtB9/P1102Obe3QE8G43GVvTmbzQW\nip3S/ohY0xqybab871UATwrU6W1TfYoIRMZCeS9RQKJbFM+dknl6m9UaEdWmePY0ZwdFuMlTE8pl\nQO6y/tyonxnX7Mqwi7T6XoeSWXp9yvxTads9fwAFJyUjHxIN6d3NMmn4KQ9C+qp84xvf0Cz6y99o\nLBS9+RuNhWLn2v7UXJKRBQXQrP7utIsCRVJiRYfTNKLySSXJtt+pm1NQyi3vNuzk0pr1iWo6KLEk\njdFBYkdlxEIBLCldmtNbskv38VYnBZTs1MtO9StX7M12vL6LAPkukh1+FWxzsz75eXifXt/X4syZ\nM8+6z93P/fn79Tzh8Xa3ob/8jcZC0Zu/0Vgodkr7n3nmmTWtcSpFVLuKRU/RezzwJWlh3eafXH2J\n1lWguO0e29+DfLrxkdNx0gJXtNtFJNIIz+REIDfmyv7ex+TzmWmDIiOR/0VF8YnqU94Cmj/Rcbo3\nQe8B+W049kPDpQtrSqKjiw4uOuY737S/0Whsxc4VfvlrScEXKM5b/iL618Z/yclclr4aDgq7TGOs\n7iNFJWW18fZcyePzy6+Qf+3JFJrsDNzUd8YWoGITxFJ8LD5GqkNBLvx6FfyEvARJUUtmz2RS7F/T\nkydPPmvcDgrs4fOv7EM2x+LtO5vKrzkl+HSPTcc2llqhv/yNxkLRm7/RWCh2nrGnOqcmpYxfT+pH\nNG4mMwxRuRnRICmw0zjy5LrlllvWZUpySUox9+BKOkjBPOh8nsKi+7jIO9LbzHboPN1BnoFkxus0\n2cs+rlwXMkt2eu/903tBCrpK4VclTJUufp6utPQ5OMjmhDwP83l5gBvyWKxEB3p/K/SXv9FYKHrz\nNxoLxU5pv3SBQpFHGAVISGpEXncUT4285KhPR6VNphh6ZK565513rstOE2mejhyj/53EIqf6bq7q\nc3Da61pjCviR83BK7bSfAptQMk2y4SARqAL1SZmE6ISB1jSv00kSvWckOlGsRG9zWz4LSrDq9iR5\nnRLWlu1uqxARd0bEpyLiixHxhYh47+p6J+tsNA4wZmj/WUm/NMZ4laQ3SPqFiHiVOllno3GgMRO6\n+yFJD63K346I+yTdrktI1nn+/Pkyzz1pXqsyeQM6vXLaSyGiiXYTbUotL9E7KnufL3nJhZSGpB2m\ncOAJ0tKTVt2ppq85xcKrAmdQxhiizuQF52Vqx8eb/ZJhF3ky0vwdLia5Br+agz9Pf25E4+lUh4y1\nqvlR4BF/Pm4clHOmk5EK+1L4rbL1vlbSp9XJOhuNA41phV9EvEjSH0n6xTHGtzYcczBZpyfq3K+T\nQ6PRuHKY2vwRcY32Nv6Hxxh/vLo8lazTE3Vef/31I22XZ2zLXQua153qzIR0JgpOGlyiz3mdvNGc\nGpPBkV8nsaPy8CPDHsrq4vTStfpOmd2Ihe7NsZN4QWtImneyf6dEpQl/VyjkO53ekJ8FeQf6+1W1\n5zTe180x8156ny52pHeq9+nr4++fjzXHQmOqMKPtD0m/Lem+Mcav2586WWejcYAx8+V/k6R/Kulv\nIuKvV9d+VXvJOT+yStz5dUnvvDJDbDQaVwIz2v7/LonSgOwrWefRo0fXNstkl06a4qRbFJ+OKHUl\nOmyiMmyhsmtmiUYSHXf4uGi8VbhyCovtcLvwbcEfJM45X4WRdgpOpy0OSkLqWmtHdZpBrtV0SkTt\nzcQwzDlRUBeap68niTfVqcJmm3kvuUuTuJKiVtv2NxqNrejN32gsFDuP5JNUhbTwZHCS17eF1pY4\nVhvlTSfDCG8naZWPlWL/EdUnQ5DKjdX7qtxsN9urshttXvcTCdJgVzb6JN44XMNPpzcUPYeMspLu\nenxGx8ya+zP3taUoSJWoRSCNPJ18kGGXi2k5RjoWp9OwfC4k2lToL3+jsVD05m80Foqd0v7z58+v\nadBM0M7KWITcHyl7itevDFg2QTb6qSkngxsy/iCjnBmtcZUrnkQUn48b8FBiR2qnEsco/DYZ/Dhm\n6CvZ9ucauWELrWdl8LJZh7ItVacZdKpALrpefuKJJ9ZlEnW8XGUS8n78GVJC2hwv+TJU6C9/o7FQ\n9OZvNBaKnUfyScwY3FTZY/w+p0OkVXbMuODuh47RaQPRfoqbT8kxq3k4dSd669rj2267bV12Dbev\nLUUBSirpFNjLLkYRTXaQJpqMXyoKS9puj2rj8yGbfwoymu8URfIhcZFESl/PEycuxLtxnwt/L6oA\nt3R64Ot5KU5z/eVvNBaK3vyNxkKxU9ofEaXhCtnTb94rMb0hm2cHxUp32kmutlXe9hljHsozMGMv\nnrTORQSq6xT4ZS972br8ile8Yl12ceDxxx9fl52aPvnkk8/ZNsXE9zb82Tq9JZHOxZEqESkZfrmG\nn4KZ0jPyNa1EA3oPKQmor5EbU9HphL8XbsSU7ZMxFfmB5IlI2/Y3Go2t2HnGnlRY0ZnrNm8qSu3t\nXwxvu0p8KbHyic6xs03/O5m9+jk7KQIptmAVI5D+7uvj2Xg8VuCrX/3qddm/SKdPn16XnQVQAs0E\nMSz/2vraUvprSj5ZZR4iBaIzNu+T2JGP3Z9F5Snp60BBUMiMnJ5tpczbrJ/w9fE9QQk80w7Fn8M2\n9Je/0VgoevM3GgvFzhV+VRYap7JOTZ3uJJ1xqklnuHT+SfnZHd5+dRZPnmRel2LIOei83pGKMA/C\nQWGsvY5nCbr55pvXZRcHnNL7dTelTepLdgBuxup1SDRzxZavkY/dlX9VFhqitS4C+DOvAmVsXnf6\nnNdnQoETBff3jBKI+rvrIkaKFbQnvH9X4OZzoaAmFfrL32gsFL35G42F4qol6nQ6SB5WVUYWpz1k\nOutwyuiU0imbt0PXs0xeYtQGZbIh01yne0mTb7311vW1kydPqoJTZ6f6VUASiRNrVlSSzFuJjvuz\nIBHMafI2z7+nnnpqfY2y/pCHpb9DFF7dkWPx8fm6uXhRiQvPBR+XnxpU3pZkN+Fr5XPL578fM9+Z\n0N3XRcT/iIj/tUrU+W9X1ztRZ6NxgDFD+38g6c1jjNdIukfS2yLiDepEnY3GgcZM6O4hKdWR16z+\nG7qERJ1jjDKO2X7Me51eOe2bSWBJRjlO5Vxr73Qs+3UaTYZFZMZJYaz9ulP2pHKksXftuc/fvcco\nexDFnKvWkag7nbw4yLCqSkJK9UlEIRGRngWZXVcGX5S5yeH901pQxiTy6ss2yYCJxJ7sn06xKkwp\n/CLi6Cphx6OSPjnG6ESdjcYBx9TmH2OcG2PcI+kOSa+PiL+78fehPTbwLETEeyLisxHx2f1EFm00\nGlcW+9L2jzGeiohPSXqbLiFR5/Hjx0fSGQqmsC07C9lT+3Wimq41pkSJZHyUIoC3V+WSl9gum4x5\nSEzJPl0D71p9p47eBgWiIPt7n7+Ptwrm4X/3Obs3oNNY0lrPxP/LZ0QfDYr9R6HQHV6/CoFNxl6U\nnJS0/V7/1KlT67Jr+yuPRBfL3AiITsaq7ErbMKPtPxURN63KL5R0r6QvqRN1NhoHGjNf/pdK+t2I\nOKq9H4uPjDE+HhF/pU7U2WgcWMxo+/+3pNcW1x/XPhN1nj9/vgy0QZrSyoiHjGlcC0xx5shYg7TQ\nFd3y+0irTDHvnHY71SNambTa3XX9BMLbdhrphkBOaWcSklZ0nLT9Xvb1JzHKn7OLMkSlE3RKQvDn\n7PeSgYzPL98z8g8gV3Q6VXBUz3az/1w7n6evJ2nz6UTiudDmvY3GQtGbv9FYKK5a6O4ZLXjl3ksn\nAxSxx6+7RtpdV512VZpf6QINIxdhL8+cZFB478pG3415SLxxzb/Pwec/I3Y4cl3Ib8LbpvDbTvW9\nPiWtrEQMiqvo8yGq7+8ZhWWvjHV8zuR+PZNJyZ8XJe2sfFjIXd1F0Spp7IyPQaK//I3GQtGbv9FY\nKHZK+8cYa+pDgR0pq02Wnda4WEBtUIafKmijxAlEU5tORkiEymhmsx23xXdNfZbJFZlEDQpd7gY3\nVN/XK6mp1/U+PUuOj4soMM2ffDRy/Z2u+3N20Y0Cr/p4fVzev/dZaeopRLdf97WlDDs+dl+7KsIU\nvTfkRpzXO5JPo9HYit78jcZCcdUi+ZBrLmk5k9YQXSeN9UzGlpl4+lnHKTIZbVCgRooV70Y8Xk66\nv832XWJNstNHP+146KGH1mWP2+91ksp6G5QfnmLLz7j9+ro4fc42ve2ZSEpkwER+IVWWIDeamknU\n6XU8kxAF8KQ2q1MQn7Of6lTGWZfdpbfRaBw+9OZvNBaKqxa3n1wwXbNdJagkN0qysybbfjLE2RZM\n0imaU30fN1E6p/pOK8m90w06EqSlJrt1p6BO7x944IF1+eGHH16X3VgnjWj8ms+HNNIuGlBiSaLJ\n25JskqGOr+2MOODwdyTLNG4y7KH3kgKekut4tjNzqlSJpW3k02g0tqI3f6OxUOyU9p87d25NZZ2e\nOH2iCDtJffw+SgtFRhZ+nWzbKUBlglJ+UeQXL5M/gxt8ON3LsVMQSKLAPhYXHbwfT7XlGnavn+34\n371Macn8+kxOBMpCXNF+p/fej78r9G5RhmXvM58XiTcu3s0EnvU5k6hZnZrMuItXIkhr+xuNxlbs\nXOGXv/JkMklZYPIXlL68/stP5/YO+kXedl5MykQfKwWC8C+PK7n8a+LX88zdFYIeBIMSX1JwDsp2\nREqsVNz5POmrTgosOi+ndam8M/0+MgWmrz15G3o7jlTiEsNwENsks2tSxlVffu/T19DXyudZmb9v\nQ3/5G42Fojd/o7FQ7JT2Hz169CKqWoFMMJNiUhJGUr4QNSVTS1IQVn93VDYJEnt++RjdO83rJAX1\nNfP+va6LTrSGtC5ur1CJTzPZdXw9yfONQNlzKiWaiyAUcnwmPqGLWtXaUfhzmg+ZNFOQGW+zsoug\nACb0Ds+s8yamv/yrrD2fj4iPr/7diTobjQOM/dD+90q6z/7diTobjQOMKdofEXdI+ieSfk3Sv1xd\n3neizmPHjq0DV5CmljziUpvutJfO/MnzyjXlfnbuIA+2HK/343/383G/7mWK/+bw+jkPOr1wCkwZ\ni+i0g8SbSmSZCdftsQcdM9Tc+69EBtKYext+2uJ1yOyaxKTKnoQ8Gen0gpJwepk8L/OdJ1HLxYjq\nVGM/9H/2y/8bkn5ZkrfciTobjQOMmXRdb5f06Bjjc1RnNlEnfZEbjcbuMUP73yTpHRHxU5Kuk/Ti\niPg9XUKizhtuuGEkhXI6RokYK20vaeZnEhQ6lSLDIkcVZIKMjJx2kqnnzPUq4IWvj5voVkYem/XJ\nmIkCgVSeehSHjk5MKFHmTBaaiuKTN6bf53SYrpPxlbeZz5wMyCiACxkzUTJVXzsSK6r7KOR59nlZ\njXzGGO8bY9wxxrhL0rsk/cUY42fUiTobjQON52Pk835J90bE/ZLeuvp3o9E4INiXkc8Y4y+1p9W/\npESdY4w1rXTtuFNDN1yp6N627DLSxTSJjCyqLC2b43I7+2p8FMCCNMwzmWSq+jNhtskQhE4hKGNM\nRRvJe5HWzedDGnkStXyuOV6KFUjek3Ri5CDjnypuIJ1MUBJQN8oij0USzfK5VAFGNvv0/XFFaH+j\n0Tic6M3faCwUPxKhu8mwprLzJiMLp31O9VwLS7nqydW2ygXvfbpNPiWhJBpGRiYVTSRaTqck5PNA\nWuVtIgC5tFbrI11M6cl1e0aUSJCvAok6Tq8pniKJXdW4KyOgTVBAGBq7P+cqyAi5X9Pa5nrOnHqt\n252u2Wg0DhV68zcaC8XOXXrT6MEpI0WVIaOMhFM6yltPNI1oJ/kIJH2jeG4+B9L2k+bX67j4kH1S\nTEICRSYikYFoZV6fSapJ/gczGZO2af6J3hOl9nlS5CGHjzffT1qfGTHGQbENyecjRUnS9tPzzOtX\nwra/0WgcMvTmbzQWip1r+5MeOTV2WkM22lUCRaJ3RG+Jjjrt9Ppuu520isbtFNDtub0+hRqngJfZ\np9uW03z8Omme6XTA51/Z5ZO4Qi7V5H9BxjJEVfO6j5t8K2ZEPRpjJWp6P0Td/T4fI52CkJFTdQrh\nz5zEuCpLURv5NBqNrejN32gsFDul/WOMNT0iLbhT6Ypi+jWnQESHSQQgWuVUypGaZXLXdFAkFzKW\nIRGkosOU3YjWgoypqM0qFj1pvmkO/gwpIOdMxqYEBQQlsYs04pXr7uYYqzlT1iUHzZnWblvSWBJR\n/X2qjMPo+VToL3+jsVD05m80FoqrRvtnYo5X7p3kCjwTzNFBhj2unXfkeJ1qUbomAmmN/V53Gc75\nkyhAOQkcFKiTjIx8jDkWF4WcglI0JKK9ZBRDRjw5FnKFpjn7ulCfJBrmevk1MjyjqDqUkJXWq4rk\n5P37+vsJVJVboLX9jUZjK3rzNxoLxU5p/5EjR9aacDJcobzlSZ+q6CUS52EnzTe5+hKtrrILk8aW\n3Ghn/Az83nQvJj8Ij5VP7soOp6CUOqrKCzAjdszkCqB0WeRene3QyQxRepqb1/d3oWqffAL8WdC7\nSD4Hfq+XfezVqZLD180jBlUZjbehv/yNxkKx0y//+fPn118zipG2DVU47c02yAuLznZnwlFXsdXI\ntsC/JM4w/Jfavzzepn8Rcn7+5fVfflpD8qQjk1oy363aozN0CmAyY4JNisj8slaKX2lO+UfxDx2V\nyazfR2HZiWGQiXoVE1K6eC2qwCLkveoMI98zYpcVZtN1fU3StyWdk3R2jPG6iLhZ0h9KukvS1yS9\nc4zx5HTPjUbjqmI/tP8fjjHuGWO8bvXvTtTZaBxgPB/av+9EnWfPntVTTz211zGc7Tqqs2CnRTOK\nPUdFqXJcCafpFZV2ikhJMykUt9NrQiW+EC33MpnXOigWHcWLy/pE0UnscHGMnq236WOplIWkcKzE\nMomVaQ6i79kOKZD9uq+nU31SZno/lMA057rN9mFzXFcyht+Q9F8i4nMR8Z7VtU7U2WgcYMx++X9i\njPFgRNwq6ZMR8SX/4xhjRAQm6pT0HokTbjQajd1javOPMR5c/f/RiPiYpNfrEhJ1Hj9+fCSFJU8u\np0yVh52Hy3Y4NZ7RdjtmQkMnHfY2KPc6eZg5TasCMWwi65CJKIkXFCiCzq79WTh9TpsMSgJKZbLh\nuPHGG8uxOCp7iZkYgj4HNwGne73/KsgGrRvR7lOnTq3L/g6TyOCafy+7+W41H0rwmeI02XhUmEnR\nfTwibsiypH8k6W/ViTobjQONmS//SyR9bKWgOSbpP44xPhERn5H0kYj4eUlfl/TOKzfMRqNxubF1\n848xvirpNcX1fSfqjIg1hSStudMap5upHfZrTq88bz21TSa4M+aoScGdFpO4QHSU2qZx5Zy9H+rT\nqSHRW4r553OqNPi33nrr+prTUu/TqTPRZDKNprh82Rdp72fMlf1UwcUOP5Gonr+vM5lxk+jic3YR\nyPuktUv4uz0T7/BS0Oa9jcZC0Zu/0Vgodmrb77TfaQ1pkyvNrtMxp04O8h4jYxFqpwrc4e0RXXca\nTWGsyefggrCeAAAO/UlEQVSgOqkgeumUkk5B3J+AgoZQFqCk3T4mH4uLADP2/w7KQlSdjmzLUrMJ\nMqyi0xEXGfP50nvo8PejyrqzWXZ6TwFKnnxyz0LeDZ/olMTLlyIO9Je/0VgoevM3GgvFzhN13nzz\nzZJYw+tU1mlS0jA3iHC6SG6RFAqcKCPR1Ko+aWTJbp6CfFB46SzTfS7GUDYYr+8igINcXfOEg+LN\nOe319fFnRElAfV3IKKtKPulrSwY/dGJEAT/ynZQurCO9H942zY3GSKJRFTTGx0qGb5U4th+X3v7y\nNxoLRW/+RmOh2CntP3bsmG655RZJbAjjtN+pVNouO12kvOV+H1FTCu/s5SopKIkrPgenZiR2kDts\nFVLb+6RMNxSK2uktnSqQUU626W1TyHMft685iTQzmu+Kynp73g+dtrioQ+04ss8ZG3kSO7x/r0On\nA5VLs5+keBv+PD2GY77PTfsbjcZW9OZvNBaKq5axx0HU2Cl+0k2nS06viTqTdpYiuTjdchGgisLj\n4yM31moOmyB79RQ7nC5S4E3KNORrRBljnPZXpxA+PqfrFMabgmxShiGiyZVhF4k9MxGWKAlrda+v\njz9bas+fBfkzkI+GG5nlvSRGkQ9D5TOzDf3lbzQWit78jcZCsVPaf/bsWT322GOSOCikU6AqXjnR\nO4dTJopqQ8EsvU3vK+k45Q1Im2xJ6zlKrDV2ykh0dLPvTVTx5iWOIU8ZdrZFPpqJBkQnCXQKQdru\nKiMPzY0i3Hg/JIJRoNasTzb5M67gdDpCPgcnT55cl1PEoHwHtFZVjoNt6C9/o7FQ9OZvNBaKndL+\nc+fOrekxJcckV8o0aCCNvYO052TzTZFfKjrsdutPPPHEuuxU30UAouAk9lQuw+QfQNTZ50aGRa61\nphRl2T+JSLT+Xoeep7dJiUirpJkUyYjEu5k0btW6eBtkTEYGaWRMRu64/ryqoK2U/qxydSbjpQr9\n5W80Foqde/XlF5y+YFXcPKlOQUwebt52hp/eBHnhEbPIvvwrXGVMkfhc1r3H6AvmyLH4PP0rSea1\nZEZLptEUoCMZASWK9OdGHoYOZxiuLKVgLtkv2Uc4qA6ZLlP9XMcZr08626f6DrIhqc75qa6vVT4L\nUkJWmPryR8RNEfHRiPhSRNwXEW+MiJsj4pMRcf/q/yeme200Glcds7T/NyV9YozxY9qL5HufOlFn\no3GgsZX2R8SNkv6BpH8mSWOMH0r6YUTsO1Hn0aNHdeLEHkFw2pkeexKfRScNIjPeGY8tUvi4eEGe\nWlmuAi9stpGei9LFtNNjsVEyR1coOjWu2quSakp8zk1KMfLwy/Yp33zlgbjZHpnJ+tpVWWq8L2+P\nQo6TfYiLOjReR/bldd0mgOZPdg6kTKU1yvZnxkom1bOY+fK/XNIZSb8TEZ+PiA+uMvd0os5G4wBj\nZvMfk/Tjkn5rjPFaSU9rg+KPvZ8rTNQZEZ+NiM9S6uhGo7F7zGj7T0s6Pcb49OrfH9Xe5t93os4T\nJ06M/AGgjDV0pl1RH6fOpHl1euVaaAqsMBO4omrDy04TPeACnVRQsIasT+fglI2HzGFp7ERfqwAR\nZJPhlN4puJvJ0qmKn8hUZq0+t8cff3xdptMLf25k80Dmzdmmj8PfMxIp6R0i02kSU/I9pmSvJC5m\nncvq1TfGeFjSAxFx9+rSWyR9UZ2os9E40Jg95/8Xkj4cEddK+qqkn9PeD0cn6mw0DiimNv8Y468l\nva74074SdZ47d25NA4kyuslslXnHY7I5vXPa71TL26YsPXRSUBlukLkoBXYgY439GJxQyGfHTPYa\np9eU7aXKauRrSPf5ejo193nS/J2yu8iU1Nz7d5ABE1Fwr3PmzJlyXBX8naCTDwcFM6GMUY58d1ws\n8VMiX5/qJKWDeTQaja3ozd9oLBQ7j+GXlMyp3EMPPbQuU+aTpENO3cgz0Os4HaWQ1pSo0pHjqgwy\npIsDMnjbFNuPgoZUxhoU741i6FEADW+HDGEqfwaKd0hGVk5Taf0pz733lbS+0mpLtR+C37fZNnlB\nOrI+nZJQZiCHv5fk+Vd58nmZTpIoA1GeKnXo7kajsRW9+RuNhWKntP+ZZ55ZU3zSznrZaV3SPaKI\npKUnakyGE07NnLImlaRElZ7jnTTiZORC8dpyLZxeurbX4RpzN6xxCkohuH1cfj3n5GPy0xYaF7ka\nz2QJqspk7OVjoexJ7h/hY6QQ6NUJi4N8K8j4x98tp+wOv55tev/+HroYUYl3HcOv0WhsRW/+RmOh\n2Dntf/DBByVdTHW87NTMKVNGwSFXR8ok49TcKTDRMadYVXQg8glwqukigM+HYrg5KsMRH5/3T0Yj\nRA2pTUdlODMTOppOBBwk3lSu016HjLBcLPT7fP5+8kDGP+QmXsHXp6Lrm2V6F6tTFYeLxe7yTsh3\nYebkaj3O6ZqNRuNQoTd/o7FQ7JT2HzlyZE23KTSxu8Defvvt63JqZP0+12qT+y0ZgpCxCkWVyXvd\nbp2SKVKyUQpBTbQ326HQ4i66OI3MaEkSB+okAxVfxxRZnLqSFpoShZJ7Kz0X8vmo6tKpAhlFOeh0\nqIqk43/35+n0flsGIOlikYGyNGV9ilhEIkWOu7X9jUZjK3rzNxoLxc5pf1IycpN02u9UKumOZ8Oh\nWP3bXDQ3+5/JJ58gqk0gAxaKKlO1STkBKMjkjBsvuaZ6O0m7aX3IsKWKJ7/Zv1NgEseyfcouNJOo\nlTTslDchnzmJOqTVJ/HC4fPwsfszqt45f5/phCfXql16G43GVvTmbzQWip3SfukC9SO7cEo1lXTG\nqRPFZKeTBNKgkoa0irDjFGwmmCO1TRSwMhzalr9e4vwAZGRCfW47+fBnQn4QfgpDfg6en8DvdQpc\nBXt1gxcSdfzd8pMPSo5aGfbQ8yEtvb+LvkZ+r8/D2/fnmM/InyG5a3sb+a407W80GlvRm7/RWChm\n0nXdLekP7dIrJP0bSf9hdf0uSV+T9M4xxpOb9zuOHDlSGpdQDH+nhpWro1Mgp46k7Xdq5iJAFTHI\n+/Syt+Gaf6eDlEaKXHpJg5vXt0Wd2QSl8doPJfR7fZ1dG+/03q+7uODPkDIc+/MiESDhGnsXASjI\npZ8eOe0no7Aqkg9FWiLNv7sR+7vg8/Hn7OuY7fiakFt2JQ5c7rj9Xx5j3DPGuEfS35P0XUkfUyfq\nbDQONPar8HuLpP87xvj6pSTqJJB3XKWIIk+yykRT4ph72xQ+BP+SktLSf+G9vn+pKLZelcmHgpNQ\nrD46o/a1mMn8k+f81DbZOTgL8DWi7D2U2DKv+/hI4eplr0NeoOTVV6WUo/n7fJyxUKxGSldXeZUS\nY9uPB+I27Ffmf5ek31+VO1Fno3GAMb35V9l63iHpP23+bTZR54xFXKPR2A32Q/v/saT/OcZ4ZPXv\nfSfqvO6660ZSIgoQQWf0WXZF0UysOPJqc6XUDMWqAluQ2aUrAingBFHJ6lyePMwoIIp7w3lgER+X\n9+NjrOwsyJONRAAKb33bbbety96mt+Pn8pXCj2L/0ZqTeS9l0snrrrQj6u5U30W6GZGKkOvy8MMP\nr6+RPUMlAlDswQr7of3v1gXKL3WizkbjQGNq80fEcUn3Svpju/x+SfdGxP2S3rr6d6PROCCYTdT5\ntKRbNq49rn0m6jx//vyaYlH+cULSITJ1dTrk1Iw0wnSeT+aoKa441XPa5efJfrbttNNpbJWZZnNc\nFZUjbzSnrpTDnrLAEK1PKksZfegsnLwNvezz9LKvUZb9XSGPQUqI6mN0Kk9zyneHxBKHz5nG6PYf\nPjcfr7+v+S74u+r9+FpVJsVVPEBCW/g1GgtFb/5GY6HYqVff0aNH11lWiKaRZrUyZfVrTntJC+/w\n606BKVd9Ukyi6z4Wp7dOzapAFc/VfxVPjjIQUQ57F0G8jmvHXWSpkpxS6GifD52CUGANz7ZDRkFV\ngAo61aH3pgpFvlkmo6jNcWyWfQ50CuJr62Wn5x6gpjLcIUMtR55wdKLORqOxFb35G42FYqe0f4xR\nZr4hIxunT0nTnFLOZEMhykShq8kWvfKaquzwN+H1nY6SwU01f6L9MwYdTsfd4Me10H6CUZ12+LpR\n9hzyOaCQ3g5vs/Jaq4JWbPZPtJ8MkaqTHO+TjGnolIisV0m82abBJ49NSjC6n5Dd6zHs+45Go3Eo\n0Ju/0VgoYsbe+LJ1FnFG0tOSHttZp1cPJ9XzPEw4KPP8O2OMUzMVd7r5JSkiPjvGeN1OO70K6Hke\nLhzGeTbtbzQWit78jcZCcTU2/weuQp9XAz3Pw4VDN8+dy/yNRuNHA037G42FYqebPyLeFhFfjoiv\nRMShCfUdEXdGxKci4osR8YWIeO/q+s0R8cmIuH/1/xPb2vpRR0QcjYjPR8THV/8+dHOUpIi4KSI+\nGhFfioj7IuKNh22uO9v8EXFU0r/TXizAV0l6d0S8alf9X2GclfRLY4xXSXqDpF9Yze0w5jZ4r6T7\n7N+HcY6S9JuSPjHG+DFJr9HenA/XXMcYO/lP0hsl/Zn9+32S3rer/nf5n/biGd4r6cuSXrq69lJJ\nX77aY3ue87pDey/9myV9fHXtUM1xNY8bJf0/rXRidv1QzXWXtP92SQ/Yv0+vrh0qRMRdkl4r6dM6\nfLkNfkPSL0tyL5LDNkdJermkM5J+ZyXifHAVx/JQzbUVfpcREfEiSX8k6RfHGN/yv429z8WBPVqJ\niLdLenSM8Tmqc9DnaDgm6ccl/dYY47XaM0m/iOIfhrnucvM/KOlO+/cdq2uHAhFxjfY2/ofHGBnl\n+JFVTgM9V26DA4I3SXpHRHxN0h9IenNE/J4O1xwTpyWdHmN8evXvj2rvx+BQzXWXm/8zkl4ZES9f\nZf95l/Zi/x94xJ5j+W9Lum+M8ev2p0OT22CM8b4xxh1jjLu09+z+YozxMzpEc0yMMR6W9MAqQ7W0\nF6X6izpkc921V99PaU9uPCrpQ2OMX9tZ51cQEfETkv6bpL/RBXn4V7Un939E0sskfV17acyfuCqD\nvIyIiJ+U9K/GGG+PiFt0OOd4j6QPSrpW0lcl/Zz2PpaHZq5t4ddoLBSt8Gs0Fore/I3GQtGbv9FY\nKHrzNxoLRW/+RmOh6M3faCwUvfkbjYWiN3+jsVD8fybBLbwbrm5RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a63278a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCaKevkLhYmiXv5CYaKol79QmCj+P9iPDg0s93lOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a63262ab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb5qV0g9/06yU/w98vSRBiq4YQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6328247b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1250\n",
    "\n",
    "hh_image, hv_image =  get_image_channels(X_train_initial, index)\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "print(y_train_initial[index])\n",
    "\n",
    "import cv2\n",
    "\n",
    "hh_image = cv2.bilateralFilter(hh_image.astype(np.float32), 5, 80, 80)\n",
    "hv_image = cv2.bilateralFilter(hv_image.astype(np.float32), 5, 80, 80)\n",
    "\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "\n",
    "image = np.dstack((hh_image, hh_image, np.zeros_like(hv_image)))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "X_flat_initial, y_flat_initial, _ = prepare_flat_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "print('Training dataset size: {}'.format(len(X_flat_initial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat_initial, y_flat_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 1176 samples.\n",
      "{'train_time': 2.3636345863342285, 'log_loss_train': 9.9920072216264148e-16, 'pred_time': 0.011500120162963867, 'acc_train': 1.0, 'log_loss_test': 6.9078393044748019, 'acc_test': 0.80000000000000004}\n",
      "AdaBoostClassifier trained on 1176 samples.\n",
      "{'train_time': 56.3687481880188, 'log_loss_train': 1.3815643824202646, 'pred_time': 0.19403505325317383, 'acc_train': 0.95999999999999996, 'log_loss_test': 8.3128281638591055, 'acc_test': 0.7593220338983051}\n",
      "SVC trained on 1176 samples.\n",
      "{'train_time': 12.975327014923096, 'log_loss_train': 8.9802097982656104, 'pred_time': 6.013159990310669, 'acc_train': 0.73999999999999999, 'log_loss_test': 8.898258258600988, 'acc_test': 0.74237288135593216}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "clf1= LogisticRegression(random_state = 3)\n",
    "clf2 = AdaBoostClassifier(random_state = 3)\n",
    "clf3 = SVC(random_state = 3)\n",
    "clf4 = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    start = time() # Get start time\n",
    "  \n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    X_train_pred = X_train[:300]\n",
    "    y_train_pred = y_train[:300]\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train_pred)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train_pred, predictions_train)\n",
    "    results['log_loss_train'] = log_loss(y_train_pred, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    results['log_loss_test'] = log_loss(y_test, predictions_test)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, len(X_train)))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "print(train_predict(clf1, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf2, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf3, X_train, y_train, X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8e87bc4f6966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# TODO: Fit the grid search object to the training data and find the optimal parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrid_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Get the estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 for train, test in cv)\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1675\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "clf_default = AdaBoostClassifier(random_state = 3, base_estimator = dtc) \n",
    "clf = AdaBoostClassifier(random_state = 3, base_estimator = dtc)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = { 'n_estimators': [5, 10, 20, 50, 100], \\\n",
    "              # 'base_estimator__criterion' : ['gini', 'entropy'], \\\n",
    "               'base_estimator__max_depth': [2, 3, 5, 10, None] \\\n",
    "             #  'base_estimator__min_samples_split': [2, 5, 10, 50], \\\n",
    "             #  'base_estimator__class_weight': [{ 1 : 0.75}, { 1 : 1}] \n",
    "             } \n",
    "\n",
    "scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring = scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_default' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-78de5f13ba73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdef_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_default\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_default' is not defined"
     ]
    }
   ],
   "source": [
    "def_model = clf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = def_model.predict(X_valid)\n",
    "best_predictions = best_clf.predict(X_valid)\n",
    "\n",
    "print (\"Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\n",
    "print (\"log_loss on testing data: {:.4f}\".format(log_loss(y_valid, predictions)))\n",
    "\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\n",
    "print (\"Final log_loss on the testing data: {:.4f}\".format(log_loss(y_valid, best_predictions)))\n",
    "                                                                     \n",
    "print (grid_fit.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 935\n",
      "Final validation dataset size: 462\n",
      "Final tet dataset size: 74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_train_initial, y_train_initial, test_size=0.05, random_state=142)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_1, y_train_1, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "print('Final tet dataset size: {}'.format(len(X_test_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range = 0.05, height_shift_range = 0.05, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#X_train_new = np.copy(X_train)\n",
    "#Y_train_new = np.copy(y_train)\n",
    "\n",
    "#for i in range(40):\n",
    "#    X_batch, y_batch =  next(datagen.flow(X_train, y_train, batch_size=2000))\n",
    "#    if i == 0:\n",
    "#        X_train_new = X_batch\n",
    "#        Y_train_new = y_batch\n",
    "#    else: \n",
    "#        X_train_new = np.concatenate((X_train_new, X_batch), axis=0)\n",
    "#        Y_train_new = np.concatenate((Y_train_new, y_batch), axis=0)\n",
    "    \n",
    "    #plt.imshow(X_batch[24, :, :, 0])\n",
    "    #plt.show()\n",
    "                                    \n",
    "    \n",
    "#print(X_batch.shape)\n",
    "\n",
    "\n",
    "#print (X_batch.shape)\n",
    "##X_train = cut_image_part(X_train_new, 10)\n",
    "#print (X_train_new.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_initial, y_train_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "\n",
    "features_train = []\n",
    "features_valid = []\n",
    "\n",
    "for x in X_train:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_train.append(flat)\n",
    "\n",
    "for x in X_valid:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_valid.append(flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(random_state = 3)\n",
    "lrc = SVC(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(features_train, y_train)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(features_valid)\n",
    "predictions_train = lrc.predict(features_train)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_valid, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_valid, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(lrc.__class__.__name__, len(features_train)))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "entities_count = 20\n",
    "\n",
    "def getModel10(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "def getModel11(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "  \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    " \n",
    "\n",
    "\n",
    "def getModel20(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def getTransferLearningModel():\n",
    "    activation = 'elu'\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    output = base_model.output\n",
    "    \n",
    "    layer = GlobalMaxPooling2D()(output)\n",
    "    layer = Dense(512, activation='relu', name='fc2')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(512, activation='relu', name='fc3')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    predictions = Dense(1, activation='sigmoid')(layer)\n",
    "   \n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel30(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel32(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    " \n",
    "\n",
    "    \n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel31(num_layers):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel40(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False,\n",
    "                     kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def getModel50(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1728      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 64)        36864     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 38, 38, 128)       73728     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 38, 38, 128)       147456    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 19, 19, 256)       294912    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 19, 19, 256)       589824    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 512)       2359296   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              26216448  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 31,949,505\n",
      "Trainable params: 31,949,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel11(3)\n",
    "#model = getTransferLearningModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "\n",
    "def init_model(model, file_name):\n",
    "    checkpointer = ModelCheckpoint(filepath=file_name, verbose=1, save_best_only=True)\n",
    "    \n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "    optimizer_small = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "    optimizer2 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model, checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1323\n",
      "Final test dataset size: 148\n",
      "Fold: 0\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 2s - loss: 0.7084 - acc: 0.4722Epoch 00000: val_loss improved from inf to 0.69315, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 66s - loss: 0.7071 - acc: 0.4735 - val_loss: 0.6932 - val_acc: 0.4887\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7608 - acc: 0.5199Epoch 00001: val_loss improved from 0.69315 to 0.69089, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.7570 - acc: 0.5176 - val_loss: 0.6909 - val_acc: 0.5113\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.4962Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6924 - acc: 0.4989 - val_loss: 0.6937 - val_acc: 0.4887\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.4841Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6936 - acc: 0.4870 - val_loss: 0.6931 - val_acc: 0.4887\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5043Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5060 - val_loss: 0.6920 - val_acc: 0.5714\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6687 - acc: 0.5972Epoch 00005: val_loss improved from 0.69089 to 0.62171, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6741 - acc: 0.6008 - val_loss: 0.6217 - val_acc: 0.6466\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.6437Epoch 00006: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6440 - acc: 0.6409 - val_loss: 0.6303 - val_acc: 0.6316\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6429 - acc: 0.6348Epoch 00007: val_loss improved from 0.62171 to 0.61499, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 3s - loss: 0.6398 - acc: 0.6394 - val_loss: 0.6150 - val_acc: 0.6090\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.6428Epoch 00008: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6542 - acc: 0.6406 - val_loss: 0.6327 - val_acc: 0.6241\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6312 - acc: 0.6465Epoch 00009: val_loss improved from 0.61499 to 0.60661, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6310 - acc: 0.6428 - val_loss: 0.6066 - val_acc: 0.6466\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6123 - acc: 0.6562Epoch 00010: val_loss improved from 0.60661 to 0.60240, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6152 - acc: 0.6516 - val_loss: 0.6024 - val_acc: 0.6466\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6222 - acc: 0.6446Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6278 - acc: 0.6332 - val_loss: 0.6288 - val_acc: 0.6391\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6105 - acc: 0.6572Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6213 - acc: 0.6478 - val_loss: 0.6180 - val_acc: 0.6466\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.6553Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6078 - acc: 0.6548 - val_loss: 0.6149 - val_acc: 0.6617\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6095 - acc: 0.6478Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6074 - acc: 0.6500 - val_loss: 0.6171 - val_acc: 0.6617\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6290 - acc: 0.6316Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6289 - acc: 0.6333 - val_loss: 0.6148 - val_acc: 0.6391\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.6735Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6116 - acc: 0.6762 - val_loss: 0.6423 - val_acc: 0.6842\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.6813Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6081 - acc: 0.6848 - val_loss: 0.6133 - val_acc: 0.6617\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.6833Epoch 00018: val_loss improved from 0.60240 to 0.59335, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5987 - acc: 0.6872 - val_loss: 0.5934 - val_acc: 0.6992\n",
      "Epoch 20/150\n",
      " 4/10 [==========>...................] - ETA: 2s - loss: 0.6051 - acc: 0.6767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.178531). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.355059). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.6887Epoch 00019: val_loss improved from 0.59335 to 0.57924, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.6032 - acc: 0.6910 - val_loss: 0.5792 - val_acc: 0.6992\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.6961Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5922 - acc: 0.6938 - val_loss: 0.6066 - val_acc: 0.6767\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.6880Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5907 - acc: 0.6901 - val_loss: 0.5889 - val_acc: 0.6767\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.7036Epoch 00022: val_loss improved from 0.57924 to 0.54292, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5764 - acc: 0.7058 - val_loss: 0.5429 - val_acc: 0.7669\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5429 - acc: 0.7297Epoch 00023: val_loss improved from 0.54292 to 0.50747, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5438 - acc: 0.7312 - val_loss: 0.5075 - val_acc: 0.8045\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5514 - acc: 0.7401Epoch 00024: val_loss improved from 0.50747 to 0.49867, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5459 - acc: 0.7446 - val_loss: 0.4987 - val_acc: 0.7744\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7672Epoch 00025: val_loss improved from 0.49867 to 0.48889, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5234 - acc: 0.7641 - val_loss: 0.4889 - val_acc: 0.7820\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.7765Epoch 00026: val_loss improved from 0.48889 to 0.44260, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5196 - acc: 0.7745 - val_loss: 0.4426 - val_acc: 0.8045\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4857 - acc: 0.7743Epoch 00027: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4840 - acc: 0.7770 - val_loss: 0.4968 - val_acc: 0.7820\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.7987Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4731 - acc: 0.7967 - val_loss: 0.4949 - val_acc: 0.7895\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.7844Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4813 - acc: 0.7939 - val_loss: 0.4616 - val_acc: 0.7970\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8144Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4528 - acc: 0.8149 - val_loss: 0.5847 - val_acc: 0.6992\n",
      "Epoch 32/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8224Epoch 00031: val_loss improved from 0.44260 to 0.40887, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4438 - acc: 0.8183 - val_loss: 0.4089 - val_acc: 0.8346\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4279 - acc: 0.8275Epoch 00032: val_loss improved from 0.40887 to 0.39435, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4247 - acc: 0.8283 - val_loss: 0.3943 - val_acc: 0.8346\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8483Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3956 - acc: 0.8472 - val_loss: 0.4147 - val_acc: 0.8195\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8305Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4271 - acc: 0.8262 - val_loss: 0.4273 - val_acc: 0.8045\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8395Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4035 - acc: 0.8367 - val_loss: 0.4097 - val_acc: 0.8195\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8481Epoch 00036: val_loss improved from 0.39435 to 0.38363, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3737 - acc: 0.8474 - val_loss: 0.3836 - val_acc: 0.8496\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8458Epoch 00037: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3781 - acc: 0.8483 - val_loss: 0.3913 - val_acc: 0.8271\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8611Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3570 - acc: 0.8609 - val_loss: 0.4232 - val_acc: 0.7970\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8331Epoch 00039: val_loss improved from 0.38363 to 0.38298, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3896 - acc: 0.8322 - val_loss: 0.3830 - val_acc: 0.8195\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8431Epoch 00040: val_loss improved from 0.38298 to 0.37070, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3714 - acc: 0.8416 - val_loss: 0.3707 - val_acc: 0.8120\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8655Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3469 - acc: 0.8622 - val_loss: 0.3776 - val_acc: 0.8195\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8641Epoch 00042: val_loss improved from 0.37070 to 0.34581, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3313 - acc: 0.8662 - val_loss: 0.3458 - val_acc: 0.8346\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8713Epoch 00043: val_loss improved from 0.34581 to 0.33589, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3125 - acc: 0.8669 - val_loss: 0.3359 - val_acc: 0.8346\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3305 - acc: 0.8594Epoch 00044: val_loss improved from 0.33589 to 0.31873, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3343 - acc: 0.8577 - val_loss: 0.3187 - val_acc: 0.8421\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8658Epoch 00045: val_loss improved from 0.31873 to 0.30952, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 3s - loss: 0.3154 - acc: 0.8691 - val_loss: 0.3095 - val_acc: 0.8421\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8676Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3050 - acc: 0.8717 - val_loss: 0.3266 - val_acc: 0.8496\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.8569Epoch 00047: val_loss improved from 0.30952 to 0.29643, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 4s - loss: 0.3130 - acc: 0.8604 - val_loss: 0.2964 - val_acc: 0.8571\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8678Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3220 - acc: 0.8679 - val_loss: 0.3489 - val_acc: 0.8045\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8556Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3158 - acc: 0.8611 - val_loss: 0.3514 - val_acc: 0.8195\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8626Epoch 00050: val_loss improved from 0.29643 to 0.29322, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3120 - acc: 0.8616 - val_loss: 0.2932 - val_acc: 0.8496\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.8683Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2809 - acc: 0.8721 - val_loss: 0.3057 - val_acc: 0.8346\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.8709Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2613 - acc: 0.8736 - val_loss: 0.3069 - val_acc: 0.8496\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.8782Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2689 - acc: 0.8752 - val_loss: 0.3960 - val_acc: 0.8346\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.8803Epoch 00054: val_loss improved from 0.29322 to 0.28514, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2686 - acc: 0.8829 - val_loss: 0.2851 - val_acc: 0.8571\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2492 - acc: 0.8874Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2521 - acc: 0.8868 - val_loss: 0.3917 - val_acc: 0.8421\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.8981Epoch 00056: val_loss improved from 0.28514 to 0.26085, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2406 - acc: 0.8977 - val_loss: 0.2609 - val_acc: 0.8571\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.8903Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2386 - acc: 0.8895 - val_loss: 0.2801 - val_acc: 0.8647\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.8942Epoch 00058: val_loss improved from 0.26085 to 0.24665, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2284 - acc: 0.8953 - val_loss: 0.2467 - val_acc: 0.8571\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.8933Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2298 - acc: 0.8894 - val_loss: 0.2480 - val_acc: 0.8947\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.8975Epoch 00060: val_loss improved from 0.24665 to 0.23785, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2157 - acc: 0.9000 - val_loss: 0.2379 - val_acc: 0.8571\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9111Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2042 - acc: 0.9105 - val_loss: 0.2633 - val_acc: 0.8797\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.8986Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2108 - acc: 0.9005 - val_loss: 0.2510 - val_acc: 0.8797\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8798Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2492 - acc: 0.8816 - val_loss: 0.2577 - val_acc: 0.8872\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9015Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2151 - acc: 0.9032 - val_loss: 0.2540 - val_acc: 0.8722\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.8827Epoch 00065: val_loss improved from 0.23785 to 0.22560, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2626 - acc: 0.8797 - val_loss: 0.2256 - val_acc: 0.8872\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.8826Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2270 - acc: 0.8865 - val_loss: 0.2469 - val_acc: 0.8872\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9155Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2034 - acc: 0.9182 - val_loss: 0.3112 - val_acc: 0.8647\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.8870Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2482 - acc: 0.8871 - val_loss: 0.3430 - val_acc: 0.8797\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.8932Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2002 - acc: 0.8988 - val_loss: 0.2588 - val_acc: 0.8722\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9085Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1886 - acc: 0.9089 - val_loss: 0.3106 - val_acc: 0.8722\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9250Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1811 - acc: 0.9271 - val_loss: 0.2886 - val_acc: 0.8647\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9113Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1783 - acc: 0.9153 - val_loss: 0.2435 - val_acc: 0.8797\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9291Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1639 - acc: 0.9291 - val_loss: 0.2890 - val_acc: 0.8872\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.8978Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2185 - acc: 0.9001 - val_loss: 0.2594 - val_acc: 0.8872\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9291Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1806 - acc: 0.9291 - val_loss: 0.2928 - val_acc: 0.8872\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9158Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1914 - acc: 0.9169 - val_loss: 0.2954 - val_acc: 0.8872\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9069Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1988 - acc: 0.9115 - val_loss: 0.2258 - val_acc: 0.8947\n",
      "Epoch 79/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.1203 - acc: 0.9542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.510239). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.255871). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9244Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1755 - acc: 0.9183 - val_loss: 0.2320 - val_acc: 0.8947\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9128Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1920 - acc: 0.9141 - val_loss: 0.2539 - val_acc: 0.8872\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9229Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1698 - acc: 0.9269 - val_loss: 0.3431 - val_acc: 0.8722\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9131Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1843 - acc: 0.9136 - val_loss: 0.2594 - val_acc: 0.8797\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9343Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1620 - acc: 0.9318 - val_loss: 0.2342 - val_acc: 0.8872\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9363Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1507 - acc: 0.9345 - val_loss: 0.2278 - val_acc: 0.9098\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9388Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1423 - acc: 0.9356 - val_loss: 0.2849 - val_acc: 0.8947\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9385Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1289 - acc: 0.9412 - val_loss: 0.3199 - val_acc: 0.8797\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9440Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1424 - acc: 0.9441 - val_loss: 0.3053 - val_acc: 0.8797\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9380Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1461 - acc: 0.9349 - val_loss: 0.3264 - val_acc: 0.8647\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9312Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1582 - acc: 0.9297 - val_loss: 0.2843 - val_acc: 0.8647\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9354Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1513 - acc: 0.9386 - val_loss: 0.3295 - val_acc: 0.8722\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9408Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1445 - acc: 0.9426 - val_loss: 0.3199 - val_acc: 0.8872\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9220Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1644 - acc: 0.9230 - val_loss: 0.2941 - val_acc: 0.8797\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9042Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2136 - acc: 0.9014 - val_loss: 0.2689 - val_acc: 0.8571\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.8882Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2295 - acc: 0.8925 - val_loss: 0.2982 - val_acc: 0.8647\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9102Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1869 - acc: 0.9127 - val_loss: 0.2714 - val_acc: 0.8722\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9322Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1435 - acc: 0.9333 - val_loss: 0.2433 - val_acc: 0.8947\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9449Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1297 - acc: 0.9460 - val_loss: 0.2982 - val_acc: 0.8722\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9364Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1602 - acc: 0.9349 - val_loss: 0.3300 - val_acc: 0.8496\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9213Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1549 - acc: 0.9233 - val_loss: 0.2914 - val_acc: 0.8872\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9533Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1161 - acc: 0.9529 - val_loss: 0.3184 - val_acc: 0.8797\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9583Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0978 - acc: 0.9592 - val_loss: 0.2536 - val_acc: 0.9023\n",
      "Epoch 102/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9622Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0986 - acc: 0.9606 - val_loss: 0.2944 - val_acc: 0.8797\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9602Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1000 - acc: 0.9571 - val_loss: 0.3249 - val_acc: 0.8571\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9574Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1037 - acc: 0.9588 - val_loss: 0.3000 - val_acc: 0.8797\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9552Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0903 - acc: 0.9542 - val_loss: 0.3428 - val_acc: 0.8797\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9583Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1007 - acc: 0.9577 - val_loss: 0.4244 - val_acc: 0.8797\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9622Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1012 - acc: 0.9599 - val_loss: 0.4183 - val_acc: 0.8872\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9403Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1416 - acc: 0.9360 - val_loss: 0.2937 - val_acc: 0.8872\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9372Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1502 - acc: 0.9356 - val_loss: 0.2508 - val_acc: 0.9023\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9544Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1009 - acc: 0.9582 - val_loss: 0.2815 - val_acc: 0.9098\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9609Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0897 - acc: 0.9614 - val_loss: 0.4383 - val_acc: 0.8797\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9575Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0953 - acc: 0.9592 - val_loss: 0.2601 - val_acc: 0.8872\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9609Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0959 - acc: 0.9621 - val_loss: 0.3267 - val_acc: 0.8947\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9641Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1040 - acc: 0.9650 - val_loss: 0.3396 - val_acc: 0.8797\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9505Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1161 - acc: 0.9520 - val_loss: 0.4387 - val_acc: 0.8722\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9638Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0930 - acc: 0.9656 - val_loss: 0.3689 - val_acc: 0.8797\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9708Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0783 - acc: 0.9720 - val_loss: 0.2891 - val_acc: 0.9098\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9632Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0877 - acc: 0.9639 - val_loss: 0.3349 - val_acc: 0.8947\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9742Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0657 - acc: 0.9750 - val_loss: 0.2787 - val_acc: 0.9098\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9680Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0837 - acc: 0.9637 - val_loss: 0.3259 - val_acc: 0.9023\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9687Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0617 - acc: 0.9718 - val_loss: 0.3085 - val_acc: 0.9098\n",
      "Epoch 122/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9752Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0565 - acc: 0.9769 - val_loss: 0.3462 - val_acc: 0.8947\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9820Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0451 - acc: 0.9829 - val_loss: 0.3877 - val_acc: 0.9098\n",
      "Epoch 124/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9695Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0757 - acc: 0.9707 - val_loss: 0.3579 - val_acc: 0.9248\n",
      "Epoch 125/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9763Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0583 - acc: 0.9770 - val_loss: 0.3184 - val_acc: 0.8947\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9775Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0575 - acc: 0.9783 - val_loss: 0.4291 - val_acc: 0.8947\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9786Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0671 - acc: 0.9792 - val_loss: 0.4487 - val_acc: 0.8947\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9716Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0738 - acc: 0.9713 - val_loss: 0.4378 - val_acc: 0.9023\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9747Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0580 - acc: 0.9756 - val_loss: 0.4268 - val_acc: 0.8872\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9744Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0704 - acc: 0.9745 - val_loss: 0.4199 - val_acc: 0.8947\n",
      "Epoch 131/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9786Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0637 - acc: 0.9785 - val_loss: 0.3376 - val_acc: 0.9023\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9672Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0891 - acc: 0.9664 - val_loss: 0.3022 - val_acc: 0.9023\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9789Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0536 - acc: 0.9793 - val_loss: 0.3466 - val_acc: 0.8947\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9747Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0593 - acc: 0.9770 - val_loss: 0.3988 - val_acc: 0.9023\n",
      "Epoch 135/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9812Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0507 - acc: 0.9800 - val_loss: 0.4976 - val_acc: 0.9023\n",
      "Epoch 136/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9687Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0884 - acc: 0.9632 - val_loss: 0.3138 - val_acc: 0.8722\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9763Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0716 - acc: 0.9749 - val_loss: 0.3390 - val_acc: 0.9023\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9820Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0467 - acc: 0.9821 - val_loss: 0.3877 - val_acc: 0.9023\n",
      "Epoch 139/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.0597 - acc: 0.9737"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.675543). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.338523). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9815Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0530 - acc: 0.9812 - val_loss: 0.3563 - val_acc: 0.8947\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9820Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0463 - acc: 0.9835 - val_loss: 0.4720 - val_acc: 0.8947\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9888Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0396 - acc: 0.9885 - val_loss: 0.4557 - val_acc: 0.9098\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9755Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0806 - acc: 0.9756 - val_loss: 0.4122 - val_acc: 0.8872\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9724Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0739 - acc: 0.9735 - val_loss: 0.3692 - val_acc: 0.8872\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9692Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1170 - acc: 0.9699 - val_loss: 0.5071 - val_acc: 0.8722\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9535Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1003 - acc: 0.9538 - val_loss: 0.4090 - val_acc: 0.8496\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.9477Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1266 - acc: 0.9479 - val_loss: 0.3659 - val_acc: 0.8797\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9555Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1061 - acc: 0.9514 - val_loss: 0.3294 - val_acc: 0.8947\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9575Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0965 - acc: 0.9599 - val_loss: 0.3041 - val_acc: 0.9023\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9750Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0615 - acc: 0.9743 - val_loss: 0.3812 - val_acc: 0.8947\n",
      "Epoch 150/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9786Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0460 - acc: 0.9805 - val_loss: 0.3171 - val_acc: 0.9248\n",
      "148/148 [==============================] - 0s     \n",
      "Test loss: 0.297666986246\n",
      "Test accuracy: 0.912162162162\n",
      "Fold: 1\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7086 - acc: 0.4905Epoch 00000: val_loss improved from inf to 0.69151, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.7073 - acc: 0.4868 - val_loss: 0.6915 - val_acc: 0.5113\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5053Epoch 00001: val_loss improved from 0.69151 to 0.68993, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6931 - acc: 0.5074 - val_loss: 0.6899 - val_acc: 0.5113\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6865 - acc: 0.5218Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6871 - acc: 0.5184 - val_loss: 0.6932 - val_acc: 0.4887\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5196Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5161 - val_loss: 0.8341 - val_acc: 0.5263\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.5168Epoch 00004: val_loss improved from 0.68993 to 0.68750, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.7302 - acc: 0.5162 - val_loss: 0.6875 - val_acc: 0.4887\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6790 - acc: 0.4668Epoch 00005: val_loss improved from 0.68750 to 0.64437, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6735 - acc: 0.4760 - val_loss: 0.6444 - val_acc: 0.4887\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.5616Epoch 00006: val_loss improved from 0.64437 to 0.63947, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6669 - acc: 0.5604 - val_loss: 0.6395 - val_acc: 0.5789\n",
      "Epoch 8/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.6377 - acc: 0.5703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.592237). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.297118). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.6577 - acc: 0.5832Epoch 00007: val_loss improved from 0.63947 to 0.58854, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6574 - acc: 0.5883 - val_loss: 0.5885 - val_acc: 0.6541\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6151 - acc: 0.6599Epoch 00008: val_loss improved from 0.58854 to 0.58445, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6152 - acc: 0.6590 - val_loss: 0.5844 - val_acc: 0.6692\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6242 - acc: 0.6387Epoch 00009: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6300 - acc: 0.6450 - val_loss: 0.5957 - val_acc: 0.6692\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6211 - acc: 0.6494Epoch 00010: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6220 - acc: 0.6464 - val_loss: 0.5959 - val_acc: 0.6541\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6169 - acc: 0.6408Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6294 - acc: 0.6321 - val_loss: 0.6313 - val_acc: 0.5940\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6622 - acc: 0.6343Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6641 - acc: 0.6302 - val_loss: 0.6827 - val_acc: 0.5865\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6631 - acc: 0.6211Epoch 00013: val_loss improved from 0.58445 to 0.57472, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6638 - acc: 0.6188 - val_loss: 0.5747 - val_acc: 0.6617\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.6491Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6334 - acc: 0.6498 - val_loss: 0.6105 - val_acc: 0.6692\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.6410Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6219 - acc: 0.6405 - val_loss: 0.6280 - val_acc: 0.6090\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6336 - acc: 0.6285Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6316 - acc: 0.6319 - val_loss: 0.5849 - val_acc: 0.6692\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6232 - acc: 0.6262Epoch 00017: val_loss improved from 0.57472 to 0.57003, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6233 - acc: 0.6290 - val_loss: 0.5700 - val_acc: 0.6692\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6157 - acc: 0.6630Epoch 00018: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6161 - acc: 0.6661 - val_loss: 0.6144 - val_acc: 0.6617\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6140 - acc: 0.6549Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6230 - acc: 0.6456 - val_loss: 0.6271 - val_acc: 0.6090\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.6604Epoch 00020: val_loss improved from 0.57003 to 0.55742, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6112 - acc: 0.6574 - val_loss: 0.5574 - val_acc: 0.6767\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6098 - acc: 0.6750Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6101 - acc: 0.6723 - val_loss: 0.6243 - val_acc: 0.6241\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6243 - acc: 0.6645Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6246 - acc: 0.6658 - val_loss: 0.5605 - val_acc: 0.7293\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.6906Epoch 00023: val_loss improved from 0.55742 to 0.55121, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5949 - acc: 0.6959 - val_loss: 0.5512 - val_acc: 0.7444\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7119Epoch 00024: val_loss improved from 0.55121 to 0.53196, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5789 - acc: 0.7071 - val_loss: 0.5320 - val_acc: 0.7594\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5700 - acc: 0.7044Epoch 00025: val_loss improved from 0.53196 to 0.48360, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5641 - acc: 0.7076 - val_loss: 0.4836 - val_acc: 0.7444\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5639 - acc: 0.6994Epoch 00026: val_loss improved from 0.48360 to 0.45898, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5619 - acc: 0.7042 - val_loss: 0.4590 - val_acc: 0.7744\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.7227Epoch 00027: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5336 - acc: 0.7220 - val_loss: 0.4867 - val_acc: 0.7669\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5336 - acc: 0.7179Epoch 00028: val_loss improved from 0.45898 to 0.43611, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5326 - acc: 0.7202 - val_loss: 0.4361 - val_acc: 0.8045\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5190 - acc: 0.7275Epoch 00029: val_loss improved from 0.43611 to 0.42268, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5185 - acc: 0.7287 - val_loss: 0.4227 - val_acc: 0.8496\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.7494Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5248 - acc: 0.7492 - val_loss: 0.5240 - val_acc: 0.8496\n",
      "Epoch 32/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.7564Epoch 00031: val_loss improved from 0.42268 to 0.38769, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5112 - acc: 0.7544 - val_loss: 0.3877 - val_acc: 0.8496\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.7508Epoch 00032: val_loss improved from 0.38769 to 0.33746, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4924 - acc: 0.7491 - val_loss: 0.3375 - val_acc: 0.8722\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4733 - acc: 0.7620Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4808 - acc: 0.7571 - val_loss: 0.3697 - val_acc: 0.7895\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4615 - acc: 0.7827Epoch 00034: val_loss improved from 0.33746 to 0.31188, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4602 - acc: 0.7832 - val_loss: 0.3119 - val_acc: 0.8872\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.7604Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.4530 - acc: 0.7646 - val_loss: 0.3511 - val_acc: 0.8496\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8164Epoch 00036: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s - loss: 0.4048 - acc: 0.8205 - val_loss: 0.3452 - val_acc: 0.8496\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8037Epoch 00037: val_loss improved from 0.31188 to 0.30795, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3965 - acc: 0.8086 - val_loss: 0.3080 - val_acc: 0.8797\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.7755Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4248 - acc: 0.7772 - val_loss: 0.3190 - val_acc: 0.8647\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3884 - acc: 0.8340Epoch 00039: val_loss improved from 0.30795 to 0.28555, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3813 - acc: 0.8366 - val_loss: 0.2855 - val_acc: 0.8872\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3499 - acc: 0.8274Epoch 00040: val_loss improved from 0.28555 to 0.25646, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3476 - acc: 0.8305 - val_loss: 0.2565 - val_acc: 0.8872\n",
      "Epoch 42/150\n",
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.3085 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.468375). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.318710). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.169044). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8517Epoch 00041: val_loss improved from 0.25646 to 0.25007, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3319 - acc: 0.8466 - val_loss: 0.2501 - val_acc: 0.9023\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3375 - acc: 0.8439Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3448 - acc: 0.8387 - val_loss: 0.2732 - val_acc: 0.8947\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.8532Epoch 00043: val_loss improved from 0.25007 to 0.24114, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3044 - acc: 0.8530 - val_loss: 0.2411 - val_acc: 0.8947\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8438Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3293 - acc: 0.8477 - val_loss: 0.2741 - val_acc: 0.8797\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8433Epoch 00045: val_loss improved from 0.24114 to 0.19083, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3147 - acc: 0.8471 - val_loss: 0.1908 - val_acc: 0.9248\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.8405Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3137 - acc: 0.8443 - val_loss: 0.2143 - val_acc: 0.9398\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8626Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3044 - acc: 0.8637 - val_loss: 0.1973 - val_acc: 0.9098\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.8607Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2847 - acc: 0.8611 - val_loss: 0.2141 - val_acc: 0.9173\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8586Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.3015 - acc: 0.8554 - val_loss: 0.2207 - val_acc: 0.9098\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.8731Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2679 - acc: 0.8760 - val_loss: 0.2185 - val_acc: 0.8872\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.8874Epoch 00051: val_loss improved from 0.19083 to 0.18359, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2571 - acc: 0.8875 - val_loss: 0.1836 - val_acc: 0.9398\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9018Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2233 - acc: 0.8998 - val_loss: 0.2173 - val_acc: 0.9323\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.8892Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2466 - acc: 0.8901 - val_loss: 0.2352 - val_acc: 0.9023\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.8874Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2337 - acc: 0.8897 - val_loss: 0.1943 - val_acc: 0.9398\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.8935Epoch 00055: val_loss improved from 0.18359 to 0.17797, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2486 - acc: 0.8877 - val_loss: 0.1780 - val_acc: 0.9398\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.8887Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2514 - acc: 0.8874 - val_loss: 0.1890 - val_acc: 0.9098\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.8968Epoch 00057: val_loss improved from 0.17797 to 0.17007, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2377 - acc: 0.8932 - val_loss: 0.1701 - val_acc: 0.9173\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.8965Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2417 - acc: 0.8945 - val_loss: 0.1855 - val_acc: 0.9248\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.8954Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2517 - acc: 0.8978 - val_loss: 0.1864 - val_acc: 0.9323\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9065Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2361 - acc: 0.9041 - val_loss: 0.2560 - val_acc: 0.9023\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.8905Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2222 - acc: 0.8943 - val_loss: 0.2694 - val_acc: 0.8722\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.8876Epoch 00062: val_loss improved from 0.17007 to 0.16584, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2444 - acc: 0.8930 - val_loss: 0.1658 - val_acc: 0.9173\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.8936Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2255 - acc: 0.8922 - val_loss: 0.2315 - val_acc: 0.9023\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9062Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2035 - acc: 0.9054 - val_loss: 0.2710 - val_acc: 0.8647\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9049Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2225 - acc: 0.9048 - val_loss: 0.2074 - val_acc: 0.9173\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9077Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2000 - acc: 0.9068 - val_loss: 0.1927 - val_acc: 0.9173\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9145Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1948 - acc: 0.9167 - val_loss: 0.2181 - val_acc: 0.9098\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.8991Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2330 - acc: 0.8982 - val_loss: 0.2021 - val_acc: 0.9098\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9135Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2008 - acc: 0.9139 - val_loss: 0.2062 - val_acc: 0.9398\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9140Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2122 - acc: 0.9125 - val_loss: 0.3003 - val_acc: 0.8947\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9158Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1948 - acc: 0.9169 - val_loss: 0.1836 - val_acc: 0.9098\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9046Epoch 00072: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 2s - loss: 0.2070 - acc: 0.9068 - val_loss: 0.1830 - val_acc: 0.9098\n",
      "Epoch 74/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.1609 - acc: 0.9336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.450933). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.229468). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9179Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1906 - acc: 0.9161 - val_loss: 0.2518 - val_acc: 0.8947\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9082Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2010 - acc: 0.9103 - val_loss: 0.2239 - val_acc: 0.9098\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9224Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1790 - acc: 0.9235 - val_loss: 0.2459 - val_acc: 0.9098\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9129Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1808 - acc: 0.9145 - val_loss: 0.2226 - val_acc: 0.9098\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9221Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1768 - acc: 0.9198 - val_loss: 0.2104 - val_acc: 0.9173\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.8937Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2356 - acc: 0.8947 - val_loss: 0.2325 - val_acc: 0.8571\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.8986Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2118 - acc: 0.8984 - val_loss: 0.2151 - val_acc: 0.9098\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9174Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1812 - acc: 0.9169 - val_loss: 0.2669 - val_acc: 0.9248\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9312Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1689 - acc: 0.9318 - val_loss: 0.2846 - val_acc: 0.9023\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9174Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1679 - acc: 0.9205 - val_loss: 0.2283 - val_acc: 0.9098\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9231Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1700 - acc: 0.9242 - val_loss: 0.2353 - val_acc: 0.9023\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9179Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1642 - acc: 0.9175 - val_loss: 0.2039 - val_acc: 0.9248\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9221Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1645 - acc: 0.9233 - val_loss: 0.2571 - val_acc: 0.8947\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9202Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1815 - acc: 0.9211 - val_loss: 0.2121 - val_acc: 0.9323\n",
      "Epoch 88/150\n",
      " 4/10 [==========>...................] - ETA: 2s - loss: 0.1706 - acc: 0.9139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.333947). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.646667). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.324333). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9265Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1643 - acc: 0.9275 - val_loss: 0.2092 - val_acc: 0.9173\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9346Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1522 - acc: 0.9341 - val_loss: 0.2693 - val_acc: 0.9023\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9213Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1727 - acc: 0.9211 - val_loss: 0.1925 - val_acc: 0.9098\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9327Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1521 - acc: 0.9339 - val_loss: 0.2141 - val_acc: 0.9248\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9430Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1440 - acc: 0.9421 - val_loss: 0.1713 - val_acc: 0.9398\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9262Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1826 - acc: 0.9217 - val_loss: 0.2184 - val_acc: 0.9173\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9223Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1844 - acc: 0.9210 - val_loss: 0.1836 - val_acc: 0.9173\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9291Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1645 - acc: 0.9276 - val_loss: 0.2518 - val_acc: 0.9098\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9241Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1680 - acc: 0.9239 - val_loss: 0.2629 - val_acc: 0.9023\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9223Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1642 - acc: 0.9231 - val_loss: 0.2209 - val_acc: 0.9173\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9320Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1598 - acc: 0.9300 - val_loss: 0.2287 - val_acc: 0.9248\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9288Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1443 - acc: 0.9325 - val_loss: 0.3084 - val_acc: 0.9248\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9293Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1635 - acc: 0.9220 - val_loss: 0.2272 - val_acc: 0.9323\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9414Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1515 - acc: 0.9379 - val_loss: 0.1915 - val_acc: 0.9398\n",
      "Epoch 102/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9265Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1620 - acc: 0.9304 - val_loss: 0.2237 - val_acc: 0.9173\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9288Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1435 - acc: 0.9318 - val_loss: 0.2131 - val_acc: 0.9323\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9466Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1268 - acc: 0.9456 - val_loss: 0.2621 - val_acc: 0.8947\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9447Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1396 - acc: 0.9426 - val_loss: 0.1813 - val_acc: 0.9248\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1240 - acc: 0.9449Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1189 - acc: 0.9481 - val_loss: 0.2220 - val_acc: 0.9248\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9502Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1125 - acc: 0.9498 - val_loss: 0.2412 - val_acc: 0.9248\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9351Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1451 - acc: 0.9325 - val_loss: 0.1828 - val_acc: 0.9248\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9230Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1449 - acc: 0.9256 - val_loss: 0.2149 - val_acc: 0.8872\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9411Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1270 - acc: 0.9415 - val_loss: 0.2411 - val_acc: 0.9173\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9424Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1298 - acc: 0.9405 - val_loss: 0.2268 - val_acc: 0.9323\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9437Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1121 - acc: 0.9439 - val_loss: 0.2274 - val_acc: 0.9398\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9549Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1211 - acc: 0.9533 - val_loss: 0.2614 - val_acc: 0.9323\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9528Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0990 - acc: 0.9570 - val_loss: 0.2558 - val_acc: 0.9248\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9552Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1106 - acc: 0.9542 - val_loss: 0.2289 - val_acc: 0.9173\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9578Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0936 - acc: 0.9564 - val_loss: 0.2809 - val_acc: 0.9098\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9688Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0712 - acc: 0.9650 - val_loss: 0.3620 - val_acc: 0.9098\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9552Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1069 - acc: 0.9542 - val_loss: 0.3303 - val_acc: 0.9098\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9491Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1013 - acc: 0.9511 - val_loss: 0.3022 - val_acc: 0.9173\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9530Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1043 - acc: 0.9528 - val_loss: 0.3120 - val_acc: 0.9023\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9583Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0968 - acc: 0.9570 - val_loss: 0.3056 - val_acc: 0.9173\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9591Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0964 - acc: 0.9592 - val_loss: 0.2475 - val_acc: 0.9248\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9420Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1400 - acc: 0.9443 - val_loss: 0.3488 - val_acc: 0.8647\n",
      "Epoch 124/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9199Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1659 - acc: 0.9217 - val_loss: 0.1842 - val_acc: 0.9398\n",
      "Epoch 125/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9599Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0955 - acc: 0.9620 - val_loss: 0.2535 - val_acc: 0.9549\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9567Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0965 - acc: 0.9595 - val_loss: 0.2864 - val_acc: 0.9248\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9476Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1288 - acc: 0.9489 - val_loss: 0.2556 - val_acc: 0.9173\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9593Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1041 - acc: 0.9582 - val_loss: 0.2948 - val_acc: 0.9173\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9641Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0866 - acc: 0.9650 - val_loss: 0.2498 - val_acc: 0.9248\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9611Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0752 - acc: 0.9646 - val_loss: 0.3782 - val_acc: 0.9173\n",
      "Epoch 131/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9583Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0851 - acc: 0.9577 - val_loss: 0.2740 - val_acc: 0.9323\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9692Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0717 - acc: 0.9692 - val_loss: 0.3715 - val_acc: 0.9398\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9621Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0885 - acc: 0.9617 - val_loss: 0.3510 - val_acc: 0.9323\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9680Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0813 - acc: 0.9679 - val_loss: 0.3004 - val_acc: 0.9323\n",
      "Epoch 135/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9739Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0625 - acc: 0.9735 - val_loss: 0.3556 - val_acc: 0.9474\n",
      "Epoch 136/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9685Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0788 - acc: 0.9685 - val_loss: 0.4196 - val_acc: 0.9098\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9682Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0708 - acc: 0.9691 - val_loss: 0.2613 - val_acc: 0.9398\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9711Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0779 - acc: 0.9729 - val_loss: 0.4068 - val_acc: 0.9098\n",
      "Epoch 139/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9591Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0846 - acc: 0.9606 - val_loss: 0.2847 - val_acc: 0.9098\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9661Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0961 - acc: 0.9577 - val_loss: 0.2912 - val_acc: 0.9323\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9797Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0527 - acc: 0.9793 - val_loss: 0.3277 - val_acc: 0.9248\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9666Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0765 - acc: 0.9683 - val_loss: 0.4685 - val_acc: 0.8947\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9544Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1287 - acc: 0.9535 - val_loss: 0.3247 - val_acc: 0.9323\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9554Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0952 - acc: 0.9561 - val_loss: 0.4830 - val_acc: 0.8872\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9742Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0701 - acc: 0.9743 - val_loss: 0.3990 - val_acc: 0.9098\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9744Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0624 - acc: 0.9733 - val_loss: 0.3950 - val_acc: 0.9098\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9789Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0590 - acc: 0.9786 - val_loss: 0.4340 - val_acc: 0.9098\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9763Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0633 - acc: 0.9770 - val_loss: 0.4731 - val_acc: 0.9323\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9731Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0665 - acc: 0.9742 - val_loss: 0.3586 - val_acc: 0.9173\n",
      "Epoch 150/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1118 - acc: 0.9572Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1142 - acc: 0.9587 - val_loss: 0.2830 - val_acc: 0.9248\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.318555162565\n",
      "Test accuracy: 0.844594594595\n",
      "Fold: 2\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6982 - acc: 0.4996Epoch 00000: val_loss improved from inf to 0.69300, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 6s - loss: 0.6975 - acc: 0.5024 - val_loss: 0.6930 - val_acc: 0.5113\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5058Epoch 00001: val_loss improved from 0.69300 to 0.68880, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6929 - acc: 0.5137 - val_loss: 0.6888 - val_acc: 0.5865\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6952 - acc: 0.5521Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6947 - acc: 0.5527 - val_loss: 0.6913 - val_acc: 0.5113\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6795 - acc: 0.5512Epoch 00003: val_loss improved from 0.68880 to 0.66436, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 4s - loss: 0.6792 - acc: 0.5574 - val_loss: 0.6644 - val_acc: 0.6466\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.6274Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6815 - acc: 0.6296 - val_loss: 0.6837 - val_acc: 0.5639\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.6071Epoch 00005: val_loss improved from 0.66436 to 0.65268, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6467 - acc: 0.6139 - val_loss: 0.6527 - val_acc: 0.6466\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.6366Epoch 00006: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6435 - acc: 0.6391 - val_loss: 0.8492 - val_acc: 0.6316\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.6697Epoch 00007: val_loss improved from 0.65268 to 0.64021, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6318 - acc: 0.6664 - val_loss: 0.6402 - val_acc: 0.6767\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.6764Epoch 00008: val_loss improved from 0.64021 to 0.61932, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6014 - acc: 0.6834 - val_loss: 0.6193 - val_acc: 0.7143\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.7143Epoch 00009: val_loss improved from 0.61932 to 0.57438, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5935 - acc: 0.7229 - val_loss: 0.5744 - val_acc: 0.7594\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7412Epoch 00010: val_loss improved from 0.57438 to 0.55517, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5506 - acc: 0.7326 - val_loss: 0.5552 - val_acc: 0.7293\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5126 - acc: 0.7727Epoch 00011: val_loss improved from 0.55517 to 0.53686, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5194 - acc: 0.7655 - val_loss: 0.5369 - val_acc: 0.7594\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.7674Epoch 00012: val_loss improved from 0.53686 to 0.53292, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5022 - acc: 0.7757 - val_loss: 0.5329 - val_acc: 0.7519\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.7841Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5294 - acc: 0.7876 - val_loss: 0.5335 - val_acc: 0.7895\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5122 - acc: 0.7794Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5162 - acc: 0.7729 - val_loss: 0.5584 - val_acc: 0.6992\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.7907Epoch 00015: val_loss improved from 0.53292 to 0.52679, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4723 - acc: 0.7887 - val_loss: 0.5268 - val_acc: 0.7820\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8201Epoch 00016: val_loss improved from 0.52679 to 0.49008, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4255 - acc: 0.8190 - val_loss: 0.4901 - val_acc: 0.7895\n",
      "Epoch 18/150\n",
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.4070 - acc: 0.8136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.263014). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.237922). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.212830). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8347Epoch 00017: val_loss improved from 0.49008 to 0.43884, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3976 - acc: 0.8314 - val_loss: 0.4388 - val_acc: 0.8045\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8514Epoch 00018: val_loss improved from 0.43884 to 0.43172, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3650 - acc: 0.8465 - val_loss: 0.4317 - val_acc: 0.8120\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8245Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4114 - acc: 0.8220 - val_loss: 0.4596 - val_acc: 0.8120\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8447Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3634 - acc: 0.8480 - val_loss: 0.4326 - val_acc: 0.7970\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3577 - acc: 0.8449Epoch 00021: val_loss improved from 0.43172 to 0.42122, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3572 - acc: 0.8414 - val_loss: 0.4212 - val_acc: 0.8271\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8674Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3146 - acc: 0.8691 - val_loss: 0.4555 - val_acc: 0.8195\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8655Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3052 - acc: 0.8668 - val_loss: 0.4579 - val_acc: 0.8271\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8655Epoch 00024: val_loss improved from 0.42122 to 0.41105, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3461 - acc: 0.8518 - val_loss: 0.4110 - val_acc: 0.8346\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8576Epoch 00025: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.3197 - acc: 0.8565 - val_loss: 0.4282 - val_acc: 0.8271\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8383Epoch 00026: val_loss improved from 0.41105 to 0.40523, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3403 - acc: 0.8398 - val_loss: 0.4052 - val_acc: 0.8496\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.8775Epoch 00027: val_loss improved from 0.40523 to 0.39070, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3122 - acc: 0.8723 - val_loss: 0.3907 - val_acc: 0.8571\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8584Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3306 - acc: 0.8586 - val_loss: 0.4198 - val_acc: 0.8120\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.8950Epoch 00029: val_loss improved from 0.39070 to 0.37186, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2543 - acc: 0.8968 - val_loss: 0.3719 - val_acc: 0.8571\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.8905Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2578 - acc: 0.8922 - val_loss: 0.3995 - val_acc: 0.8195\n",
      "Epoch 32/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.8822Epoch 00031: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2577 - acc: 0.8833 - val_loss: 0.3958 - val_acc: 0.8271\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.8976Epoch 00032: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2276 - acc: 0.8947 - val_loss: 0.4267 - val_acc: 0.8496\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2508 - acc: 0.8899Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2523 - acc: 0.8912 - val_loss: 0.4655 - val_acc: 0.8045\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9051Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2185 - acc: 0.9024 - val_loss: 0.4078 - val_acc: 0.8346\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.8907Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2435 - acc: 0.8916 - val_loss: 0.3974 - val_acc: 0.8120\n",
      "Epoch 37/150\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.2280 - acc: 0.9193"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.747078). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.628525). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.509971). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.2323 - acc: 0.9096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.255986). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.8892Epoch 00036: val_loss improved from 0.37186 to 0.36003, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.2729 - acc: 0.8830 - val_loss: 0.3600 - val_acc: 0.8571\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.8732Epoch 00037: val_loss improved from 0.36003 to 0.35117, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2620 - acc: 0.8783 - val_loss: 0.3512 - val_acc: 0.8346\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.8926Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2183 - acc: 0.8938 - val_loss: 0.4483 - val_acc: 0.8120\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.8722Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2783 - acc: 0.8632 - val_loss: 0.3826 - val_acc: 0.8195\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.8810Epoch 00040: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2688 - acc: 0.8835 - val_loss: 0.4147 - val_acc: 0.7970\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.8984Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2266 - acc: 0.8975 - val_loss: 0.4184 - val_acc: 0.8421\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9096Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2005 - acc: 0.9083 - val_loss: 0.4098 - val_acc: 0.8195\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9132Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2048 - acc: 0.9118 - val_loss: 0.4064 - val_acc: 0.7970\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.8996Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2230 - acc: 0.9024 - val_loss: 0.3970 - val_acc: 0.8421\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9129Epoch 00045: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1840 - acc: 0.9124 - val_loss: 0.4682 - val_acc: 0.8346\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9121Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1726 - acc: 0.9153 - val_loss: 0.5171 - val_acc: 0.8496\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9113Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1860 - acc: 0.9160 - val_loss: 0.4584 - val_acc: 0.8346\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9077Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1939 - acc: 0.9075 - val_loss: 0.4365 - val_acc: 0.8421\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9194Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1802 - acc: 0.9151 - val_loss: 0.4883 - val_acc: 0.8045\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9056Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1919 - acc: 0.9037 - val_loss: 0.4653 - val_acc: 0.8045\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9182Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1684 - acc: 0.9191 - val_loss: 0.5014 - val_acc: 0.8271\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9197Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1788 - acc: 0.9191 - val_loss: 0.4685 - val_acc: 0.8571\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9252Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1635 - acc: 0.9233 - val_loss: 0.4337 - val_acc: 0.8271\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9312Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1830 - acc: 0.9261 - val_loss: 0.5612 - val_acc: 0.8346\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9271Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1809 - acc: 0.9242 - val_loss: 0.5010 - val_acc: 0.8045\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9207Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1714 - acc: 0.9231 - val_loss: 0.3996 - val_acc: 0.8647\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9243Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1651 - acc: 0.9259 - val_loss: 0.4988 - val_acc: 0.8346\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9194Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1782 - acc: 0.9189 - val_loss: 0.5236 - val_acc: 0.8271\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9322Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1399 - acc: 0.9380 - val_loss: 0.4895 - val_acc: 0.8496\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9447Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1247 - acc: 0.9469 - val_loss: 0.6978 - val_acc: 0.8045\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9209Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1712 - acc: 0.9200 - val_loss: 0.5750 - val_acc: 0.8195\n",
      "Epoch 63/150\n",
      " 4/10 [==========>...................] - ETA: 3s - loss: 0.1540 - acc: 0.9238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.315778). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.630555). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9268Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1616 - acc: 0.9291 - val_loss: 0.5132 - val_acc: 0.8496\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9307Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1580 - acc: 0.9312 - val_loss: 0.5394 - val_acc: 0.8195\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9273Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1623 - acc: 0.9268 - val_loss: 0.4724 - val_acc: 0.8722\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9377Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1508 - acc: 0.9355 - val_loss: 0.4953 - val_acc: 0.8346\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9361Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1317 - acc: 0.9398 - val_loss: 0.5011 - val_acc: 0.8346\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9267Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1651 - acc: 0.9287 - val_loss: 0.5912 - val_acc: 0.7895\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9168Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1758 - acc: 0.9167 - val_loss: 0.5022 - val_acc: 0.8271\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9366Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1295 - acc: 0.9399 - val_loss: 0.5478 - val_acc: 0.8195\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9435Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1197 - acc: 0.9413 - val_loss: 0.5907 - val_acc: 0.7895\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9516Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1176 - acc: 0.9500 - val_loss: 0.6398 - val_acc: 0.8195\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9541Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1132 - acc: 0.9541 - val_loss: 0.8000 - val_acc: 0.8120\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9410Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1178 - acc: 0.9424 - val_loss: 0.6804 - val_acc: 0.8346\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9606Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0950 - acc: 0.9599 - val_loss: 0.7754 - val_acc: 0.8195\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9382Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1649 - acc: 0.9382 - val_loss: 0.6536 - val_acc: 0.7820\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9351Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1573 - acc: 0.9325 - val_loss: 0.5654 - val_acc: 0.8647\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9429Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1356 - acc: 0.9404 - val_loss: 0.5864 - val_acc: 0.8346\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9549Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1107 - acc: 0.9512 - val_loss: 0.6111 - val_acc: 0.8195\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9544Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1059 - acc: 0.9559 - val_loss: 0.5708 - val_acc: 0.8120\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9567Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1156 - acc: 0.9527 - val_loss: 0.6558 - val_acc: 0.8195\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9560Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1054 - acc: 0.9542 - val_loss: 0.7143 - val_acc: 0.8271\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9622Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1195 - acc: 0.9570 - val_loss: 0.6200 - val_acc: 0.8045\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9547Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1068 - acc: 0.9564 - val_loss: 0.7260 - val_acc: 0.8120\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9563Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0966 - acc: 0.9543 - val_loss: 0.8093 - val_acc: 0.8271\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9692Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0766 - acc: 0.9685 - val_loss: 0.7765 - val_acc: 0.8421\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9606Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1017 - acc: 0.9606 - val_loss: 0.6362 - val_acc: 0.7820\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9750Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0688 - acc: 0.9743 - val_loss: 0.7229 - val_acc: 0.7970\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9635Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0786 - acc: 0.9648 - val_loss: 0.7334 - val_acc: 0.8271\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9703Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0719 - acc: 0.9704 - val_loss: 0.8894 - val_acc: 0.8496\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9627Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0871 - acc: 0.9612 - val_loss: 0.7486 - val_acc: 0.8195\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9773Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0610 - acc: 0.9764 - val_loss: 0.8296 - val_acc: 0.8120\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9763Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0497 - acc: 0.9777 - val_loss: 0.7620 - val_acc: 0.8346\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9820Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0459 - acc: 0.9793 - val_loss: 0.8776 - val_acc: 0.8195\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9766Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0588 - acc: 0.9743 - val_loss: 0.9267 - val_acc: 0.7895\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9747Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0716 - acc: 0.9720 - val_loss: 0.8530 - val_acc: 0.8271\n",
      "Epoch 97/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9572Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1198 - acc: 0.9541 - val_loss: 0.7115 - val_acc: 0.7895\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9554Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1071 - acc: 0.9525 - val_loss: 0.8384 - val_acc: 0.8045\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9658Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0773 - acc: 0.9669 - val_loss: 0.8564 - val_acc: 0.8120\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9666Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0750 - acc: 0.9673 - val_loss: 0.8047 - val_acc: 0.8271\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9794Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0666 - acc: 0.9799 - val_loss: 0.7882 - val_acc: 0.8045\n",
      "Epoch 102/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9679Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0738 - acc: 0.9689 - val_loss: 1.0229 - val_acc: 0.8195\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9731Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0619 - acc: 0.9727 - val_loss: 0.8927 - val_acc: 0.7820\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9708Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0774 - acc: 0.9685 - val_loss: 1.0603 - val_acc: 0.8120\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9533Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1109 - acc: 0.9541 - val_loss: 0.6557 - val_acc: 0.8271\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9750Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0713 - acc: 0.9743 - val_loss: 0.6797 - val_acc: 0.8346\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9810Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0412 - acc: 0.9820 - val_loss: 0.8240 - val_acc: 0.8271\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9875Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0334 - acc: 0.9886 - val_loss: 1.0054 - val_acc: 0.8195\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9747Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0569 - acc: 0.9763 - val_loss: 0.7979 - val_acc: 0.8195\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9880Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0297 - acc: 0.9891 - val_loss: 0.9930 - val_acc: 0.8045\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9812Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0544 - acc: 0.9814 - val_loss: 0.8066 - val_acc: 0.8045\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9852Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0456 - acc: 0.9843 - val_loss: 0.8647 - val_acc: 0.8271\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9883Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0275 - acc: 0.9886 - val_loss: 0.7343 - val_acc: 0.8045\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9791Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0463 - acc: 0.9798 - val_loss: 0.9818 - val_acc: 0.7970\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9758Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0575 - acc: 0.9750 - val_loss: 0.9109 - val_acc: 0.8120\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9802Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0484 - acc: 0.9792 - val_loss: 0.8790 - val_acc: 0.8195\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9930Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0277 - acc: 0.9914 - val_loss: 1.0230 - val_acc: 0.7895\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9922Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0232 - acc: 0.9914 - val_loss: 1.0261 - val_acc: 0.7970\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9898Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0232 - acc: 0.9907 - val_loss: 1.1719 - val_acc: 0.7970\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9523Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1196 - acc: 0.9520 - val_loss: 0.7847 - val_acc: 0.8271\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9716Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0884 - acc: 0.9706 - val_loss: 0.5626 - val_acc: 0.8346\n",
      "Epoch 122/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9625Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1013 - acc: 0.9629 - val_loss: 0.6614 - val_acc: 0.7744\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9875Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0367 - acc: 0.9879 - val_loss: 0.8945 - val_acc: 0.7895\n",
      "Epoch 124/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0236 - acc: 0.9929 - val_loss: 1.1711 - val_acc: 0.7820\n",
      "Epoch 125/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9799Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0562 - acc: 0.9812 - val_loss: 1.1342 - val_acc: 0.7820\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9742Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0608 - acc: 0.9757 - val_loss: 0.7758 - val_acc: 0.7970\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9883Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0325 - acc: 0.9893 - val_loss: 1.1043 - val_acc: 0.7744\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9914Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0297 - acc: 0.9914 - val_loss: 0.8133 - val_acc: 0.8120\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9922Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0202 - acc: 0.9929 - val_loss: 1.0023 - val_acc: 0.7744\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9953Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0117 - acc: 0.9957 - val_loss: 1.2730 - val_acc: 0.7895\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9938Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0191 - acc: 0.9943 - val_loss: 1.2021 - val_acc: 0.8045\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9935Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0148 - acc: 0.9942 - val_loss: 1.2079 - val_acc: 0.8271\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9906Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0287 - acc: 0.9893 - val_loss: 1.2879 - val_acc: 0.8120\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9911Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0467 - acc: 0.9870 - val_loss: 1.2632 - val_acc: 0.8195\n",
      "Epoch 135/150\n",
      " 5/10 [=============>................] - ETA: 2s - loss: 0.0472 - acc: 0.9828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.209632). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9817Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 4s - loss: 0.0454 - acc: 0.9827 - val_loss: 0.8352 - val_acc: 0.8271\n",
      "Epoch 136/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9891Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0298 - acc: 0.9900 - val_loss: 1.1035 - val_acc: 0.7970\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9930Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0171 - acc: 0.9929 - val_loss: 1.1259 - val_acc: 0.8195\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9953Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0172 - acc: 0.9957 - val_loss: 1.0208 - val_acc: 0.8647\n",
      "Epoch 139/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9945Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0206 - acc: 0.9943 - val_loss: 1.1777 - val_acc: 0.7895\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9841Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0344 - acc: 0.9855 - val_loss: 1.2568 - val_acc: 0.7820\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9856Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0449 - acc: 0.9870 - val_loss: 0.8577 - val_acc: 0.8195\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9953Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0196 - acc: 0.9943 - val_loss: 0.7470 - val_acc: 0.8195\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9938Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0198 - acc: 0.9936 - val_loss: 0.9562 - val_acc: 0.8045\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9969Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0100 - acc: 0.9971 - val_loss: 1.3658 - val_acc: 0.8120\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9961Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0124 - acc: 0.9957 - val_loss: 1.1084 - val_acc: 0.7970\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9977Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0111 - acc: 0.9971 - val_loss: 1.1456 - val_acc: 0.8045\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9953Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0062 - acc: 0.9957 - val_loss: 1.3048 - val_acc: 0.8045\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 1.0000Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0020 - acc: 1.0000 - val_loss: 1.5032 - val_acc: 0.7970\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9992Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0026 - acc: 0.9986 - val_loss: 1.7567 - val_acc: 0.8045\n",
      "Epoch 150/150\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.0039 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.917206). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.459354). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9977Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0199 - acc: 0.9956 - val_loss: 1.5589 - val_acc: 0.8045\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.274270320261\n",
      "Test accuracy: 0.885135136746\n",
      "Fold: 3\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7101 - acc: 0.4771Epoch 00000: val_loss improved from inf to 0.66061, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 7s - loss: 0.7045 - acc: 0.4752 - val_loss: 0.6606 - val_acc: 0.6165\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.5419Epoch 00001: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6941 - acc: 0.5420 - val_loss: 0.6930 - val_acc: 0.5113\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4997Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.5020 - val_loss: 0.6930 - val_acc: 0.5113\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5207Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6920 - acc: 0.5205 - val_loss: 0.6938 - val_acc: 0.5113\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5031Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6913 - acc: 0.5071 - val_loss: 0.6916 - val_acc: 0.5113\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6901 - acc: 0.5052Epoch 00005: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6899 - acc: 0.5063 - val_loss: 0.6896 - val_acc: 0.5113\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.5474Epoch 00006: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6802 - acc: 0.5485 - val_loss: 0.6836 - val_acc: 0.6241\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5122Epoch 00007: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6926 - acc: 0.5092 - val_loss: 0.6937 - val_acc: 0.4887\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7998 - acc: 0.4931Epoch 00008: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7908 - acc: 0.4816 - val_loss: 0.6923 - val_acc: 0.6842\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.5872Epoch 00009: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6893 - acc: 0.5836 - val_loss: 0.6872 - val_acc: 0.5263\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6874 - acc: 0.5369Epoch 00010: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6877 - acc: 0.5414 - val_loss: 0.6924 - val_acc: 0.5188\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.5872Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6853 - acc: 0.5836 - val_loss: 0.6852 - val_acc: 0.6391\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.6409Epoch 00012: val_loss improved from 0.66061 to 0.62654, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6376 - acc: 0.6389 - val_loss: 0.6265 - val_acc: 0.6767\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5878 - acc: 0.6978Epoch 00013: val_loss improved from 0.62654 to 0.60195, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5831 - acc: 0.7000 - val_loss: 0.6019 - val_acc: 0.6767\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5570 - acc: 0.7302Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5534 - acc: 0.7297 - val_loss: 0.6344 - val_acc: 0.6241\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7652Epoch 00015: val_loss improved from 0.60195 to 0.58853, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5041 - acc: 0.7592 - val_loss: 0.5885 - val_acc: 0.7143\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5106 - acc: 0.7649Epoch 00016: val_loss improved from 0.58853 to 0.54498, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5135 - acc: 0.7605 - val_loss: 0.5450 - val_acc: 0.7444\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.7671Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4856 - acc: 0.7666 - val_loss: 0.5456 - val_acc: 0.7368\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8050Epoch 00018: val_loss improved from 0.54498 to 0.52418, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4690 - acc: 0.8035 - val_loss: 0.5242 - val_acc: 0.7519\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8220Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4383 - acc: 0.8212 - val_loss: 0.8142 - val_acc: 0.6015\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.7996Epoch 00020: val_loss improved from 0.52418 to 0.51748, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4577 - acc: 0.8010 - val_loss: 0.5175 - val_acc: 0.7368\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8086Epoch 00021: val_loss improved from 0.51748 to 0.50787, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4384 - acc: 0.8120 - val_loss: 0.5079 - val_acc: 0.7444\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8095Epoch 00022: val_loss improved from 0.50787 to 0.48325, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4396 - acc: 0.8116 - val_loss: 0.4832 - val_acc: 0.7594\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8175Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4251 - acc: 0.8206 - val_loss: 0.5148 - val_acc: 0.7444\n",
      "Epoch 25/150\n",
      " 3/10 [=======>......................] - ETA: 3s - loss: 0.4290 - acc: 0.8229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.843657). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.532359). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.221062). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8311Epoch 00024: val_loss improved from 0.48325 to 0.47455, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.4099 - acc: 0.8329 - val_loss: 0.4745 - val_acc: 0.7744\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4013 - acc: 0.8235Epoch 00025: val_loss improved from 0.47455 to 0.42989, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3998 - acc: 0.8291 - val_loss: 0.4299 - val_acc: 0.7970\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8483Epoch 00026: val_loss improved from 0.42989 to 0.37368, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3709 - acc: 0.8458 - val_loss: 0.3737 - val_acc: 0.8195\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8454Epoch 00027: val_loss improved from 0.37368 to 0.36968, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3685 - acc: 0.8477 - val_loss: 0.3697 - val_acc: 0.8271\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8397Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3675 - acc: 0.8429 - val_loss: 0.3790 - val_acc: 0.8120\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8400Epoch 00029: val_loss improved from 0.36968 to 0.35786, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3598 - acc: 0.8422 - val_loss: 0.3579 - val_acc: 0.7970\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3401 - acc: 0.8525Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3397 - acc: 0.8530 - val_loss: 0.3796 - val_acc: 0.8120\n",
      "Epoch 32/150\n",
      " 3/10 [=======>......................] - ETA: 3s - loss: 0.3070 - acc: 0.8568"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.777591). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.542852). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.308114). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.3069 - acc: 0.8547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.154557). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8479Epoch 00031: val_loss improved from 0.35786 to 0.35431, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.3235 - acc: 0.8517 - val_loss: 0.3543 - val_acc: 0.8271\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8394Epoch 00032: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3170 - acc: 0.8478 - val_loss: 0.3770 - val_acc: 0.8271\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8514Epoch 00033: val_loss improved from 0.35431 to 0.30029, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3345 - acc: 0.8522 - val_loss: 0.3003 - val_acc: 0.8195\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.8770Epoch 00034: val_loss improved from 0.30029 to 0.26740, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2729 - acc: 0.8781 - val_loss: 0.2674 - val_acc: 0.8872\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8507Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3329 - acc: 0.8495 - val_loss: 0.5913 - val_acc: 0.7293\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8726Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3127 - acc: 0.8711 - val_loss: 0.3317 - val_acc: 0.8421\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.8649Epoch 00037: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2961 - acc: 0.8670 - val_loss: 0.2728 - val_acc: 0.8797\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2785 - acc: 0.8720Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2681 - acc: 0.8773 - val_loss: 0.3300 - val_acc: 0.8421\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.8926Epoch 00039: val_loss improved from 0.26740 to 0.25302, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2640 - acc: 0.8905 - val_loss: 0.2530 - val_acc: 0.8947\n",
      "Epoch 41/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.2606 - acc: 0.8672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.442921). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.221961). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.8840Epoch 00040: val_loss improved from 0.25302 to 0.23803, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2566 - acc: 0.8860 - val_loss: 0.2380 - val_acc: 0.9098\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.8850Epoch 00041: val_loss improved from 0.23803 to 0.22966, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2394 - acc: 0.8872 - val_loss: 0.2297 - val_acc: 0.9098\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.8952Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2407 - acc: 0.8918 - val_loss: 0.2774 - val_acc: 0.8647\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.8947Epoch 00043: val_loss improved from 0.22966 to 0.21760, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2178 - acc: 0.8948 - val_loss: 0.2176 - val_acc: 0.9098\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9099Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2117 - acc: 0.9063 - val_loss: 0.2254 - val_acc: 0.9023\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.8894Epoch 00045: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2342 - acc: 0.8906 - val_loss: 0.2573 - val_acc: 0.8872\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.8822Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2478 - acc: 0.8826 - val_loss: 0.3433 - val_acc: 0.8421\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9150Epoch 00047: val_loss improved from 0.21760 to 0.21519, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2126 - acc: 0.9119 - val_loss: 0.2152 - val_acc: 0.9248\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9080Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2001 - acc: 0.9105 - val_loss: 0.2304 - val_acc: 0.8947\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.8852Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2070 - acc: 0.8866 - val_loss: 0.3011 - val_acc: 0.8722\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.8973Epoch 00050: val_loss improved from 0.21519 to 0.20112, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2250 - acc: 0.8960 - val_loss: 0.2011 - val_acc: 0.9398\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9121Epoch 00051: val_loss improved from 0.20112 to 0.19504, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1978 - acc: 0.9095 - val_loss: 0.1950 - val_acc: 0.9248\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.8988Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2280 - acc: 0.8985 - val_loss: 0.2127 - val_acc: 0.9398\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9002Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2012 - acc: 0.9005 - val_loss: 0.2184 - val_acc: 0.9248\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9143Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1756 - acc: 0.9119 - val_loss: 0.1991 - val_acc: 0.9248\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9192Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1931 - acc: 0.9188 - val_loss: 0.2263 - val_acc: 0.9173\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9060Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2041 - acc: 0.9085 - val_loss: 0.2702 - val_acc: 0.8947\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9134Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1898 - acc: 0.9130 - val_loss: 0.2283 - val_acc: 0.9173\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9124Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1795 - acc: 0.9168 - val_loss: 0.2934 - val_acc: 0.8872\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9236Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1755 - acc: 0.9255 - val_loss: 0.3039 - val_acc: 0.8872\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9244Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1752 - acc: 0.9191 - val_loss: 0.2420 - val_acc: 0.9173\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9185Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1658 - acc: 0.9185 - val_loss: 0.2573 - val_acc: 0.9248\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9200Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1856 - acc: 0.9185 - val_loss: 0.2661 - val_acc: 0.9173\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9210Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1665 - acc: 0.9218 - val_loss: 0.3135 - val_acc: 0.8722\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9241Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1761 - acc: 0.9239 - val_loss: 0.2161 - val_acc: 0.9173\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9208Epoch 00065: val_loss improved from 0.19504 to 0.18393, saving model to saved_models/fold3.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1533 - acc: 0.9206 - val_loss: 0.1839 - val_acc: 0.9398\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9338Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1662 - acc: 0.9319 - val_loss: 0.2065 - val_acc: 0.9248\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9335Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1429 - acc: 0.9339 - val_loss: 0.2352 - val_acc: 0.9173\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9273Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1577 - acc: 0.9268 - val_loss: 0.2460 - val_acc: 0.9098\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9330Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1524 - acc: 0.9274 - val_loss: 0.1938 - val_acc: 0.9398\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9359Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1395 - acc: 0.9375 - val_loss: 0.1925 - val_acc: 0.9248\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.9084Epoch 00071: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s - loss: 0.1678 - acc: 0.9122 - val_loss: 0.2556 - val_acc: 0.9098\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9116Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1785 - acc: 0.9147 - val_loss: 0.2356 - val_acc: 0.9248\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9406Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1337 - acc: 0.9421 - val_loss: 0.2378 - val_acc: 0.9098\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9366Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1298 - acc: 0.9361 - val_loss: 0.2310 - val_acc: 0.9248\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9246Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1642 - acc: 0.9245 - val_loss: 0.2257 - val_acc: 0.9023\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9442Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1278 - acc: 0.9442 - val_loss: 0.2224 - val_acc: 0.9023\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9458Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1346 - acc: 0.9427 - val_loss: 0.3846 - val_acc: 0.8722\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9181Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1936 - acc: 0.9187 - val_loss: 0.2087 - val_acc: 0.9248\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9359Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1501 - acc: 0.9410 - val_loss: 0.2233 - val_acc: 0.9173\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9444Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1409 - acc: 0.9454 - val_loss: 0.1984 - val_acc: 0.9173\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9400Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1242 - acc: 0.9426 - val_loss: 0.2995 - val_acc: 0.8872\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9304Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1531 - acc: 0.9332 - val_loss: 0.3602 - val_acc: 0.8872\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9450Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1383 - acc: 0.9435 - val_loss: 0.2562 - val_acc: 0.9248\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9502Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1182 - acc: 0.9491 - val_loss: 0.2681 - val_acc: 0.9173\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9489Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1165 - acc: 0.9442 - val_loss: 0.2625 - val_acc: 0.9098\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9611Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0975 - acc: 0.9612 - val_loss: 0.2449 - val_acc: 0.9173\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9513Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1089 - acc: 0.9535 - val_loss: 0.2734 - val_acc: 0.8947\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9400Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1441 - acc: 0.9405 - val_loss: 0.2207 - val_acc: 0.9173\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9505Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1166 - acc: 0.9523 - val_loss: 0.3328 - val_acc: 0.8947\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9641Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0959 - acc: 0.9643 - val_loss: 0.2603 - val_acc: 0.9173\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1000 - acc: 0.9525Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0993 - acc: 0.9526 - val_loss: 0.3155 - val_acc: 0.9023\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9481Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1189 - acc: 0.9467 - val_loss: 0.2996 - val_acc: 0.9173\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9609Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0927 - acc: 0.9636 - val_loss: 0.2916 - val_acc: 0.9098\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9661Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0839 - acc: 0.9670 - val_loss: 0.3581 - val_acc: 0.9323\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9591Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0971 - acc: 0.9613 - val_loss: 0.3183 - val_acc: 0.8947\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9596Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0951 - acc: 0.9612 - val_loss: 0.3529 - val_acc: 0.9023\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9630Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0836 - acc: 0.9642 - val_loss: 0.2903 - val_acc: 0.9248\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9724Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0655 - acc: 0.9713 - val_loss: 0.3013 - val_acc: 0.9323\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9692Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0753 - acc: 0.9696 - val_loss: 0.3253 - val_acc: 0.9173\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9734Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0589 - acc: 0.9736 - val_loss: 0.4332 - val_acc: 0.9098\n",
      "Epoch 102/150\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.0482 - acc: 0.9792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.310638). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.562756). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9771Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0644 - acc: 0.9763 - val_loss: 0.3605 - val_acc: 0.9173\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9669Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0721 - acc: 0.9692 - val_loss: 0.3746 - val_acc: 0.8947\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9852Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0432 - acc: 0.9850 - val_loss: 0.3847 - val_acc: 0.9173\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9820Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0550 - acc: 0.9814 - val_loss: 0.4604 - val_acc: 0.9098\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9859Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0423 - acc: 0.9864 - val_loss: 0.4566 - val_acc: 0.9023\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9708Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0777 - acc: 0.9677 - val_loss: 0.3780 - val_acc: 0.8872\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9614Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0941 - acc: 0.9620 - val_loss: 0.4261 - val_acc: 0.8872\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0675 - acc: 0.9679Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0621 - acc: 0.9704 - val_loss: 0.5580 - val_acc: 0.8872\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9533Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1250 - acc: 0.9551 - val_loss: 0.3125 - val_acc: 0.9098\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9716Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0844 - acc: 0.9699 - val_loss: 0.3811 - val_acc: 0.9023\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9789Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0558 - acc: 0.9807 - val_loss: 0.6402 - val_acc: 0.8872\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9744Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0636 - acc: 0.9755 - val_loss: 0.5107 - val_acc: 0.8872\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9731Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0646 - acc: 0.9749 - val_loss: 0.4704 - val_acc: 0.8872\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9812Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0465 - acc: 0.9829 - val_loss: 0.4489 - val_acc: 0.8947\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9841Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0383 - acc: 0.9849 - val_loss: 0.3394 - val_acc: 0.9248\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9891Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0435 - acc: 0.9829 - val_loss: 0.4786 - val_acc: 0.8947\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9682Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0773 - acc: 0.9683 - val_loss: 0.4100 - val_acc: 0.9098\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9914Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0298 - acc: 0.9907 - val_loss: 0.4886 - val_acc: 0.9023\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9922Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0214 - acc: 0.9928 - val_loss: 0.6175 - val_acc: 0.8947\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9763Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0694 - acc: 0.9756 - val_loss: 0.6056 - val_acc: 0.8797\n",
      "Epoch 122/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9700Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0678 - acc: 0.9706 - val_loss: 0.4533 - val_acc: 0.8872\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9810Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0468 - acc: 0.9799 - val_loss: 0.4271 - val_acc: 0.9173\n",
      "Epoch 124/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9805Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0458 - acc: 0.9807 - val_loss: 0.4625 - val_acc: 0.8947\n",
      "Epoch 125/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9856Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0419 - acc: 0.9835 - val_loss: 0.4962 - val_acc: 0.9023\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9715Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0605 - acc: 0.9731 - val_loss: 0.3683 - val_acc: 0.9173\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9864Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0272 - acc: 0.9877 - val_loss: 0.4203 - val_acc: 0.8872\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9896Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0308 - acc: 0.9877 - val_loss: 0.6055 - val_acc: 0.8647\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9914Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0208 - acc: 0.9914 - val_loss: 0.6978 - val_acc: 0.8647\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9898Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0393 - acc: 0.9884 - val_loss: 0.5388 - val_acc: 0.8947\n",
      "Epoch 131/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9791Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0531 - acc: 0.9798 - val_loss: 0.8271 - val_acc: 0.8571\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9650Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0938 - acc: 0.9662 - val_loss: 0.4674 - val_acc: 0.8797\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9773Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0650 - acc: 0.9789 - val_loss: 0.4548 - val_acc: 0.8872\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9875Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0433 - acc: 0.9871 - val_loss: 0.5008 - val_acc: 0.9023\n",
      "Epoch 135/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9883Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0273 - acc: 0.9893 - val_loss: 0.6440 - val_acc: 0.8722\n",
      "Epoch 136/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9812Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0527 - acc: 0.9829 - val_loss: 0.6080 - val_acc: 0.8947\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9611Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0808 - acc: 0.9630 - val_loss: 0.3835 - val_acc: 0.8947\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9810Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0487 - acc: 0.9820 - val_loss: 0.5317 - val_acc: 0.8797\n",
      "Epoch 139/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9953Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0204 - acc: 0.9957 - val_loss: 0.4705 - val_acc: 0.8947\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9908Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0202 - acc: 0.9919 - val_loss: 0.4909 - val_acc: 0.8872\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9961Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0134 - acc: 0.9957 - val_loss: 0.6492 - val_acc: 0.8947\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9945Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0170 - acc: 0.9950 - val_loss: 0.6633 - val_acc: 0.8947\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9927Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0157 - acc: 0.9935 - val_loss: 0.6332 - val_acc: 0.8647\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9763Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0473 - acc: 0.9785 - val_loss: 0.4728 - val_acc: 0.8947\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9789Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 4s - loss: 0.0545 - acc: 0.9779 - val_loss: 0.4143 - val_acc: 0.8797\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9922Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0268 - acc: 0.9914 - val_loss: 0.5066 - val_acc: 0.8797\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9945Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0183 - acc: 0.9950 - val_loss: 0.5273 - val_acc: 0.8797\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9961Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0132 - acc: 0.9964 - val_loss: 0.5768 - val_acc: 0.8872\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9984Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0099 - acc: 0.9979 - val_loss: 0.6853 - val_acc: 0.8722\n",
      "Epoch 150/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9969Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0078 - acc: 0.9971 - val_loss: 0.7243 - val_acc: 0.8797\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.374284366901\n",
      "Test accuracy: 0.837837834616\n",
      "Fold: 4\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.5123Epoch 00000: val_loss improved from inf to 0.67565, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 8s - loss: 0.7031 - acc: 0.5102 - val_loss: 0.6756 - val_acc: 0.4887\n",
      "Epoch 2/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.6943 - acc: 0.4727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.414987). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.208494). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.4781Epoch 00001: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6936 - acc: 0.4793 - val_loss: 0.6933 - val_acc: 0.4887\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5338Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6861 - val_acc: 0.5113\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6868 - acc: 0.5960Epoch 00003: val_loss improved from 0.67565 to 0.65186, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6861 - acc: 0.5968 - val_loss: 0.6519 - val_acc: 0.6090\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6813 - acc: 0.5866Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6822 - acc: 0.5768 - val_loss: 0.6805 - val_acc: 0.5489\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6722 - acc: 0.5925Epoch 00005: val_loss improved from 0.65186 to 0.65109, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6664 - acc: 0.6001 - val_loss: 0.6511 - val_acc: 0.5865\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.6124Epoch 00006: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6485 - acc: 0.6163 - val_loss: 0.6666 - val_acc: 0.6617\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.6779Epoch 00007: val_loss improved from 0.65109 to 0.59860, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5884 - acc: 0.6783 - val_loss: 0.5986 - val_acc: 0.6541\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7025Epoch 00008: val_loss improved from 0.59860 to 0.58636, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5676 - acc: 0.7014 - val_loss: 0.5864 - val_acc: 0.6992\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.6992Epoch 00009: val_loss improved from 0.58636 to 0.56166, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5616 - acc: 0.7114 - val_loss: 0.5617 - val_acc: 0.6842\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.7190Epoch 00010: val_loss improved from 0.56166 to 0.53960, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5372 - acc: 0.7174 - val_loss: 0.5396 - val_acc: 0.7444\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.7367Epoch 00011: val_loss improved from 0.53960 to 0.49935, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5289 - acc: 0.7423 - val_loss: 0.4993 - val_acc: 0.7594\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5002 - acc: 0.7915Epoch 00012: val_loss improved from 0.49935 to 0.46483, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4990 - acc: 0.7937 - val_loss: 0.4648 - val_acc: 0.8195\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4641 - acc: 0.7956Epoch 00013: val_loss improved from 0.46483 to 0.45155, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4569 - acc: 0.8006 - val_loss: 0.4515 - val_acc: 0.8045\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.8241Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4324 - acc: 0.8229 - val_loss: 0.4563 - val_acc: 0.8045\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8058Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4564 - acc: 0.7985 - val_loss: 0.5329 - val_acc: 0.7368\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8129Epoch 00016: val_loss improved from 0.45155 to 0.42392, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.4347 - acc: 0.8188 - val_loss: 0.4239 - val_acc: 0.8195\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8414Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3745 - acc: 0.8441 - val_loss: 0.4245 - val_acc: 0.8421\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8201Epoch 00018: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4061 - acc: 0.8183 - val_loss: 0.4260 - val_acc: 0.8195\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3714 - acc: 0.8414Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3623 - acc: 0.8486 - val_loss: 0.4321 - val_acc: 0.8271\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3654 - acc: 0.8439Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3703 - acc: 0.8380 - val_loss: 0.4842 - val_acc: 0.7820\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8527Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3597 - acc: 0.8499 - val_loss: 0.4367 - val_acc: 0.7820\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8493Epoch 00022: val_loss improved from 0.42392 to 0.41378, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3531 - acc: 0.8484 - val_loss: 0.4138 - val_acc: 0.7820\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8678Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3383 - acc: 0.8658 - val_loss: 0.4622 - val_acc: 0.7820\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3038 - acc: 0.8686Epoch 00024: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3080 - acc: 0.8693 - val_loss: 0.4227 - val_acc: 0.8045\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8736Epoch 00025: val_loss improved from 0.41378 to 0.36328, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3197 - acc: 0.8759 - val_loss: 0.3633 - val_acc: 0.8271\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8863Epoch 00026: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2922 - acc: 0.8831 - val_loss: 0.3715 - val_acc: 0.8120\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8459Epoch 00027: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3513 - acc: 0.8458 - val_loss: 0.3807 - val_acc: 0.8195\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8848Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2829 - acc: 0.8874 - val_loss: 0.3746 - val_acc: 0.8421\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.8724Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3019 - acc: 0.8660 - val_loss: 0.3881 - val_acc: 0.8195\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.8734Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2897 - acc: 0.8718 - val_loss: 0.3888 - val_acc: 0.8195\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8792Epoch 00031: val_loss improved from 0.36328 to 0.33859, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2863 - acc: 0.8785 - val_loss: 0.3386 - val_acc: 0.8195\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8944Epoch 00032: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2515 - acc: 0.8918 - val_loss: 0.4158 - val_acc: 0.8271\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.8773Epoch 00033: val_loss improved from 0.33859 to 0.30159, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2776 - acc: 0.8739 - val_loss: 0.3016 - val_acc: 0.8271\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.8903Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2762 - acc: 0.8917 - val_loss: 0.3437 - val_acc: 0.8195\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.8861Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2533 - acc: 0.8880 - val_loss: 0.3515 - val_acc: 0.8496\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8975Epoch 00036: val_loss improved from 0.30159 to 0.29172, saving model to saved_models/fold4.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2505 - acc: 0.8950 - val_loss: 0.2917 - val_acc: 0.8647\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2351 - acc: 0.8991Epoch 00037: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2334 - acc: 0.9018 - val_loss: 0.3123 - val_acc: 0.8496\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9135Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1941 - acc: 0.9155 - val_loss: 0.2931 - val_acc: 0.8271\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9118Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2266 - acc: 0.9061 - val_loss: 0.3585 - val_acc: 0.8271\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9106Epoch 00040: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2304 - acc: 0.9070 - val_loss: 0.3503 - val_acc: 0.7895\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.8980Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2180 - acc: 0.9006 - val_loss: 0.3427 - val_acc: 0.8271\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9018Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2260 - acc: 0.9026 - val_loss: 0.2952 - val_acc: 0.8496\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9231Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2080 - acc: 0.9227 - val_loss: 0.3677 - val_acc: 0.8421\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9082Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2122 - acc: 0.9081 - val_loss: 0.3070 - val_acc: 0.8496\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9171Epoch 00045: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2018 - acc: 0.9139 - val_loss: 0.3037 - val_acc: 0.8421\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9041Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2326 - acc: 0.9033 - val_loss: 0.3102 - val_acc: 0.8571\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9091Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2266 - acc: 0.9063 - val_loss: 0.3907 - val_acc: 0.8421\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.8986Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2391 - acc: 0.8973 - val_loss: 0.3344 - val_acc: 0.8496\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9101Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2072 - acc: 0.9110 - val_loss: 0.3215 - val_acc: 0.8647\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9244Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1824 - acc: 0.9226 - val_loss: 0.3447 - val_acc: 0.8647\n",
      "Epoch 52/150\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.1787 - acc: 0.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.447723). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.601536). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.1635 - acc: 0.9313"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.224862). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9152Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1888 - acc: 0.9156 - val_loss: 0.3117 - val_acc: 0.8722\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9138Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1882 - acc: 0.9185 - val_loss: 0.3277 - val_acc: 0.8496\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9224Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1749 - acc: 0.9213 - val_loss: 0.3443 - val_acc: 0.8496\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9282Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1773 - acc: 0.9280 - val_loss: 0.3400 - val_acc: 0.8346\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9330Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1694 - acc: 0.9319 - val_loss: 0.3331 - val_acc: 0.8195\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9346Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1744 - acc: 0.9319 - val_loss: 0.3642 - val_acc: 0.8346\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9364Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1559 - acc: 0.9342 - val_loss: 0.3054 - val_acc: 0.8421\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9361Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1499 - acc: 0.9398 - val_loss: 0.4255 - val_acc: 0.8421\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9236Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1647 - acc: 0.9255 - val_loss: 0.3968 - val_acc: 0.8496\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9411Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1435 - acc: 0.9420 - val_loss: 0.5367 - val_acc: 0.7744\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9312Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1620 - acc: 0.9311 - val_loss: 0.4380 - val_acc: 0.8571\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9377Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1473 - acc: 0.9376 - val_loss: 0.4561 - val_acc: 0.8421\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9372Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1350 - acc: 0.9363 - val_loss: 0.4827 - val_acc: 0.8571\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9429Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1440 - acc: 0.9389 - val_loss: 0.4211 - val_acc: 0.8271\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9380Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1353 - acc: 0.9385 - val_loss: 0.4469 - val_acc: 0.8421\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9437Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1295 - acc: 0.9443 - val_loss: 0.3757 - val_acc: 0.8421\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9403Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1382 - acc: 0.9420 - val_loss: 0.4053 - val_acc: 0.8421\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9442Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1346 - acc: 0.9442 - val_loss: 0.4658 - val_acc: 0.8647\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9278Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1636 - acc: 0.9319 - val_loss: 0.4731 - val_acc: 0.8271\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9225Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1690 - acc: 0.9279 - val_loss: 0.3526 - val_acc: 0.8271\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9521Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1225 - acc: 0.9506 - val_loss: 0.4984 - val_acc: 0.8571\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9481Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1177 - acc: 0.9485 - val_loss: 0.3863 - val_acc: 0.8496\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9585Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0927 - acc: 0.9604 - val_loss: 0.4990 - val_acc: 0.8271\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9539Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0939 - acc: 0.9543 - val_loss: 0.5524 - val_acc: 0.8496\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9641Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0911 - acc: 0.9621 - val_loss: 0.4780 - val_acc: 0.8571\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9416Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1314 - acc: 0.9405 - val_loss: 0.4057 - val_acc: 0.8346\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9435Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1358 - acc: 0.9449 - val_loss: 0.3432 - val_acc: 0.8421\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9544Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1053 - acc: 0.9542 - val_loss: 0.4775 - val_acc: 0.8271\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9664Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0702 - acc: 0.9691 - val_loss: 0.6582 - val_acc: 0.8421\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9716Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0804 - acc: 0.9677 - val_loss: 0.5792 - val_acc: 0.8647\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9664Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0867 - acc: 0.9664 - val_loss: 0.4058 - val_acc: 0.8571\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9752Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0598 - acc: 0.9762 - val_loss: 0.4781 - val_acc: 0.8647\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9794Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0592 - acc: 0.9785 - val_loss: 0.5871 - val_acc: 0.8571\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9692Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0640 - acc: 0.9692 - val_loss: 0.5502 - val_acc: 0.8571\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9742Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0758 - acc: 0.9707 - val_loss: 0.5808 - val_acc: 0.8571\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9528Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1136 - acc: 0.9513 - val_loss: 0.4735 - val_acc: 0.8045\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9560Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1057 - acc: 0.9549 - val_loss: 0.4310 - val_acc: 0.8722\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9609Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0938 - acc: 0.9625 - val_loss: 0.4014 - val_acc: 0.8722\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9650Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0744 - acc: 0.9682 - val_loss: 0.4824 - val_acc: 0.8571\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9778Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0658 - acc: 0.9777 - val_loss: 0.5895 - val_acc: 0.8421\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9742Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0628 - acc: 0.9757 - val_loss: 0.6917 - val_acc: 0.8271\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9661Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0845 - acc: 0.9635 - val_loss: 0.6061 - val_acc: 0.8571\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9752Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0745 - acc: 0.9762 - val_loss: 0.6388 - val_acc: 0.7970\n",
      "Epoch 95/150\n",
      " 5/10 [=============>................] - ETA: 2s - loss: 0.0781 - acc: 0.9609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.199080). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9507Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1069 - acc: 0.9539 - val_loss: 0.4834 - val_acc: 0.8271\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9622Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0945 - acc: 0.9620 - val_loss: 0.6217 - val_acc: 0.8271\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9734Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0714 - acc: 0.9757 - val_loss: 0.6318 - val_acc: 0.8571\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9666Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0867 - acc: 0.9669 - val_loss: 0.5970 - val_acc: 0.8346\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9653Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0987 - acc: 0.9635 - val_loss: 0.4931 - val_acc: 0.8797\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9697Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0798 - acc: 0.9725 - val_loss: 0.5206 - val_acc: 0.8571\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9833Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0477 - acc: 0.9827 - val_loss: 0.7927 - val_acc: 0.8496\n",
      "Epoch 102/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9830Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0604 - acc: 0.9841 - val_loss: 0.6981 - val_acc: 0.8496\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9679Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0660 - acc: 0.9682 - val_loss: 0.7243 - val_acc: 0.8571\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9836Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0571 - acc: 0.9793 - val_loss: 0.6082 - val_acc: 0.8872\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9739Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0613 - acc: 0.9756 - val_loss: 0.5570 - val_acc: 0.8722\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9891Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0285 - acc: 0.9893 - val_loss: 0.7873 - val_acc: 0.8571\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9849Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0340 - acc: 0.9849 - val_loss: 0.8380 - val_acc: 0.8496\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9875Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0391 - acc: 0.9879 - val_loss: 0.7951 - val_acc: 0.8722\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9914Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0207 - acc: 0.9914 - val_loss: 0.8143 - val_acc: 0.8647\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9883Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0274 - acc: 0.9892 - val_loss: 0.8846 - val_acc: 0.8571\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9969Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0201 - acc: 0.9950 - val_loss: 0.9354 - val_acc: 0.8647\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9930Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0221 - acc: 0.9921 - val_loss: 0.8876 - val_acc: 0.8571\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9914Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0219 - acc: 0.9921 - val_loss: 0.9518 - val_acc: 0.8571\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9930Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0242 - acc: 0.9921 - val_loss: 0.9983 - val_acc: 0.8421\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9861Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0391 - acc: 0.9869 - val_loss: 0.8255 - val_acc: 0.8647\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9953Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0206 - acc: 0.9950 - val_loss: 0.7650 - val_acc: 0.8872\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9896Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0327 - acc: 0.9906 - val_loss: 0.9073 - val_acc: 0.8496\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9883Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0344 - acc: 0.9893 - val_loss: 0.9247 - val_acc: 0.8571\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9898Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0274 - acc: 0.9900 - val_loss: 0.9714 - val_acc: 0.8346\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9708Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0658 - acc: 0.9710 - val_loss: 0.6001 - val_acc: 0.8722\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9883Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0341 - acc: 0.9871 - val_loss: 0.7515 - val_acc: 0.8346\n",
      "Epoch 122/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9891Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0356 - acc: 0.9886 - val_loss: 0.6894 - val_acc: 0.8496\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9977Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0109 - acc: 0.9979 - val_loss: 0.9136 - val_acc: 0.8571\n",
      "Epoch 124/150\n",
      " 4/10 [==========>...................] - ETA: 2s - loss: 0.0202 - acc: 0.9961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.201402). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9977Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.0116 - acc: 0.9979 - val_loss: 1.1156 - val_acc: 0.8722\n",
      "Epoch 125/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9888Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0282 - acc: 0.9899 - val_loss: 0.8219 - val_acc: 0.8346\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9953Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0139 - acc: 0.9957 - val_loss: 0.8844 - val_acc: 0.8797\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9880Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0230 - acc: 0.9885 - val_loss: 0.7871 - val_acc: 0.8872\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9810Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0462 - acc: 0.9813 - val_loss: 0.8084 - val_acc: 0.8571\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9906Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0220 - acc: 0.9914 - val_loss: 0.8032 - val_acc: 0.8496\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9930Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0290 - acc: 0.9913 - val_loss: 1.1431 - val_acc: 0.8045\n",
      "Epoch 131/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9778Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0607 - acc: 0.9785 - val_loss: 0.5962 - val_acc: 0.8797\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9914Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0336 - acc: 0.9914 - val_loss: 0.5711 - val_acc: 0.8722\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9938Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0171 - acc: 0.9929 - val_loss: 0.9675 - val_acc: 0.8647\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9906Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0266 - acc: 0.9893 - val_loss: 0.9473 - val_acc: 0.8496\n",
      "Epoch 135/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9961Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0155 - acc: 0.9950 - val_loss: 0.8134 - val_acc: 0.8722\n",
      "Epoch 136/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9961Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0122 - acc: 0.9964 - val_loss: 0.8193 - val_acc: 0.8647\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9930Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0220 - acc: 0.9929 - val_loss: 1.0513 - val_acc: 0.8571\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9953Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0200 - acc: 0.9943 - val_loss: 1.0238 - val_acc: 0.8647\n",
      "Epoch 139/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9911Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0283 - acc: 0.9906 - val_loss: 0.8823 - val_acc: 0.8421\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9888Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0314 - acc: 0.9876 - val_loss: 0.8115 - val_acc: 0.8647\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9880Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0260 - acc: 0.9877 - val_loss: 0.9171 - val_acc: 0.8647\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9977Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0147 - acc: 0.9957 - val_loss: 0.9451 - val_acc: 0.8647\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9969Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0147 - acc: 0.9971 - val_loss: 0.8664 - val_acc: 0.8797\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9961Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0072 - acc: 0.9964 - val_loss: 1.1330 - val_acc: 0.8346\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9922Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0182 - acc: 0.9929 - val_loss: 0.9025 - val_acc: 0.8496\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9815Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0482 - acc: 0.9791 - val_loss: 0.8144 - val_acc: 0.8647\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9828Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0525 - acc: 0.9836 - val_loss: 0.5278 - val_acc: 0.8421\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9938Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0220 - acc: 0.9943 - val_loss: 0.8952 - val_acc: 0.8346\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9880Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0219 - acc: 0.9892 - val_loss: 0.8301 - val_acc: 0.8722\n",
      "Epoch 150/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9935Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0134 - acc: 0.9941 - val_loss: 1.0143 - val_acc: 0.8647\n",
      " 96/148 [==================>...........] - ETA: 1sTest loss: 0.276260461356\n",
      "Test accuracy: 0.89189188867\n",
      "Fold: 5\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.8271 - acc: 0.4850Epoch 00000: val_loss improved from inf to 0.68221, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 9s - loss: 0.8155 - acc: 0.4831 - val_loss: 0.6822 - val_acc: 0.6894\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.5916Epoch 00001: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7571 - acc: 0.5855 - val_loss: 0.6928 - val_acc: 0.5076\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5074Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.5141 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5489Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6926 - acc: 0.5532 - val_loss: 0.6887 - val_acc: 0.5985\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.5899Epoch 00004: val_loss improved from 0.68221 to 0.64736, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6861 - acc: 0.5953 - val_loss: 0.6474 - val_acc: 0.6439\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.5984Epoch 00005: val_loss improved from 0.64736 to 0.62556, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 4s - loss: 0.6691 - acc: 0.6057 - val_loss: 0.6256 - val_acc: 0.7348\n",
      "Epoch 7/150\n",
      " 4/10 [==========>...................] - ETA: 2s - loss: 0.6811 - acc: 0.6734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.225814). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.449627). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.6588 - acc: 0.6389Epoch 00006: val_loss improved from 0.62556 to 0.58987, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.6541 - acc: 0.6406 - val_loss: 0.5899 - val_acc: 0.7121\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6387 - acc: 0.6363Epoch 00007: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6351 - acc: 0.6405 - val_loss: 0.5922 - val_acc: 0.6667\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.6679Epoch 00008: val_loss improved from 0.58987 to 0.57939, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6078 - acc: 0.6658 - val_loss: 0.5794 - val_acc: 0.7273\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.6945Epoch 00009: val_loss improved from 0.57939 to 0.57561, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5898 - acc: 0.6897 - val_loss: 0.5756 - val_acc: 0.7045\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.6950Epoch 00010: val_loss improved from 0.57561 to 0.47711, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5842 - acc: 0.6965 - val_loss: 0.4771 - val_acc: 0.7879\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.6988Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5774 - acc: 0.7002 - val_loss: 0.5560 - val_acc: 0.7803\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.7205Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5460 - acc: 0.7195 - val_loss: 0.4824 - val_acc: 0.7576\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.7462Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5239 - acc: 0.7509 - val_loss: 0.5299 - val_acc: 0.7348\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.7467Epoch 00014: val_loss improved from 0.47711 to 0.44646, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.4981 - acc: 0.7474 - val_loss: 0.4465 - val_acc: 0.8182\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.7997Epoch 00015: val_loss improved from 0.44646 to 0.40413, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4624 - acc: 0.8069 - val_loss: 0.4041 - val_acc: 0.8182\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8073Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4511 - acc: 0.8097 - val_loss: 0.4442 - val_acc: 0.8106\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8101Epoch 00017: val_loss improved from 0.40413 to 0.35057, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4317 - acc: 0.8144 - val_loss: 0.3506 - val_acc: 0.8712\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.8035Epoch 00018: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4367 - acc: 0.8109 - val_loss: 0.3509 - val_acc: 0.8636\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8327Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4057 - acc: 0.8379 - val_loss: 0.3632 - val_acc: 0.8712\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8342Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4013 - acc: 0.8360 - val_loss: 0.3778 - val_acc: 0.8485\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8456Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3980 - acc: 0.8418 - val_loss: 0.3837 - val_acc: 0.8182\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8384Epoch 00022: val_loss improved from 0.35057 to 0.33975, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3907 - acc: 0.8423 - val_loss: 0.3397 - val_acc: 0.8561\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4102 - acc: 0.8246Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.4066 - acc: 0.8252 - val_loss: 0.3587 - val_acc: 0.8409\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.8268Epoch 00024: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4129 - acc: 0.8297 - val_loss: 0.3591 - val_acc: 0.8182\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8555Epoch 00025: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3656 - acc: 0.8589 - val_loss: 0.3913 - val_acc: 0.8561\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8470Epoch 00026: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3654 - acc: 0.8494 - val_loss: 0.3609 - val_acc: 0.8485\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8684Epoch 00027: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3246 - acc: 0.8662 - val_loss: 0.3732 - val_acc: 0.8561\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8595Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3269 - acc: 0.8591 - val_loss: 0.3547 - val_acc: 0.8636\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8592Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3244 - acc: 0.8496 - val_loss: 0.3855 - val_acc: 0.8182\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.8751Epoch 00030: val_loss improved from 0.33975 to 0.33309, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2965 - acc: 0.8727 - val_loss: 0.3331 - val_acc: 0.8106\n",
      "Epoch 32/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.8726Epoch 00031: val_loss improved from 0.33309 to 0.32693, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2972 - acc: 0.8712 - val_loss: 0.3269 - val_acc: 0.8636\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.8588Epoch 00032: val_loss improved from 0.32693 to 0.30967, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3113 - acc: 0.8554 - val_loss: 0.3097 - val_acc: 0.8561\n",
      "Epoch 34/150\n",
      " 1/10 [=>............................] - ETA: 1s - loss: 0.2651 - acc: 0.8828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.303125). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.2946 - acc: 0.8594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.227060). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8834Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2637 - acc: 0.8821 - val_loss: 0.3613 - val_acc: 0.8333\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.8817Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2606 - acc: 0.8783 - val_loss: 0.3767 - val_acc: 0.8333\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.8682Epoch 00035: val_loss improved from 0.30967 to 0.30513, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3143 - acc: 0.8604 - val_loss: 0.3051 - val_acc: 0.8561\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.8841Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2622 - acc: 0.8826 - val_loss: 0.3913 - val_acc: 0.8182\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.8941Epoch 00037: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2460 - acc: 0.8949 - val_loss: 0.3640 - val_acc: 0.8636\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.8710Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2796 - acc: 0.8748 - val_loss: 0.3060 - val_acc: 0.8712\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.8917Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2349 - acc: 0.8963 - val_loss: 0.3344 - val_acc: 0.8636\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9050Epoch 00040: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2189 - acc: 0.9085 - val_loss: 0.3170 - val_acc: 0.8712\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.8964Epoch 00041: val_loss improved from 0.30513 to 0.29888, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2302 - acc: 0.8978 - val_loss: 0.2989 - val_acc: 0.8788\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.8976Epoch 00042: val_loss improved from 0.29888 to 0.27816, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2215 - acc: 0.8991 - val_loss: 0.2782 - val_acc: 0.8864\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9173Epoch 00043: val_loss improved from 0.27816 to 0.27461, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.1924 - acc: 0.9170 - val_loss: 0.2746 - val_acc: 0.8712\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9054Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2002 - acc: 0.9055 - val_loss: 0.2856 - val_acc: 0.8485\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9137Epoch 00045: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2085 - acc: 0.9104 - val_loss: 0.3650 - val_acc: 0.8485\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9148Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2022 - acc: 0.9129 - val_loss: 0.3244 - val_acc: 0.8485\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.8942Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2111 - acc: 0.8983 - val_loss: 0.2786 - val_acc: 0.8712\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9214Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2014 - acc: 0.9227 - val_loss: 0.3551 - val_acc: 0.8712\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9007Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2076 - acc: 0.9048 - val_loss: 0.2770 - val_acc: 0.8636\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9201Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1872 - acc: 0.9207 - val_loss: 0.3457 - val_acc: 0.8561\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9241Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1727 - acc: 0.9255 - val_loss: 0.3235 - val_acc: 0.8788\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9104Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2019 - acc: 0.9082 - val_loss: 0.3402 - val_acc: 0.8636\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9050Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1954 - acc: 0.9099 - val_loss: 0.3250 - val_acc: 0.8485\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9179Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1870 - acc: 0.9162 - val_loss: 0.3156 - val_acc: 0.8788\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9113Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1919 - acc: 0.9120 - val_loss: 0.2817 - val_acc: 0.8712\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9345Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1720 - acc: 0.9327 - val_loss: 0.3433 - val_acc: 0.8636\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9154Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1950 - acc: 0.9142 - val_loss: 0.3586 - val_acc: 0.8409\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9189Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1819 - acc: 0.9177 - val_loss: 0.3087 - val_acc: 0.8409\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9414Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1493 - acc: 0.9417 - val_loss: 0.3212 - val_acc: 0.8712\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9243Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1601 - acc: 0.9234 - val_loss: 0.3104 - val_acc: 0.8485\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9182Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1658 - acc: 0.9204 - val_loss: 0.2982 - val_acc: 0.8788\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9280Epoch 00062: val_loss improved from 0.27461 to 0.26848, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1407 - acc: 0.9326 - val_loss: 0.2685 - val_acc: 0.8939\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9302Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1396 - acc: 0.9335 - val_loss: 0.3017 - val_acc: 0.8864\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9272Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1554 - acc: 0.9298 - val_loss: 0.3045 - val_acc: 0.8409\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9374Epoch 00065: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s - loss: 0.1322 - acc: 0.9376 - val_loss: 0.3182 - val_acc: 0.8333\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9311Epoch 00066: val_loss improved from 0.26848 to 0.25300, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1458 - acc: 0.9326 - val_loss: 0.2530 - val_acc: 0.8864\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9437Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1322 - acc: 0.9436 - val_loss: 0.3058 - val_acc: 0.9091\n",
      "Epoch 69/150\n",
      " 2/10 [====>.........................] - ETA: 2s - loss: 0.1579 - acc: 0.9136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332602). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.167300). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9327Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1478 - acc: 0.9319 - val_loss: 0.3439 - val_acc: 0.8712\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9210Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1897 - acc: 0.9213 - val_loss: 0.3161 - val_acc: 0.8485\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9204Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1759 - acc: 0.9205 - val_loss: 0.2936 - val_acc: 0.8788\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9314Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1516 - acc: 0.9305 - val_loss: 0.2888 - val_acc: 0.9091\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9467Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1255 - acc: 0.9428 - val_loss: 0.3331 - val_acc: 0.8939\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9529Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1115 - acc: 0.9528 - val_loss: 0.3122 - val_acc: 0.8864\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9444Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1265 - acc: 0.9455 - val_loss: 0.3564 - val_acc: 0.8485\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9435Epoch 00075: val_loss improved from 0.25300 to 0.25022, saving model to saved_models/fold5.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1523 - acc: 0.9407 - val_loss: 0.2502 - val_acc: 0.9015\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9410Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1479 - acc: 0.9356 - val_loss: 0.3071 - val_acc: 0.8864\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9476Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1237 - acc: 0.9469 - val_loss: 0.3099 - val_acc: 0.8788\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9282Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1569 - acc: 0.9291 - val_loss: 0.3076 - val_acc: 0.8788\n",
      "Epoch 80/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9368Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1326 - acc: 0.9379 - val_loss: 0.3397 - val_acc: 0.8636\n",
      "Epoch 81/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9503Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1141 - acc: 0.9506 - val_loss: 0.3843 - val_acc: 0.8712\n",
      "Epoch 82/150\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.0797 - acc: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.765188). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.606738). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.448289). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.0850 - acc: 0.9672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.225146). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9491Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1286 - acc: 0.9512 - val_loss: 0.2924 - val_acc: 0.8712\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9474Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1241 - acc: 0.9478 - val_loss: 0.3161 - val_acc: 0.8864\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9605Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1101 - acc: 0.9606 - val_loss: 0.3829 - val_acc: 0.8939\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9374Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1447 - acc: 0.9398 - val_loss: 0.4035 - val_acc: 0.9015\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9422Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1278 - acc: 0.9407 - val_loss: 0.3299 - val_acc: 0.8409\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9568Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1063 - acc: 0.9578 - val_loss: 0.3983 - val_acc: 0.9091\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9662Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0806 - acc: 0.9678 - val_loss: 0.3976 - val_acc: 0.9015\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9660Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0803 - acc: 0.9663 - val_loss: 0.3776 - val_acc: 0.8939\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9756Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0656 - acc: 0.9755 - val_loss: 0.5389 - val_acc: 0.8712\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9740Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0551 - acc: 0.9749 - val_loss: 0.5642 - val_acc: 0.9091\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9646Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0839 - acc: 0.9649 - val_loss: 0.3278 - val_acc: 0.8864\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9680Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0784 - acc: 0.9671 - val_loss: 0.4721 - val_acc: 0.9015\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9717Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0777 - acc: 0.9699 - val_loss: 0.3652 - val_acc: 0.9015\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9771Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0576 - acc: 0.9757 - val_loss: 0.4407 - val_acc: 0.8788\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9828Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0525 - acc: 0.9793 - val_loss: 0.5040 - val_acc: 0.8864\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9810Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0787 - acc: 0.9778 - val_loss: 0.4041 - val_acc: 0.8788\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9703Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0782 - acc: 0.9700 - val_loss: 0.3865 - val_acc: 0.8561\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9734Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0803 - acc: 0.9736 - val_loss: 0.4633 - val_acc: 0.8636\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9654Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0862 - acc: 0.9683 - val_loss: 0.5365 - val_acc: 0.8409\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9521Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1172 - acc: 0.9521 - val_loss: 0.3621 - val_acc: 0.8788\n",
      "Epoch 102/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9552Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0979 - acc: 0.9585 - val_loss: 0.3338 - val_acc: 0.9242\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9742Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0677 - acc: 0.9757 - val_loss: 0.3923 - val_acc: 0.8939\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9805Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0458 - acc: 0.9807 - val_loss: 0.5426 - val_acc: 0.8485\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9852Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0342 - acc: 0.9857 - val_loss: 0.6215 - val_acc: 0.8939\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9779Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0731 - acc: 0.9778 - val_loss: 0.4364 - val_acc: 0.8485\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9875Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0348 - acc: 0.9886 - val_loss: 0.5408 - val_acc: 0.8409\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9795Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0579 - acc: 0.9785 - val_loss: 0.5879 - val_acc: 0.8712\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9555Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1281 - acc: 0.9536 - val_loss: 0.3574 - val_acc: 0.8788\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9579Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1055 - acc: 0.9596 - val_loss: 0.3303 - val_acc: 0.8636\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9589Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0952 - acc: 0.9606 - val_loss: 0.3839 - val_acc: 0.8636\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9828Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0515 - acc: 0.9821 - val_loss: 0.4919 - val_acc: 0.8939\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9795Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0552 - acc: 0.9778 - val_loss: 0.4510 - val_acc: 0.8864\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9761Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0551 - acc: 0.9777 - val_loss: 0.3808 - val_acc: 0.8864\n",
      "Epoch 115/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9859Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0379 - acc: 0.9864 - val_loss: 0.6379 - val_acc: 0.8864\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9808Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0472 - acc: 0.9813 - val_loss: 0.5948 - val_acc: 0.8561\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9605Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1158 - acc: 0.9627 - val_loss: 0.3881 - val_acc: 0.8788\n",
      "Epoch 118/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9607Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0961 - acc: 0.9607 - val_loss: 0.4218 - val_acc: 0.9015\n",
      "Epoch 119/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9667Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0852 - acc: 0.9677 - val_loss: 0.3604 - val_acc: 0.9091\n",
      "Epoch 120/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9883Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0331 - acc: 0.9892 - val_loss: 0.5617 - val_acc: 0.8788\n",
      "Epoch 121/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9883Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0326 - acc: 0.9886 - val_loss: 0.6665 - val_acc: 0.8712\n",
      "Epoch 122/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9808Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0499 - acc: 0.9827 - val_loss: 0.4693 - val_acc: 0.9167\n",
      "Epoch 123/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9766Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0553 - acc: 0.9771 - val_loss: 0.4069 - val_acc: 0.9015\n",
      "Epoch 124/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9842Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0431 - acc: 0.9828 - val_loss: 0.4407 - val_acc: 0.9167\n",
      "Epoch 125/150\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.0498 - acc: 0.9870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.502731). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.608147). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.0523 - acc: 0.9828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.252367). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9867Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0416 - acc: 0.9864 - val_loss: 0.5578 - val_acc: 0.8636\n",
      "Epoch 126/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9857Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0463 - acc: 0.9835 - val_loss: 0.4737 - val_acc: 0.8409\n",
      "Epoch 127/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9857Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0292 - acc: 0.9864 - val_loss: 0.4605 - val_acc: 0.8864\n",
      "Epoch 128/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9922Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0257 - acc: 0.9907 - val_loss: 0.5482 - val_acc: 0.8939\n",
      "Epoch 129/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9945Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0205 - acc: 0.9936 - val_loss: 0.6185 - val_acc: 0.8636\n",
      "Epoch 130/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9922Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0205 - acc: 0.9928 - val_loss: 0.6665 - val_acc: 0.9015\n",
      "Epoch 131/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9873Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0359 - acc: 0.9871 - val_loss: 0.4174 - val_acc: 0.9167\n",
      "Epoch 132/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9914Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0244 - acc: 0.9900 - val_loss: 0.5992 - val_acc: 0.8788\n",
      "Epoch 133/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9878Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0396 - acc: 0.9863 - val_loss: 0.6877 - val_acc: 0.8712\n",
      "Epoch 134/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9781Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0689 - acc: 0.9786 - val_loss: 0.5453 - val_acc: 0.8712\n",
      "Epoch 135/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9805Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0547 - acc: 0.9779 - val_loss: 0.4219 - val_acc: 0.8712\n",
      "Epoch 136/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9930Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0257 - acc: 0.9914 - val_loss: 0.5383 - val_acc: 0.9015\n",
      "Epoch 137/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9930Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0239 - acc: 0.9929 - val_loss: 0.6314 - val_acc: 0.8864\n",
      "Epoch 138/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9969Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0138 - acc: 0.9971 - val_loss: 0.6740 - val_acc: 0.8636\n",
      "Epoch 139/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9891Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.0250 - acc: 0.9886 - val_loss: 0.8206 - val_acc: 0.8561\n",
      "Epoch 140/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9906Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0302 - acc: 0.9914 - val_loss: 0.6230 - val_acc: 0.8864\n",
      "Epoch 141/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9845Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0363 - acc: 0.9841 - val_loss: 0.6866 - val_acc: 0.8561\n",
      "Epoch 142/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9672Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0741 - acc: 0.9686 - val_loss: 0.4767 - val_acc: 0.8636\n",
      "Epoch 143/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9844Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0356 - acc: 0.9836 - val_loss: 0.5511 - val_acc: 0.8788\n",
      "Epoch 144/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9938Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0215 - acc: 0.9929 - val_loss: 0.6103 - val_acc: 0.8939\n",
      "Epoch 145/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9865Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0336 - acc: 0.9857 - val_loss: 0.5029 - val_acc: 0.9015\n",
      "Epoch 146/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9898Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0285 - acc: 0.9900 - val_loss: 0.4542 - val_acc: 0.9091\n",
      "Epoch 147/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9914Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0202 - acc: 0.9921 - val_loss: 0.4355 - val_acc: 0.9167\n",
      "Epoch 148/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.6779 - val_acc: 0.8561\n",
      "Epoch 149/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9938Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0151 - acc: 0.9936 - val_loss: 0.5550 - val_acc: 0.9091\n",
      "Epoch 150/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9977Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0054 - acc: 0.9978 - val_loss: 0.6239 - val_acc: 0.9091\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.361144632907\n",
      "Test accuracy: 0.83783784106\n",
      "Fold: 6\n",
      "Epoch 1/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7127 - acc: 0.5018Epoch 00000: val_loss improved from inf to 0.69309, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 9s - loss: 0.7110 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 2/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4833Epoch 00001: val_loss improved from 0.69309 to 0.69249, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6931 - acc: 0.4878 - val_loss: 0.6925 - val_acc: 0.5076\n",
      "Epoch 3/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5285Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5277 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 4/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5051Epoch 00003: val_loss improved from 0.69249 to 0.68846, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6929 - acc: 0.5052 - val_loss: 0.6885 - val_acc: 0.5076\n",
      "Epoch 5/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.4946Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.4917 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 6/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4815Epoch 00005: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6935 - acc: 0.4772 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 7/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5142Epoch 00006: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.5134 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 8/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4867Epoch 00007: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6932 - acc: 0.4862 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 9/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4932Epoch 00008: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.4918 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 10/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5253Epoch 00009: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5300 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 11/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5053Epoch 00010: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.5052 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 12/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5207Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6927 - acc: 0.5177 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 13/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5137Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5098 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 14/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5321Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6925 - acc: 0.5260 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 15/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4956Epoch 00014: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6933 - acc: 0.5010 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 16/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.4993Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6934 - acc: 0.5017 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 17/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5334Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6923 - acc: 0.5285 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 18/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.4939Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6937 - acc: 0.4951 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 19/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5040Epoch 00018: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6933 - acc: 0.5003 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 20/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5217Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6928 - acc: 0.5182 - val_loss: 0.6929 - val_acc: 0.5076\n",
      "Epoch 21/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7795 - acc: 0.5151Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7723 - acc: 0.5082 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 22/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7331 - acc: 0.5233Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7301 - acc: 0.5199 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 23/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5181Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6926 - acc: 0.5291 - val_loss: 0.6925 - val_acc: 0.5076\n",
      "Epoch 24/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5027Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6934 - acc: 0.4963 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 25/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5206Epoch 00024: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6927 - acc: 0.5207 - val_loss: 0.6914 - val_acc: 0.5076\n",
      "Epoch 26/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5188Epoch 00025: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5217 - val_loss: 0.6909 - val_acc: 0.5076\n",
      "Epoch 27/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.4869Epoch 00026: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6940 - acc: 0.4930 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 28/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5004Epoch 00027: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6932 - acc: 0.5066 - val_loss: 0.6925 - val_acc: 0.5076\n",
      "Epoch 29/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6972 - acc: 0.5171Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6971 - acc: 0.5112 - val_loss: 0.6931 - val_acc: 0.4924\n",
      "Epoch 30/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4837Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6932 - acc: 0.4907 - val_loss: 0.6887 - val_acc: 0.5758\n",
      "Epoch 31/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5411Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6914 - acc: 0.5382 - val_loss: 0.6890 - val_acc: 0.5076\n",
      "Epoch 32/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.5292Epoch 00031: val_loss improved from 0.68846 to 0.54232, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6709 - acc: 0.5357 - val_loss: 0.5423 - val_acc: 0.7500\n",
      "Epoch 33/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.5241Epoch 00032: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.7189 - acc: 0.5245 - val_loss: 0.6912 - val_acc: 0.5076\n",
      "Epoch 34/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.5054Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6941 - acc: 0.4976 - val_loss: 0.6928 - val_acc: 0.5076\n",
      "Epoch 35/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5112Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6925 - acc: 0.5146 - val_loss: 0.6906 - val_acc: 0.5076\n",
      "Epoch 36/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5151Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.5089 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 37/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5310Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6922 - acc: 0.5268 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 38/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6936 - acc: 0.4931Epoch 00037: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6924 - val_acc: 0.5076\n",
      "Epoch 39/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7060 - acc: 0.5145Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7051 - acc: 0.5102 - val_loss: 0.6910 - val_acc: 0.5076\n",
      "Epoch 40/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.5014Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6934 - acc: 0.4896 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 41/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4803Epoch 00040: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 1s - loss: 0.6935 - acc: 0.4766 - val_loss: 0.6931 - val_acc: 0.4924\n",
      "Epoch 42/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5289Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5250 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 43/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.5070Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6906 - acc: 0.5090 - val_loss: 0.6906 - val_acc: 0.5076\n",
      "Epoch 44/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5113Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5077 - val_loss: 0.6914 - val_acc: 0.5076\n",
      "Epoch 45/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4798Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.4899 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 46/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5176Epoch 00045: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5113 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 47/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5032Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6932 - acc: 0.5053 - val_loss: 0.6929 - val_acc: 0.5076\n",
      "Epoch 48/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.4952Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6934 - acc: 0.4938 - val_loss: 0.6931 - val_acc: 0.5076\n",
      "Epoch 49/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5050Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5125 - val_loss: 0.7069 - val_acc: 0.5076\n",
      "Epoch 50/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.5030Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6962 - acc: 0.4954 - val_loss: 0.6930 - val_acc: 0.5076\n",
      "Epoch 51/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5093Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6929 - acc: 0.5083 - val_loss: 0.6923 - val_acc: 0.5076\n",
      "Epoch 52/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.5620Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6915 - acc: 0.5546 - val_loss: 0.6748 - val_acc: 0.5455\n",
      "Epoch 53/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5166Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6928 - acc: 0.5084 - val_loss: 0.6933 - val_acc: 0.4924\n",
      "Epoch 54/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4998Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 55/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5090Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6930 - acc: 0.5144 - val_loss: 0.6900 - val_acc: 0.5076\n",
      "Epoch 56/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5099Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6908 - acc: 0.5132 - val_loss: 0.6833 - val_acc: 0.5076\n",
      "Epoch 57/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.5142Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6888 - acc: 0.5130 - val_loss: 0.6932 - val_acc: 0.4924\n",
      "Epoch 58/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6937 - acc: 0.4751Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6937 - acc: 0.4746 - val_loss: 0.6926 - val_acc: 0.6591\n",
      "Epoch 59/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.5509Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6907 - acc: 0.5419 - val_loss: 0.6654 - val_acc: 0.7273\n",
      "Epoch 60/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6882 - acc: 0.5349Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6815 - acc: 0.5507 - val_loss: 0.7300 - val_acc: 0.5530\n",
      "Epoch 61/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6862 - acc: 0.6207Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6860 - acc: 0.6220 - val_loss: 0.6717 - val_acc: 0.7576\n",
      "Epoch 62/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6769 - acc: 0.6252Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.6769 - acc: 0.6248 - val_loss: 0.6711 - val_acc: 0.6061\n",
      "Epoch 63/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6710 - acc: 0.6362Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6714 - acc: 0.6307 - val_loss: 0.6359 - val_acc: 0.7652\n",
      "Epoch 64/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.5992Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6658 - acc: 0.6069 - val_loss: 0.6166 - val_acc: 0.6667\n",
      "Epoch 65/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6487 - acc: 0.6378Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6451 - acc: 0.6417 - val_loss: 0.5718 - val_acc: 0.7121\n",
      "Epoch 66/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6362 - acc: 0.6373Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6304 - acc: 0.6492 - val_loss: 0.5807 - val_acc: 0.6970\n",
      "Epoch 67/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6120 - acc: 0.6707Epoch 00066: val_loss improved from 0.54232 to 0.50068, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6127 - acc: 0.6681 - val_loss: 0.5007 - val_acc: 0.7652\n",
      "Epoch 68/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.7061Epoch 00067: val_loss improved from 0.50068 to 0.50021, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5927 - acc: 0.7092 - val_loss: 0.5002 - val_acc: 0.7727\n",
      "Epoch 69/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7326Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5570 - acc: 0.7289 - val_loss: 0.5572 - val_acc: 0.7348\n",
      "Epoch 70/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7373Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5479 - acc: 0.7309 - val_loss: 0.5194 - val_acc: 0.7273\n",
      "Epoch 71/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5512 - acc: 0.7329Epoch 00070: val_loss improved from 0.50021 to 0.48172, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5488 - acc: 0.7328 - val_loss: 0.4817 - val_acc: 0.7803\n",
      "Epoch 72/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.7511Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.5213 - acc: 0.7459 - val_loss: 0.4868 - val_acc: 0.7576\n",
      "Epoch 73/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.7474Epoch 00072: val_loss improved from 0.48172 to 0.46843, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5256 - acc: 0.7451 - val_loss: 0.4684 - val_acc: 0.7879\n",
      "Epoch 74/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4974 - acc: 0.7761Epoch 00073: val_loss improved from 0.46843 to 0.44264, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 4s - loss: 0.4933 - acc: 0.7774 - val_loss: 0.4426 - val_acc: 0.8030\n",
      "Epoch 75/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8057Epoch 00074: val_loss improved from 0.44264 to 0.44162, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4663 - acc: 0.8032 - val_loss: 0.4416 - val_acc: 0.7955\n",
      "Epoch 76/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8085Epoch 00075: val_loss improved from 0.44162 to 0.40837, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4446 - acc: 0.8115 - val_loss: 0.4084 - val_acc: 0.8258\n",
      "Epoch 77/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4423 - acc: 0.8181Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4396 - acc: 0.8170 - val_loss: 0.5252 - val_acc: 0.7273\n",
      "Epoch 78/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8104Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4399 - acc: 0.8068 - val_loss: 0.4194 - val_acc: 0.8182\n",
      "Epoch 79/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8376Epoch 00078: val_loss improved from 0.40837 to 0.39454, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3839 - acc: 0.8384 - val_loss: 0.3945 - val_acc: 0.8182\n",
      "Epoch 80/150\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.4234 - acc: 0.7702"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.503690). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.252845). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8220Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.3834 - acc: 0.8202 - val_loss: 0.4070 - val_acc: 0.8030\n",
      "Epoch 81/150\n",
      " 4/10 [==========>...................] - ETA: 3s - loss: 0.3625 - acc: 0.8398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.232526). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.463054). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/10 [======================>.......] - ETA: 1s - loss: 0.3481 - acc: 0.8398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.232528). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.232529). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8310Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 4s - loss: 0.3521 - acc: 0.8353 - val_loss: 0.4006 - val_acc: 0.8182\n",
      "Epoch 82/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3418 - acc: 0.8379Epoch 00081: val_loss improved from 0.39454 to 0.35074, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.3466 - acc: 0.8366 - val_loss: 0.3507 - val_acc: 0.8409\n",
      "Epoch 83/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8409Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3408 - acc: 0.8425 - val_loss: 0.3880 - val_acc: 0.8258\n",
      "Epoch 84/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8516Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3300 - acc: 0.8532 - val_loss: 0.3605 - val_acc: 0.8182\n",
      "Epoch 85/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8608Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3015 - acc: 0.8662 - val_loss: 0.3884 - val_acc: 0.7955\n",
      "Epoch 86/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.8759Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2932 - acc: 0.8784 - val_loss: 0.3665 - val_acc: 0.8182\n",
      "Epoch 87/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.8611Epoch 00086: val_loss improved from 0.35074 to 0.34400, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2927 - acc: 0.8627 - val_loss: 0.3440 - val_acc: 0.8106\n",
      "Epoch 88/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.8607Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2903 - acc: 0.8617 - val_loss: 0.3544 - val_acc: 0.8485\n",
      "Epoch 89/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.8616Epoch 00088: val_loss improved from 0.34400 to 0.33038, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2870 - acc: 0.8648 - val_loss: 0.3304 - val_acc: 0.8409\n",
      "Epoch 90/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8665Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2935 - acc: 0.8666 - val_loss: 0.3844 - val_acc: 0.8409\n",
      "Epoch 91/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3059 - acc: 0.8550Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3041 - acc: 0.8581 - val_loss: 0.3492 - val_acc: 0.8258\n",
      "Epoch 92/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.8700Epoch 00091: val_loss improved from 0.33038 to 0.32966, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2757 - acc: 0.8669 - val_loss: 0.3297 - val_acc: 0.8333\n",
      "Epoch 93/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.8833Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2659 - acc: 0.8776 - val_loss: 0.3806 - val_acc: 0.8258\n",
      "Epoch 94/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.8712Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2790 - acc: 0.8677 - val_loss: 0.3436 - val_acc: 0.8182\n",
      "Epoch 95/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.8739Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2671 - acc: 0.8747 - val_loss: 0.3651 - val_acc: 0.8106\n",
      "Epoch 96/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.8911Epoch 00095: val_loss improved from 0.32966 to 0.32793, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2447 - acc: 0.8912 - val_loss: 0.3279 - val_acc: 0.8258\n",
      "Epoch 97/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.8950Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2404 - acc: 0.8969 - val_loss: 0.3881 - val_acc: 0.8106\n",
      "Epoch 98/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9035Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2286 - acc: 0.9042 - val_loss: 0.4006 - val_acc: 0.8258\n",
      "Epoch 99/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9146Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2202 - acc: 0.9100 - val_loss: 0.3915 - val_acc: 0.8030\n",
      "Epoch 100/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.8997Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2211 - acc: 0.8975 - val_loss: 0.3477 - val_acc: 0.8333\n",
      "Epoch 101/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9177Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2072 - acc: 0.9150 - val_loss: 0.3777 - val_acc: 0.8182\n",
      "Epoch 102/150\n",
      " 5/10 [=============>................] - ETA: 2s - loss: 0.1830 - acc: 0.9234"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.196104). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9209Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1864 - acc: 0.9242 - val_loss: 0.3775 - val_acc: 0.8333\n",
      "Epoch 103/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.8872Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2252 - acc: 0.8904 - val_loss: 0.3829 - val_acc: 0.8258\n",
      "Epoch 104/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9056Epoch 00103: val_loss improved from 0.32793 to 0.31765, saving model to saved_models/fold6.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2231 - acc: 0.9056 - val_loss: 0.3177 - val_acc: 0.8409\n",
      "Epoch 105/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.8950Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2219 - acc: 0.8947 - val_loss: 0.3727 - val_acc: 0.8561\n",
      "Epoch 106/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9157Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2077 - acc: 0.9163 - val_loss: 0.3594 - val_acc: 0.8030\n",
      "Epoch 107/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.8990Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2276 - acc: 0.9010 - val_loss: 0.3418 - val_acc: 0.8409\n",
      "Epoch 108/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9059Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2031 - acc: 0.9069 - val_loss: 0.3681 - val_acc: 0.8409\n",
      "Epoch 109/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9272Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1734 - acc: 0.9269 - val_loss: 0.3716 - val_acc: 0.8636\n",
      "Epoch 110/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9144Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1848 - acc: 0.9149 - val_loss: 0.3846 - val_acc: 0.8106\n",
      "Epoch 111/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9200Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1831 - acc: 0.9211 - val_loss: 0.3383 - val_acc: 0.8409\n",
      "Epoch 112/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9248Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1764 - acc: 0.9257 - val_loss: 0.3698 - val_acc: 0.8182\n",
      "Epoch 113/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9236Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1879 - acc: 0.9241 - val_loss: 0.3853 - val_acc: 0.8182\n",
      "Epoch 114/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9295Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1856 - acc: 0.9257 - val_loss: 0.3566 - val_acc: 0.8409\n",
      "Epoch 115/150\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.1546 - acc: 0.9297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.839318). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.420409). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9133Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2034 - acc: 0.9089 - val_loss: 0.4843 - val_acc: 0.7727\n",
      "Epoch 116/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9006Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2199 - acc: 0.9017 - val_loss: 0.3749 - val_acc: 0.8409\n",
      "Epoch 117/150\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9056Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2194 - acc: 0.9056 - val_loss: 0.3383 - val_acc: 0.8106\n",
      "Epoch 118/150\n",
      " 4/10 [==========>...................] - ETA: 0s - loss: 0.1705 - acc: 0.9241"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ae2a22dce370>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "#X_train_initial2 = X_train_initial[:,:,:, 1:3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_initial, y_train_initial, test_size=0.1, random_state=17)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final test dataset size: {}'.format(len(X_test)))\n",
    "\n",
    "K = 6\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=129).split(X_train, y_train))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=0, width_shift_range = 0, height_shift_range = 0, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "\n",
    "#res = model.fit(train_tensors, train_targets, \n",
    "#          validation_data = (X_valid, y_valid),\n",
    "#          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
    "mean_loss = 0\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print(\"Fold: {}\".format(i))\n",
    "    file_name = 'saved_models/fold{}.weights.best.from_scratch.hdf5'.format(i)\n",
    "\n",
    "    model, checkpointer = init_model(getModel11(3), file_name)\n",
    "    \n",
    "    X_train_ = X_train[train_idx]\n",
    "    y_train_ = y_train[train_idx]\n",
    "    X_valid_ = X_train[valid_idx]\n",
    "    y_valid_ =  y_train[valid_idx]\n",
    "\n",
    "    datagen.fit(X_train_)\n",
    "\n",
    "    res = model.fit_generator(datagen.flow(X_train_, y_train_, batch_size=batch_size),  steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid_, y_valid_), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    \n",
    "    prediction_model = load_model(file_name )\n",
    "    \n",
    "    test_score = prediction_model.evaluate(X_test, y_test)\n",
    " \n",
    "    print('Test loss:', test_score[0])\n",
    "    print('Test accuracy:', test_score[1])\n",
    "    \n",
    "    mean_loss += test_score[0] / K\n",
    "        \n",
    "#res = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "#                    steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid, y_valid), verbose=1, callbacks=[checkpointer])\n",
    "\n",
    "print('Mean loss:',   mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDoAAAQ8CAYAAABzdP6kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2311f052128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphs = [res]\n",
    "\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, hist in enumerate(graphs):\n",
    "    ax1 = fig.add_subplot(110 + i + 1)\n",
    "    #plt.setp([ax1], xticks=[], yticks=[])\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_ylabel('loss')\n",
    "    #ax1.set_xlabel('epoch')\n",
    "    ax1.plot(hist.history['loss'])\n",
    "    ax1.plot(hist.history['val_loss'])\n",
    "    ax1.xaxis.set_ticks(np.arange(0, epochs, epochs // 10))\n",
    "    ax1.yaxis.set_ticks(np.arange(0, 1, 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test.json'\n",
    "test_data = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test, y_test, ids = prepare_dataset_ignore_angle(test_data, global_min, global_max)\n",
    "X_test, y_test, ids = prepare_dataset_with_angles(test_data, ptocessing_lambda, ptocessing_angle_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "prediction_model = load_model('saved_models/fold1.weights.best.from_scratch.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_model_0 = load_model('saved_models/fold0.weights.best.from_scratch.hdf5')\n",
    "prediction_model_1 = load_model('saved_models/fold1.weights.best.from_scratch.hdf5')\n",
    "prediction_model_2 = load_model('saved_models/fold2.weights.best.from_scratch.hdf5')\n",
    "prediction_model_3 = load_model('saved_models/fold3.weights.best.from_scratch.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train__, X_test__, y_train__, y_test__ = train_test_split(X_train_initial, y_train_initial, test_size=0.1, random_state=147)\n",
    "\n",
    "res1 = prediction_model_0.predict(X_train__)\n",
    "res2 = prediction_model_1.predict(X_train__)\n",
    "res3 = prediction_model_2.predict(X_train__)\n",
    "res4 = prediction_model_3.predict(X_train__)\n",
    "\n",
    "res1v = prediction_model_0.predict(X_test__)\n",
    "res2v = prediction_model_1.predict(X_test__)\n",
    "res3v = prediction_model_2.predict(X_test__)\n",
    "res4v = prediction_model_3.predict(X_test__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.84739224e-06]\n"
     ]
    }
   ],
   "source": [
    "print(res4v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1323,)\n",
      "0.161517681746\n",
      "0.210019761456\n"
     ]
    }
   ],
   "source": [
    "res_f = np.concatenate([res1, res2], axis = 1)\n",
    "res_f_v = np.concatenate([res1v, res2v], axis = 1)\\\n",
    "\n",
    "res_f_mean = np.mean(res_f, axis=1)\n",
    "res_f_v_mean = np.mean(res_f_v, axis=1)\n",
    "print(res_f_mean.shape)\n",
    "\n",
    "print(log_loss(y_train__, res_f_mean))\n",
    "print(log_loss(y_test__, res_f_v_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = np.sort(res_f_v, axis=1, kind='quicksort', order=None)\n",
    "d1 = np.sort(res_f_v, axis=1, kind='quicksort', order=None)\n",
    "\n",
    "rr = []\n",
    "for item in d1:\n",
    "    if np.mean(item) > 0.5:\n",
    "        rr.append(np.max(item))\n",
    "    else:\n",
    "        rr.append(np.min(item))\n",
    "\n",
    "print(y_test__.shape)\n",
    "d = np.concatenate([d, np.expand_dims(y_test__, axis=1)], axis = 1)\n",
    "\n",
    "#d = d[:,3:4]\n",
    "print(log_loss(y_test__, rr))\n",
    "\n",
    "for i in range(132):\n",
    "    print(i, d[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_test__, 6)\n",
    "display_image(hh_channel)\n",
    "display_image(hv_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lrc = AdaBoostClassifier(random_state = 3)\n",
    "#lrc = SVC(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(res_f, y_train__)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(res_f_v)\n",
    "predictions_train = lrc.predict(res_f)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train__, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train__, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_test__, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_test__, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(lrc.__class__.__name__, len(res_f)))\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 2)\n"
     ]
    }
   ],
   "source": [
    "res1 = prediction_model_0.predict(X_test)\n",
    "res2 = prediction_model_1.predict(X_test)\n",
    "res_t = np.concatenate([res1, res2], axis = 1)\n",
    "print(res_t.shape)\n",
    "res = np.mean(res_t, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424,)\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'is_iceberg']\n",
    "\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for prediction, item_id in zip(res, ids):\n",
    "        csv_writer.writerow([item_id, np.asscalar(prediction)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
