{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Capstone: Iceberg Classifier\n",
    "\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Icebergs presents threats to the ships navigation and various offshore activities. Especially, it as actual problem for the area offshore to Newfoundland and Labrador known as Iceberg Alley. The primary iceberg detection method for now is aerial reconnaissance using vessel-based monitoring data. Also, data received though satellites are widely being integrated now onto the monitoring systems greatly reduce monitoring cost. Additionally, Synthetic Aperture Radar (SAR) satellites can still monitor in various weather conditions such as clouds and fog.\n",
    "However, manual visual classification of SAR images to identify iceberg is very time-consuming process. So, C‑CORE company (https://www.c-core.ca/) has developed a computer vision system that analyzes SAR data to automatically detect and classify icebergs and vessels. Now it challenges ML community to build effective classification algorithm for their detection system [1]\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of the project is to build an algorithm which can reliably classify data to identify either it is iceberg or ship, based on given Synthetic Aperture Radar data. Also, the results are clearly measurable using prediction accuracy and it is important to have classifier with higher accuracy (ideally 100%).\n",
    "Additionally, analysis and classification SAR data is interesting problem. Even if it seems like standard image classification task it has some important differences which makes it challengeable to use pre-trained neural networks with transfer learning for the image classification such as VGG [2] or Inception [3]:\n",
    "• SAR data is not a three-channels regular image\n",
    "• Radar detected shapes are different than visually detected shapes.\n",
    "• Data set has additional incidence angle parameter of which the image was taken. So, it is additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "CORE provided dataset of satellite SAR images containing either a ship or an iceberg including 1604 training samples and 8424 test samples (5000 from them are autogenerated) Data was collected from SAR which bounces a signal off an object and records the echo, then that data is translated into an image. Two channels of image are provided: HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). See [4] for more details. Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format:\n",
    "\n",
    "Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format: \n",
    "* Id of the image \n",
    "* band1 – flatten image data of the HH channel (5625 elements, 75x75 image), each element is float value measured in dB. \n",
    "* band2 – flatten image data of the HV channel (5625 elements, 75x75 image) , each element is float value measured in dB. \n",
    "* inc_angle - the incidence angle of which the image was taken \n",
    "* is_iceberg – classification label of the image. 1 is for iceberg, 0 for ship.\n",
    "\n",
    "Training dataset the only dataset which has labels assigned – so it will be used for training and validation. Test dataset does not have labels and will be used for the model evaluation.\n",
    "For the model features we are going to use band1, band2 and inc_angle data as features and is_iceberg field as labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to load data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to find global dataset characteristics (signal strength min/max, angle min/max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_signal_minmax(data):\n",
    "    mins = [min(min(item['band_1']), min(item['band_2'])) for item in data ]\n",
    "    maxes = [max(max(item['band_1']), max(item['band_2'])) for item in data ]\n",
    "\n",
    "    global_min = min(mins)\n",
    "    global_max = max(maxes)\n",
    "    \n",
    "    return global_min, global_max\n",
    "\n",
    "def find_angle_minmax(data):\n",
    "    global_min  = min( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'  ])\n",
    "    global_max = max( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'])\n",
    "  \n",
    "    return global_min, global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to extract and display 75*75 image from raw SAR JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_image(data_item, key, image_size = 75):\n",
    "    image = np.array(data_item[key])\n",
    "    image = image.reshape(image_size, image_size)\n",
    "    return image\n",
    "    \n",
    "def extract_images(data_item, image_size = 75):\n",
    "    hh_image = extract_image(data_item, 'band_1', image_size)\n",
    "    hv_image = extract_image(data_item, 'band_2', image_size)\n",
    "    return hh_image, hv_image\n",
    "     \n",
    "def display_image(image, cmap='gray'):\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image pre-processing and normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_min_max_scale_sar_image(image, global_min, global_max):\n",
    "    image = (image - global_min) / (global_max - global_min)\n",
    "    return image\n",
    "\n",
    "def local_min_max_scale_sar_image(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    \n",
    "    image = (image - img_min) / (img_max - img_min)\n",
    "    return image\n",
    "\n",
    "def local_standard_scale_sar_image(image):\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    \n",
    "    image = (image - np.ones_like(image) * img_mean) / img_std;\n",
    "    \n",
    "    return image\n",
    "\n",
    "def flatten_image(image):\n",
    "    image = image.flatten()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 2-channel Images for CNN from HH and HV SAR channels ignoring angle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_ignore_angles(data, process_func):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        hh_image, hv_image = extract_images(item)\n",
    "        hh_image = process_func(hh_image)\n",
    "        hv_image = process_func(hv_image)\n",
    "        \n",
    "        image = np.dstack((hh_image, hv_image))\n",
    "        X.append(image)\n",
    "        if 'is_iceberg' in item.keys():\n",
    "            labels.append(item['is_iceberg'])\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            \n",
    "        ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3-channel Images for CNN from HH and HV SAR channels + angle data as separate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image)\n",
    "            hv_image = process_func(hv_image)\n",
    "            angle_layer = np.ones_like(hh_image) * angle_processing(angle)\n",
    "            image = np.dstack((hh_image, hv_image, angle_layer))\n",
    "            X.append(image)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create flat features for simple classifiers algoirithms like Logistic Regression, Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_flat_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image).flatten()\n",
    "            hv_image = process_func(hv_image).flatten()\n",
    "            angle_layer = angle_processing(angle)\n",
    "            x_item = np.concatenate((hh_image, hv_image, [angle_layer]))\n",
    "            X.append(x_item)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_image_part(data, margin):\n",
    "    return data[:, margin : -margin, margin : - margin, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1604\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train.json'\n",
    "train_data = read_data(train_file)\n",
    "print('Training dataset size: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum incidence angle = 24.7546, Maximum incidence angle = 45.9375\n",
      "Minimum signal strength (dB)= -45.655499, Maximum signal strength (dB) = 34.574917\n"
     ]
    }
   ],
   "source": [
    "image_size = 75\n",
    "\n",
    "angle_min, angle_max = find_angle_minmax(train_data)\n",
    "global_min, global_max = find_signal_minmax(train_data)\n",
    "\n",
    "print(\"Minimum incidence angle = {}, Maximum incidence angle = {}\".format(angle_min, angle_max))\n",
    "print(\"Minimum signal strength (dB)= {}, Maximum signal strength (dB) = {}\".format(global_min, global_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and run image workflow to  extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    #image = cv2.bilateralFilter(image.astype(np.float32), 5, 80, 80)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "#X_train_initial, y_train_initial, _ = prepare_dataset_ignore_angles(train_data, ptocessing_lambda)\n",
    "X_train_initial, y_train_initial, _ = prepare_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "#X_train, y_train, _ = prepare_dataset_with_angle(train_data, global_min, global_max, angle_min, angle_max)\n",
    "print('Training dataset size: {}'.format(len(X_train_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_channels(data, index):\n",
    "    hh_channel = data[index, :, :, 0]\n",
    "    hv_channel = data[index, :, :, 1]\n",
    "    return hh_channel, hv_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4RcKE0R9+IXCBFEffqEwQfx/hoOKLrFdm60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb2833d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb180f3f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb26d796a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 24)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 25)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 26)\n",
    "display_image(hh_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb28364cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb18034da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb282c1b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb0ff8dd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb282efc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1250\n",
    "\n",
    "hh_image, hv_image =  get_image_channels(X_train_initial, index)\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "print(y_train_initial[index])\n",
    "\n",
    "import cv2\n",
    "\n",
    "hh_image = cv2.bilateralFilter(hh_image.astype(np.float32), 5, 80, 80)\n",
    "hv_image = cv2.bilateralFilter(hv_image.astype(np.float32), 5, 80, 80)\n",
    "\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "\n",
    "image = np.dstack((hh_image, hh_image, np.zeros_like(hv_image)))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "X_flat_initial, y_flat_initial, _ = prepare_flat_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "print('Training dataset size: {}'.format(len(X_flat_initial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat_initial, y_flat_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 1176 samples.\n",
      "{'acc_test': 0.80000000000000004, 'pred_time': 0.013001203536987305, 'train_time': 2.2691128253936768, 'log_loss_test': 6.9078393044748019, 'acc_train': 1.0, 'log_loss_train': 9.9920072216264148e-16}\n",
      "AdaBoostClassifier trained on 1176 samples.\n",
      "{'acc_test': 0.7593220338983051, 'pred_time': 0.190018892288208, 'train_time': 53.97029089927673, 'log_loss_test': 8.3128281638591055, 'acc_train': 0.95999999999999996, 'log_loss_train': 1.3815643824202646}\n",
      "SVC trained on 1176 samples.\n",
      "{'acc_test': 0.74237288135593216, 'pred_time': 6.02459454536438, 'train_time': 13.175135850906372, 'log_loss_test': 8.898258258600988, 'acc_train': 0.73999999999999999, 'log_loss_train': 8.9802097982656104}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "clf1= LogisticRegression(random_state = 3)\n",
    "clf2 = AdaBoostClassifier(random_state = 3)\n",
    "clf3 = SVC(random_state = 3)\n",
    "clf4 = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    start = time() # Get start time\n",
    "  \n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    X_train_pred = X_train[:300]\n",
    "    y_train_pred = y_train[:300]\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train_pred)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train_pred, predictions_train)\n",
    "    results['log_loss_train'] = log_loss(y_train_pred, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    results['log_loss_test'] = log_loss(y_test, predictions_test)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, len(X_train)))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "print(train_predict(clf1, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf2, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf3, X_train, y_train, X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8e87bc4f6966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# TODO: Fit the grid search object to the training data and find the optimal parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrid_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Get the estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 for train, test in cv)\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1675\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "clf_default = AdaBoostClassifier(random_state = 3, base_estimator = dtc) \n",
    "clf = AdaBoostClassifier(random_state = 3, base_estimator = dtc)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = { 'n_estimators': [5, 10, 20, 50, 100], \\\n",
    "              # 'base_estimator__criterion' : ['gini', 'entropy'], \\\n",
    "               'base_estimator__max_depth': [2, 3, 5, 10, None] \\\n",
    "             #  'base_estimator__min_samples_split': [2, 5, 10, 50], \\\n",
    "             #  'base_estimator__class_weight': [{ 1 : 0.75}, { 1 : 1}] \n",
    "             } \n",
    "\n",
    "scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring = scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def_model = clf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "Accuracy score on testing data: 0.7254\n",
      "log_loss on testing data: 9.4837\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.7254\n",
      "Final log_loss on the testing data: 9.4837\n",
      "{'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = def_model.predict(X_valid)\n",
    "best_predictions = best_clf.predict(X_valid)\n",
    "\n",
    "print (\"Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\n",
    "print (\"log_loss on testing data: {:.4f}\".format(log_loss(y_valid, predictions)))\n",
    "\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\n",
    "print (\"Final log_loss on the testing data: {:.4f}\".format(log_loss(y_valid, best_predictions)))\n",
    "                                                                     \n",
    "print (grid_fit.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 935\n",
      "Final validation dataset size: 462\n",
      "Final tet dataset size: 74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_train_initial, y_train_initial, test_size=0.05, random_state=142)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_1, y_train_1, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "print('Final tet dataset size: {}'.format(len(X_test_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range = 0.05, height_shift_range = 0.05, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#X_train_new = np.copy(X_train)\n",
    "#Y_train_new = np.copy(y_train)\n",
    "\n",
    "#for i in range(40):\n",
    "#    X_batch, y_batch =  next(datagen.flow(X_train, y_train, batch_size=2000))\n",
    "#    if i == 0:\n",
    "#        X_train_new = X_batch\n",
    "#        Y_train_new = y_batch\n",
    "#    else: \n",
    "#        X_train_new = np.concatenate((X_train_new, X_batch), axis=0)\n",
    "#        Y_train_new = np.concatenate((Y_train_new, y_batch), axis=0)\n",
    "    \n",
    "    #plt.imshow(X_batch[24, :, :, 0])\n",
    "    #plt.show()\n",
    "                                    \n",
    "    \n",
    "#print(X_batch.shape)\n",
    "\n",
    "\n",
    "#print (X_batch.shape)\n",
    "##X_train = cut_image_part(X_train_new, 10)\n",
    "#print (X_train_new.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexeys\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_initial, y_train_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "\n",
    "features_train = []\n",
    "features_valid = []\n",
    "\n",
    "for x in X_train:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_train.append(flat)\n",
    "\n",
    "for x in X_valid:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_valid.append(flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 1176 samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc_test': 0.8067796610169492,\n",
       " 'acc_train': 0.81037414965986398,\n",
       " 'log_loss_test': 6.6736943715756825,\n",
       " 'log_loss_train': 6.549559751726858,\n",
       " 'pred_time': 4.1282172203063965,\n",
       " 'train_time': 3.5089163780212402}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc = LogisticRegression(random_state = 3)\n",
    "lrc = SVC(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(features_train, y_train)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(features_valid)\n",
    "predictions_train = lrc.predict(features_train)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_valid, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_valid, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(lrc.__class__.__name__, len(features_train)))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "entities_count = 20\n",
    "\n",
    "def getModel10(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def getModel20(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def getTransferLearningModel():\n",
    "    activation = 'elu'\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    output = base_model.output\n",
    "    \n",
    "    layer = GlobalMaxPooling2D()(output)\n",
    "    layer = Dense(512, activation='relu', name='fc2')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(512, activation='relu', name='fc3')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    predictions = Dense(1, activation='sigmoid')(layer)\n",
    "   \n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel30(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel32(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    " \n",
    "\n",
    "    \n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel31(num_layers):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel40(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False,\n",
    "                     kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def getModel50(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_221 (Conv2D)          (None, 75, 75, 16)        448       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 38, 38, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_222 (Conv2D)          (None, 38, 38, 32)        4640      \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_223 (Conv2D)          (None, 19, 19, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 843,041\n",
      "Trainable params: 843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel10(3)\n",
    "#model = getTransferLearningModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1397\n",
      "Final test dataset size: 74\n",
      "Fold: 0\n",
      "Epoch 1/200\n",
      " 9/21 [===========>..................] - ETA: 8s - loss: 0.7186 - acc: 0.5434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.584000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.280000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.140999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.6968 - acc: 0.5477Epoch 00000: val_loss improved from inf to 0.64429, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 10s - loss: 0.6941 - acc: 0.5525 - val_loss: 0.6443 - val_acc: 0.6545\n",
      "Epoch 2/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.6303 - acc: 0.6435Epoch 00001: val_loss improved from 0.64429 to 0.61741, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.6284 - acc: 0.6453 - val_loss: 0.6174 - val_acc: 0.6588\n",
      "Epoch 3/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.6024 - acc: 0.6703Epoch 00002: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.6054 - acc: 0.6645 - val_loss: 0.6502 - val_acc: 0.5944\n",
      "Epoch 4/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.6004 - acc: 0.6821Epoch 00003: val_loss improved from 0.61741 to 0.58756, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 2s - loss: 0.6012 - acc: 0.6795 - val_loss: 0.5876 - val_acc: 0.6910\n",
      "Epoch 5/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5930 - acc: 0.6944Epoch 00004: val_loss improved from 0.58756 to 0.58011, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.5902 - acc: 0.6983 - val_loss: 0.5801 - val_acc: 0.6953\n",
      "Epoch 6/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5685 - acc: 0.7197Epoch 00005: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.5710 - acc: 0.7176 - val_loss: 0.6060 - val_acc: 0.6674\n",
      "Epoch 7/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5769 - acc: 0.7018Epoch 00006: val_loss improved from 0.58011 to 0.57697, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.5778 - acc: 0.7034 - val_loss: 0.5770 - val_acc: 0.6974\n",
      "Epoch 8/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.5684 - acc: 0.7084Epoch 00007: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.5637 - acc: 0.7107 - val_loss: 0.5927 - val_acc: 0.6717\n",
      "Epoch 9/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5642 - acc: 0.7175Epoch 00008: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.5679 - acc: 0.7134 - val_loss: 0.6024 - val_acc: 0.6717\n",
      "Epoch 10/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5473 - acc: 0.7315Epoch 00009: val_loss improved from 0.57697 to 0.54698, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 2s - loss: 0.5495 - acc: 0.7281 - val_loss: 0.5470 - val_acc: 0.7103\n",
      "Epoch 11/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.5213 - acc: 0.7489Epoch 00010: val_loss improved from 0.54698 to 0.51658, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.5246 - acc: 0.7475 - val_loss: 0.5166 - val_acc: 0.7253\n",
      "Epoch 12/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4943 - acc: 0.7634Epoch 00011: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.4944 - acc: 0.7622 - val_loss: 0.5231 - val_acc: 0.7167\n",
      "Epoch 13/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.5185 - acc: 0.7439Epoch 00012: val_loss improved from 0.51658 to 0.49263, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 2s - loss: 0.5213 - acc: 0.7397 - val_loss: 0.4926 - val_acc: 0.7489\n",
      "Epoch 14/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4671 - acc: 0.7721Epoch 00013: val_loss improved from 0.49263 to 0.48613, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.4641 - acc: 0.7754 - val_loss: 0.4861 - val_acc: 0.7597\n",
      "Epoch 15/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4501 - acc: 0.7803Epoch 00014: val_loss improved from 0.48613 to 0.46854, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.4528 - acc: 0.7787 - val_loss: 0.4685 - val_acc: 0.7768\n",
      "Epoch 16/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4352 - acc: 0.7897Epoch 00015: val_loss improved from 0.46854 to 0.41921, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.4300 - acc: 0.7943 - val_loss: 0.4192 - val_acc: 0.7854\n",
      "Epoch 17/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4053 - acc: 0.8058Epoch 00016: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.4076 - acc: 0.8061 - val_loss: 0.4556 - val_acc: 0.7833\n",
      "Epoch 18/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4560 - acc: 0.7729Epoch 00017: val_loss improved from 0.41921 to 0.41911, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.4515 - acc: 0.7770 - val_loss: 0.4191 - val_acc: 0.7854\n",
      "Epoch 19/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.4072 - acc: 0.7999Epoch 00018: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.4228 - acc: 0.7933 - val_loss: 0.4796 - val_acc: 0.7811\n",
      "Epoch 20/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3893 - acc: 0.8165Epoch 00019: val_loss improved from 0.41911 to 0.40829, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3885 - acc: 0.8169 - val_loss: 0.4083 - val_acc: 0.8026\n",
      "Epoch 21/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3834 - acc: 0.8169Epoch 00020: val_loss improved from 0.40829 to 0.39001, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3798 - acc: 0.8180 - val_loss: 0.3900 - val_acc: 0.8283\n",
      "Epoch 22/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3803 - acc: 0.8239Epoch 00021: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3820 - acc: 0.8261 - val_loss: 0.3922 - val_acc: 0.8133\n",
      "Epoch 23/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3822 - acc: 0.8102Epoch 00022: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3867 - acc: 0.8067 - val_loss: 0.5973 - val_acc: 0.7210\n",
      "Epoch 24/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3969 - acc: 0.8076Epoch 00023: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3928 - acc: 0.8093 - val_loss: 0.4028 - val_acc: 0.8219\n",
      "Epoch 25/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3795 - acc: 0.8201Epoch 00024: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3807 - acc: 0.8183 - val_loss: 0.3928 - val_acc: 0.8155\n",
      "Epoch 26/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3739 - acc: 0.8283Epoch 00025: val_loss improved from 0.39001 to 0.36477, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3716 - acc: 0.8276 - val_loss: 0.3648 - val_acc: 0.8391\n",
      "Epoch 27/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3611 - acc: 0.8281Epoch 00026: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3590 - acc: 0.8310 - val_loss: 0.4329 - val_acc: 0.7983\n",
      "Epoch 28/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.3861 - acc: 0.8037Epoch 00027: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3753 - acc: 0.8121 - val_loss: 0.3677 - val_acc: 0.8240\n",
      "Epoch 29/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3478 - acc: 0.8319Epoch 00028: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3463 - acc: 0.8338 - val_loss: 0.3658 - val_acc: 0.8262\n",
      "Epoch 30/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3544 - acc: 0.8254Epoch 00029: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/21 [==============================] - 1s - loss: 0.3499 - acc: 0.8305 - val_loss: 0.3696 - val_acc: 0.8176\n",
      "Epoch 31/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3748 - acc: 0.8320Epoch 00030: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3711 - acc: 0.8354 - val_loss: 0.3923 - val_acc: 0.8305\n",
      "Epoch 32/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3533 - acc: 0.8350Epoch 00031: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3527 - acc: 0.8354 - val_loss: 0.4701 - val_acc: 0.7790\n",
      "Epoch 33/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3642 - acc: 0.8297Epoch 00032: val_loss improved from 0.36477 to 0.35551, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3604 - acc: 0.8303 - val_loss: 0.3555 - val_acc: 0.8305\n",
      "Epoch 34/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3386 - acc: 0.8470Epoch 00033: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3351 - acc: 0.8489 - val_loss: 0.3571 - val_acc: 0.8240\n",
      "Epoch 35/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3402 - acc: 0.8437Epoch 00034: val_loss improved from 0.35551 to 0.34700, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3404 - acc: 0.8422 - val_loss: 0.3470 - val_acc: 0.8391\n",
      "Epoch 36/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3360 - acc: 0.8336Epoch 00035: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3297 - acc: 0.8377 - val_loss: 0.4012 - val_acc: 0.8219\n",
      "Epoch 37/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3357 - acc: 0.8362Epoch 00036: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3366 - acc: 0.8373 - val_loss: 0.3669 - val_acc: 0.8519\n",
      "Epoch 38/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3461 - acc: 0.8450Epoch 00037: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3470 - acc: 0.8421 - val_loss: 0.4091 - val_acc: 0.8026\n",
      "Epoch 39/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3526 - acc: 0.8382Epoch 00038: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3514 - acc: 0.8377 - val_loss: 0.3606 - val_acc: 0.8391\n",
      "Epoch 40/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3289 - acc: 0.8456Epoch 00039: val_loss improved from 0.34700 to 0.34657, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3280 - acc: 0.8464 - val_loss: 0.3466 - val_acc: 0.8391\n",
      "Epoch 41/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.3130 - acc: 0.8565Epoch 00040: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3168 - acc: 0.8546 - val_loss: 0.3617 - val_acc: 0.8498\n",
      "Epoch 42/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3274 - acc: 0.8562Epoch 00041: val_loss improved from 0.34657 to 0.33894, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3266 - acc: 0.8549 - val_loss: 0.3389 - val_acc: 0.8455\n",
      "Epoch 43/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3105 - acc: 0.8567- ETA: 0s - loss: 0.3110 - acc: 0.Epoch 00042: val_loss improved from 0.33894 to 0.33638, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3128 - acc: 0.8539 - val_loss: 0.3364 - val_acc: 0.8519\n",
      "Epoch 44/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3181 - acc: 0.8610Epoch 00043: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3214 - acc: 0.8589 - val_loss: 0.3468 - val_acc: 0.8348\n",
      "Epoch 45/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3145 - acc: 0.8636Epoch 00044: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3195 - acc: 0.8582 - val_loss: 0.3462 - val_acc: 0.8433\n",
      "Epoch 46/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3168 - acc: 0.8570Epoch 00045: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3168 - acc: 0.8550 - val_loss: 0.3810 - val_acc: 0.8176\n",
      "Epoch 47/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3250 - acc: 0.8573Epoch 00046: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3271 - acc: 0.8560 - val_loss: 0.3798 - val_acc: 0.8219\n",
      "Epoch 48/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3017 - acc: 0.8538Epoch 00047: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3042 - acc: 0.8507 - val_loss: 0.3587 - val_acc: 0.8476\n",
      "Epoch 49/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2941 - acc: 0.8658Epoch 00048: val_loss improved from 0.33638 to 0.33567, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.2985 - acc: 0.8634 - val_loss: 0.3357 - val_acc: 0.8412\n",
      "Epoch 50/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3282 - acc: 0.8479Epoch 00049: val_loss improved from 0.33567 to 0.32819, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.3291 - acc: 0.8462 - val_loss: 0.3282 - val_acc: 0.8348\n",
      "Epoch 51/200\n",
      " 3/21 [===>..........................] - ETA: 2s - loss: 0.2880 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.128251). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/21 [==================>...........] - ETA: 0s - loss: 0.2869 - acc: 0.8609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.255001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.228751). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.202501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.102250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2991 - acc: 0.8559Epoch 00050: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2975 - acc: 0.8574 - val_loss: 0.3490 - val_acc: 0.8283\n",
      "Epoch 52/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3170 - acc: 0.8582Epoch 00051: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3196 - acc: 0.8553 - val_loss: 0.3821 - val_acc: 0.8069\n",
      "Epoch 53/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3133 - acc: 0.8624Epoch 00052: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3084 - acc: 0.8644 - val_loss: 0.3353 - val_acc: 0.8541\n",
      "Epoch 54/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2953 - acc: 0.8527Epoch 00053: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2948 - acc: 0.8515 - val_loss: 0.3366 - val_acc: 0.8455\n",
      "Epoch 55/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3066 - acc: 0.8533Epoch 00054: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3084 - acc: 0.8507 - val_loss: 0.3664 - val_acc: 0.8391\n",
      "Epoch 56/200\n",
      "12/21 [===============>..............] - ETA: 1s - loss: 0.3238 - acc: 0.8518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.981501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.602001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.222501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.112500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3127 - acc: 0.8579Epoch 00055: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3112 - acc: 0.8595 - val_loss: 0.3831 - val_acc: 0.8391\n",
      "Epoch 57/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3268 - acc: 0.8577Epoch 00056: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3257 - acc: 0.8570 - val_loss: 0.3816 - val_acc: 0.8391\n",
      "Epoch 58/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2884 - acc: 0.8580Epoch 00057: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2914 - acc: 0.8541 - val_loss: 0.3659 - val_acc: 0.8498\n",
      "Epoch 59/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2958 - acc: 0.8718Epoch 00058: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2964 - acc: 0.8712 - val_loss: 0.3468 - val_acc: 0.8498\n",
      "Epoch 60/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.3018 - acc: 0.8554Epoch 00059: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2999 - acc: 0.8566 - val_loss: 0.3470 - val_acc: 0.8369\n",
      "Epoch 61/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2892 - acc: 0.8549Epoch 00060: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2890 - acc: 0.8559 - val_loss: 0.3729 - val_acc: 0.8498\n",
      "Epoch 62/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3016 - acc: 0.8665Epoch 00061: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3008 - acc: 0.8668 - val_loss: 0.3686 - val_acc: 0.8197\n",
      "Epoch 63/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2895 - acc: 0.8538Epoch 00062: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2882 - acc: 0.8549 - val_loss: 0.3784 - val_acc: 0.8391\n",
      "Epoch 64/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3157 - acc: 0.8501Epoch 00063: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3113 - acc: 0.8535 - val_loss: 0.3434 - val_acc: 0.8391\n",
      "Epoch 65/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3002 - acc: 0.8692Epoch 00064: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3003 - acc: 0.8694 - val_loss: 0.3433 - val_acc: 0.8605\n",
      "Epoch 66/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2921 - acc: 0.8554Epoch 00065: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2899 - acc: 0.8577 - val_loss: 0.3666 - val_acc: 0.8455\n",
      "Epoch 67/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2993 - acc: 0.8608Epoch 00066: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2977 - acc: 0.8607 - val_loss: 0.3511 - val_acc: 0.8412\n",
      "Epoch 68/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2993 - acc: 0.8704Epoch 00067: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2986 - acc: 0.8685 - val_loss: 0.3419 - val_acc: 0.8412\n",
      "Epoch 69/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3069 - acc: 0.8507Epoch 00068: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3074 - acc: 0.8527 - val_loss: 0.4917 - val_acc: 0.7897\n",
      "Epoch 70/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.3222 - acc: 0.8495Epoch 00069: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3181 - acc: 0.8544 - val_loss: 0.3551 - val_acc: 0.8476\n",
      "Epoch 71/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2827 - acc: 0.8621Epoch 00070: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2871 - acc: 0.8620 - val_loss: 0.3481 - val_acc: 0.8412\n",
      "Epoch 72/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3049 - acc: 0.8645Epoch 00071: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.3025 - acc: 0.8671 - val_loss: 0.3345 - val_acc: 0.8412\n",
      "Epoch 73/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2943 - acc: 0.8586Epoch 00072: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2961 - acc: 0.8552 - val_loss: 0.3303 - val_acc: 0.8433\n",
      "Epoch 74/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2831 - acc: 0.8668Epoch 00073: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2858 - acc: 0.8672 - val_loss: 0.3565 - val_acc: 0.8412\n",
      "Epoch 75/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2870 - acc: 0.8509Epoch 00074: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2830 - acc: 0.8527 - val_loss: 0.3301 - val_acc: 0.8498\n",
      "Epoch 76/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2954 - acc: 0.8612Epoch 00075: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2930 - acc: 0.8632 - val_loss: 0.3312 - val_acc: 0.8433\n",
      "Epoch 77/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2566 - acc: 0.8821Epoch 00076: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2639 - acc: 0.8775 - val_loss: 0.3927 - val_acc: 0.8326\n",
      "Epoch 78/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2933 - acc: 0.8538Epoch 00077: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2953 - acc: 0.8548 - val_loss: 0.3350 - val_acc: 0.8498\n",
      "Epoch 79/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2696 - acc: 0.8761Epoch 00078: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2704 - acc: 0.8768 - val_loss: 0.3524 - val_acc: 0.8433\n",
      "Epoch 80/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.3019 - acc: 0.8597Epoch 00079: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.3015 - acc: 0.8626 - val_loss: 0.3423 - val_acc: 0.8412\n",
      "Epoch 81/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2600 - acc: 0.8833Epoch 00080: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2697 - acc: 0.8765 - val_loss: 0.4212 - val_acc: 0.8219\n",
      "Epoch 82/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2872 - acc: 0.8680Epoch 00081: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2897 - acc: 0.8669 - val_loss: 0.3625 - val_acc: 0.8433\n",
      "Epoch 83/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2767 - acc: 0.8723Epoch 00082: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2778 - acc: 0.8709 - val_loss: 0.3675 - val_acc: 0.8391\n",
      "Epoch 84/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2790 - acc: 0.8728Epoch 00083: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2801 - acc: 0.8716 - val_loss: 0.3483 - val_acc: 0.8476\n",
      "Epoch 85/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2868 - acc: 0.8606Epoch 00084: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2838 - acc: 0.8641 - val_loss: 0.3298 - val_acc: 0.8562\n",
      "Epoch 86/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2845 - acc: 0.8723Epoch 00085: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2898 - acc: 0.8710 - val_loss: 0.4209 - val_acc: 0.8133\n",
      "Epoch 87/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2760 - acc: 0.8712Epoch 00086: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2792 - acc: 0.8685 - val_loss: 0.3826 - val_acc: 0.8455\n",
      "Epoch 88/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2789 - acc: 0.8654Epoch 00087: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2787 - acc: 0.8647 - val_loss: 0.3570 - val_acc: 0.8391\n",
      "Epoch 89/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2592 - acc: 0.8760Epoch 00088: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2586 - acc: 0.8766 - val_loss: 0.3667 - val_acc: 0.8541\n",
      "Epoch 90/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2690 - acc: 0.8784Epoch 00089: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/21 [==============================] - 2s - loss: 0.2712 - acc: 0.8761 - val_loss: 0.3695 - val_acc: 0.8541\n",
      "Epoch 91/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2495 - acc: 0.8865Epoch 00090: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2496 - acc: 0.8853 - val_loss: 0.3647 - val_acc: 0.8498\n",
      "Epoch 92/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2684 - acc: 0.8745Epoch 00091: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2697 - acc: 0.8723 - val_loss: 0.4425 - val_acc: 0.8348\n",
      "Epoch 93/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2791 - acc: 0.8691Epoch 00092: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2823 - acc: 0.8666 - val_loss: 0.3508 - val_acc: 0.8391\n",
      "Epoch 94/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.2692 - acc: 0.8757Epoch 00093: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2693 - acc: 0.8735 - val_loss: 0.3810 - val_acc: 0.8369\n",
      "Epoch 95/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2651 - acc: 0.8737Epoch 00094: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2665 - acc: 0.8710 - val_loss: 0.3373 - val_acc: 0.8562\n",
      "Epoch 96/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2509 - acc: 0.8919Epoch 00095: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2512 - acc: 0.8904 - val_loss: 0.3471 - val_acc: 0.8584\n",
      "Epoch 97/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2621 - acc: 0.8708Epoch 00096: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2595 - acc: 0.8717 - val_loss: 0.3690 - val_acc: 0.8584\n",
      "Epoch 98/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2655 - acc: 0.8796Epoch 00097: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2727 - acc: 0.8795 - val_loss: 0.3884 - val_acc: 0.8369\n",
      "Epoch 99/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2755 - acc: 0.8693Epoch 00098: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2737 - acc: 0.8734 - val_loss: 0.3720 - val_acc: 0.8369\n",
      "Epoch 100/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2671 - acc: 0.8750Epoch 00099: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2652 - acc: 0.8758 - val_loss: 0.3672 - val_acc: 0.8369\n",
      "Epoch 101/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2648 - acc: 0.8742Epoch 00100: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2642 - acc: 0.8757 - val_loss: 0.3319 - val_acc: 0.8433\n",
      "Epoch 102/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2641 - acc: 0.8805Epoch 00101: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2685 - acc: 0.8794 - val_loss: 0.3927 - val_acc: 0.8326\n",
      "Epoch 103/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2533 - acc: 0.8784Epoch 00102: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2441 - acc: 0.8859 - val_loss: 0.3921 - val_acc: 0.8455\n",
      "Epoch 104/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2561 - acc: 0.8808Epoch 00103: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2526 - acc: 0.8842 - val_loss: 0.3589 - val_acc: 0.8562\n",
      "Epoch 105/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2560 - acc: 0.8796Epoch 00104: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2583 - acc: 0.8798 - val_loss: 0.3453 - val_acc: 0.8541\n",
      "Epoch 106/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2747 - acc: 0.8700Epoch 00105: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2723 - acc: 0.8695 - val_loss: 0.3646 - val_acc: 0.8369\n",
      "Epoch 107/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2517 - acc: 0.8794Epoch 00106: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2543 - acc: 0.8806 - val_loss: 0.3976 - val_acc: 0.8455\n",
      "Epoch 108/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2560 - acc: 0.8821Epoch 00107: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2518 - acc: 0.8852 - val_loss: 0.3669 - val_acc: 0.8476\n",
      "Epoch 109/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2565 - acc: 0.8789Epoch 00108: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2607 - acc: 0.8773 - val_loss: 0.3791 - val_acc: 0.8476\n",
      "Epoch 110/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2535 - acc: 0.8825Epoch 00109: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2503 - acc: 0.8863 - val_loss: 0.3557 - val_acc: 0.8283\n",
      "Epoch 111/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2993 - acc: 0.8671Epoch 00110: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2997 - acc: 0.8661 - val_loss: 0.3601 - val_acc: 0.8541\n",
      "Epoch 112/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2643 - acc: 0.8810Epoch 00111: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2664 - acc: 0.8806 - val_loss: 0.3669 - val_acc: 0.8562\n",
      "Epoch 113/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2544 - acc: 0.8841Epoch 00112: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2576 - acc: 0.8816 - val_loss: 0.3751 - val_acc: 0.8476\n",
      "Epoch 114/200\n",
      "14/21 [==================>...........] - ETA: 1s - loss: 0.2408 - acc: 0.8869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.282001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2465 - acc: 0.8805Epoch 00113: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2434 - acc: 0.8816 - val_loss: 0.4426 - val_acc: 0.8369\n",
      "Epoch 115/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2405 - acc: 0.8782Epoch 00114: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2396 - acc: 0.8802 - val_loss: 0.3656 - val_acc: 0.8562\n",
      "Epoch 116/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.8908Epoch 00115: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2315 - acc: 0.8936 - val_loss: 0.3939 - val_acc: 0.8433\n",
      "Epoch 117/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2422 - acc: 0.8846Epoch 00116: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2505 - acc: 0.8820 - val_loss: 0.3805 - val_acc: 0.8476\n",
      "Epoch 118/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2475 - acc: 0.8811Epoch 00117: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2440 - acc: 0.8839 - val_loss: 0.3588 - val_acc: 0.8541\n",
      "Epoch 119/200\n",
      "11/21 [==============>...............] - ETA: 0s - loss: 0.2486 - acc: 0.8807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.594501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.298250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2485 - acc: 0.8830Epoch 00118: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2453 - acc: 0.8861 - val_loss: 0.3867 - val_acc: 0.8348\n",
      "Epoch 120/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2693 - acc: 0.8822Epoch 00119: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2756 - acc: 0.8760 - val_loss: 0.4321 - val_acc: 0.8412\n",
      "Epoch 121/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2614 - acc: 0.8803Epoch 00120: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2601 - acc: 0.8800 - val_loss: 0.3805 - val_acc: 0.8455\n",
      "Epoch 122/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2620 - acc: 0.8781Epoch 00121: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2575 - acc: 0.8816 - val_loss: 0.3531 - val_acc: 0.8519\n",
      "Epoch 123/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2705 - acc: 0.8866Epoch 00122: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.2730 - acc: 0.8869 - val_loss: 0.3593 - val_acc: 0.8433\n",
      "Epoch 124/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2403 - acc: 0.8950Epoch 00123: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2397 - acc: 0.8955 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 125/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2601 - acc: 0.8811Epoch 00124: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2604 - acc: 0.8815 - val_loss: 0.3486 - val_acc: 0.8369\n",
      "Epoch 126/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2612 - acc: 0.8734Epoch 00125: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2636 - acc: 0.8721 - val_loss: 0.4157 - val_acc: 0.8305\n",
      "Epoch 127/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2472 - acc: 0.8815Epoch 00126: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2514 - acc: 0.8805 - val_loss: 0.3319 - val_acc: 0.8627\n",
      "Epoch 128/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2492 - acc: 0.8802Epoch 00127: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2553 - acc: 0.8783 - val_loss: 0.3642 - val_acc: 0.8476\n",
      "Epoch 129/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2415 - acc: 0.8940Epoch 00128: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2388 - acc: 0.8953 - val_loss: 0.3472 - val_acc: 0.8519\n",
      "Epoch 130/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2239 - acc: 0.8931Epoch 00129: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2269 - acc: 0.8908 - val_loss: 0.3737 - val_acc: 0.8519\n",
      "Epoch 131/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2592 - acc: 0.8797Epoch 00130: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2551 - acc: 0.8816 - val_loss: 0.3432 - val_acc: 0.8691\n",
      "Epoch 132/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2616 - acc: 0.8757Epoch 00131: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2638 - acc: 0.8744 - val_loss: 0.3832 - val_acc: 0.8433\n",
      "Epoch 133/200\n",
      "11/21 [==============>...............] - ETA: 0s - loss: 0.2511 - acc: 0.8812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.683000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.342751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2456 - acc: 0.8815Epoch 00132: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2517 - acc: 0.8770 - val_loss: 0.3627 - val_acc: 0.8541\n",
      "Epoch 134/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2340 - acc: 0.8935Epoch 00133: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2402 - acc: 0.8906 - val_loss: 0.4220 - val_acc: 0.8412\n",
      "Epoch 135/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2244 - acc: 0.8991Epoch 00134: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2215 - acc: 0.8997 - val_loss: 0.3813 - val_acc: 0.8498\n",
      "Epoch 136/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2350 - acc: 0.8898Epoch 00135: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2340 - acc: 0.8913 - val_loss: 0.3800 - val_acc: 0.8627\n",
      "Epoch 137/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.2527 - acc: 0.8853Epoch 00136: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2519 - acc: 0.8839 - val_loss: 0.3856 - val_acc: 0.8455\n",
      "Epoch 138/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2183 - acc: 0.8997Epoch 00137: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2174 - acc: 0.8993 - val_loss: 0.4345 - val_acc: 0.8412\n",
      "Epoch 139/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2360 - acc: 0.8963Epoch 00138: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2317 - acc: 0.8982 - val_loss: 0.3802 - val_acc: 0.8541\n",
      "Epoch 140/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2222 - acc: 0.8920Epoch 00139: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2245 - acc: 0.8912 - val_loss: 0.3586 - val_acc: 0.8712\n",
      "Epoch 141/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2433 - acc: 0.8839Epoch 00140: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2443 - acc: 0.8836 - val_loss: 0.4095 - val_acc: 0.8348\n",
      "Epoch 142/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2369 - acc: 0.8907Epoch 00141: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2358 - acc: 0.8922 - val_loss: 0.4193 - val_acc: 0.8369\n",
      "Epoch 143/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2358 - acc: 0.8868Epoch 00142: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2357 - acc: 0.8884 - val_loss: 0.4168 - val_acc: 0.8369\n",
      "Epoch 144/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2346 - acc: 0.8898Epoch 00143: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2366 - acc: 0.8905 - val_loss: 0.4396 - val_acc: 0.8412\n",
      "Epoch 145/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2343 - acc: 0.8869Epoch 00144: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2365 - acc: 0.8836 - val_loss: 0.4289 - val_acc: 0.8412\n",
      "Epoch 146/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2390 - acc: 0.8877Epoch 00145: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2349 - acc: 0.8908 - val_loss: 0.4342 - val_acc: 0.8412\n",
      "Epoch 147/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2235 - acc: 0.9019Epoch 00146: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2228 - acc: 0.9028 - val_loss: 0.3959 - val_acc: 0.8391\n",
      "Epoch 148/200\n",
      "11/21 [==============>...............] - ETA: 1s - loss: 0.2429 - acc: 0.8892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.716008). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.731709). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.359004). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2288 - acc: 0.8969Epoch 00147: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2316 - acc: 0.8982 - val_loss: 0.3997 - val_acc: 0.8605\n",
      "Epoch 149/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2160 - acc: 0.9072Epoch 00148: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2151 - acc: 0.9086 - val_loss: 0.3696 - val_acc: 0.8712\n",
      "Epoch 150/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2297 - acc: 0.8889Epoch 00149: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2321 - acc: 0.8862 - val_loss: 0.3617 - val_acc: 0.8498\n",
      "Epoch 151/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2195 - acc: 0.8992Epoch 00150: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2213 - acc: 0.8974 - val_loss: 0.4557 - val_acc: 0.8391\n",
      "Epoch 152/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2193 - acc: 0.8926Epoch 00151: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2235 - acc: 0.8918 - val_loss: 0.3685 - val_acc: 0.8562\n",
      "Epoch 153/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2205 - acc: 0.8966Epoch 00152: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2259 - acc: 0.8943 - val_loss: 0.3755 - val_acc: 0.8648\n",
      "Epoch 154/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2234 - acc: 0.8998Epoch 00153: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2211 - acc: 0.9022 - val_loss: 0.4049 - val_acc: 0.8605\n",
      "Epoch 155/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2518 - acc: 0.8788Epoch 00154: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2481 - acc: 0.8801 - val_loss: 0.3817 - val_acc: 0.8412\n",
      "Epoch 156/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2264 - acc: 0.9053Epoch 00155: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2279 - acc: 0.9039 - val_loss: 0.4373 - val_acc: 0.8455\n",
      "Epoch 157/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2158 - acc: 0.9027Epoch 00156: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2160 - acc: 0.9014 - val_loss: 0.4030 - val_acc: 0.8433\n",
      "Epoch 158/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2179 - acc: 0.9016Epoch 00157: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.2201 - acc: 0.8996 - val_loss: 0.5270 - val_acc: 0.8197\n",
      "Epoch 159/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2292 - acc: 0.9048Epoch 00158: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2302 - acc: 0.9027 - val_loss: 0.4383 - val_acc: 0.8584\n",
      "Epoch 160/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2154 - acc: 0.8970Epoch 00159: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2136 - acc: 0.8967 - val_loss: 0.4404 - val_acc: 0.8476\n",
      "Epoch 161/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2191 - acc: 0.8982Epoch 00160: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2196 - acc: 0.8972 - val_loss: 0.4319 - val_acc: 0.8455\n",
      "Epoch 162/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2125 - acc: 0.9037Epoch 00161: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2127 - acc: 0.9031 - val_loss: 0.4794 - val_acc: 0.8476\n",
      "Epoch 163/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2328 - acc: 0.8971Epoch 00162: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2335 - acc: 0.8974 - val_loss: 0.3780 - val_acc: 0.8541\n",
      "Epoch 164/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2415 - acc: 0.8956Epoch 00163: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2421 - acc: 0.8940 - val_loss: 0.4077 - val_acc: 0.8412\n",
      "Epoch 165/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2171 - acc: 0.9095Epoch 00164: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2108 - acc: 0.9122 - val_loss: 0.4120 - val_acc: 0.8498\n",
      "Epoch 166/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2462 - acc: 0.8935Epoch 00165: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2444 - acc: 0.8948 - val_loss: 0.4607 - val_acc: 0.8219\n",
      "Epoch 167/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2277 - acc: 0.8960Epoch 00166: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.2310 - acc: 0.8971 - val_loss: 0.3777 - val_acc: 0.8605\n",
      "Epoch 168/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2121 - acc: 0.9042Epoch 00167: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2125 - acc: 0.9043 - val_loss: 0.4015 - val_acc: 0.8584\n",
      "Epoch 169/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2103 - acc: 0.9019- ETA: 1s - loss: 0.2Epoch 00168: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2070 - acc: 0.9028 - val_loss: 0.3948 - val_acc: 0.8541\n",
      "Epoch 170/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2286 - acc: 0.8915Epoch 00169: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2282 - acc: 0.8929 - val_loss: 0.3899 - val_acc: 0.8433\n",
      "Epoch 171/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2210 - acc: 0.8993Epoch 00170: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2189 - acc: 0.8989 - val_loss: 0.4239 - val_acc: 0.8476\n",
      "Epoch 172/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2344 - acc: 0.9050Epoch 00171: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2326 - acc: 0.9044 - val_loss: 0.3703 - val_acc: 0.8541\n",
      "Epoch 173/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2327 - acc: 0.9006Epoch 00172: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2307 - acc: 0.9022 - val_loss: 0.3907 - val_acc: 0.8455\n",
      "Epoch 174/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2032 - acc: 0.9139Epoch 00173: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2063 - acc: 0.9114 - val_loss: 0.3971 - val_acc: 0.8519\n",
      "Epoch 175/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2181 - acc: 0.8967Epoch 00174: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2153 - acc: 0.8994 - val_loss: 0.3810 - val_acc: 0.8498\n",
      "Epoch 176/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2334 - acc: 0.8966Epoch 00175: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2320 - acc: 0.8963 - val_loss: 0.4471 - val_acc: 0.8519\n",
      "Epoch 177/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1990 - acc: 0.9003Epoch 00176: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2015 - acc: 0.8994 - val_loss: 0.3961 - val_acc: 0.8541\n",
      "Epoch 178/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2173 - acc: 0.9038Epoch 00177: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2125 - acc: 0.9097 - val_loss: 0.4792 - val_acc: 0.8369\n",
      "Epoch 179/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1983 - acc: 0.9024Epoch 00178: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2030 - acc: 0.9005 - val_loss: 0.4276 - val_acc: 0.8519\n",
      "Epoch 180/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2227 - acc: 0.8942Epoch 00179: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2192 - acc: 0.8977 - val_loss: 0.4209 - val_acc: 0.8562\n",
      "Epoch 181/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.2076 - acc: 0.9094Epoch 00180: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2071 - acc: 0.9083 - val_loss: 0.4303 - val_acc: 0.8326\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2521 - acc: 0.8843Epoch 00181: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2479 - acc: 0.8846 - val_loss: 0.3927 - val_acc: 0.8455\n",
      "Epoch 183/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2087 - acc: 0.9090Epoch 00182: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2089 - acc: 0.9074 - val_loss: 0.3937 - val_acc: 0.8712\n",
      "Epoch 184/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2162 - acc: 0.9012Epoch 00183: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2150 - acc: 0.9006 - val_loss: 0.4005 - val_acc: 0.8691\n",
      "Epoch 185/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1905 - acc: 0.9151Epoch 00184: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1905 - acc: 0.9146 - val_loss: 0.4139 - val_acc: 0.8519\n",
      "Epoch 186/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2156 - acc: 0.8984Epoch 00185: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2123 - acc: 0.9002 - val_loss: 0.3860 - val_acc: 0.8648\n",
      "Epoch 187/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2113 - acc: 0.9033Epoch 00186: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2087 - acc: 0.9034 - val_loss: 0.3740 - val_acc: 0.8498\n",
      "Epoch 188/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2021 - acc: 0.9105Epoch 00187: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2065 - acc: 0.9103 - val_loss: 0.4076 - val_acc: 0.8584\n",
      "Epoch 189/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2155 - acc: 0.9047Epoch 00188: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2135 - acc: 0.9054 - val_loss: 0.4150 - val_acc: 0.8605\n",
      "Epoch 190/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2052 - acc: 0.9127Epoch 00189: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2057 - acc: 0.9124 - val_loss: 0.3902 - val_acc: 0.8670\n",
      "Epoch 191/200\n",
      " 9/21 [===========>..................] - ETA: 2s - loss: 0.1955 - acc: 0.9129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.231001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.460503). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.236752). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1888 - acc: 0.9146Epoch 00190: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1872 - acc: 0.9149 - val_loss: 0.3971 - val_acc: 0.8648\n",
      "Epoch 192/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2133 - acc: 0.9065Epoch 00191: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2159 - acc: 0.9050 - val_loss: 0.4212 - val_acc: 0.8541\n",
      "Epoch 193/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1947 - acc: 0.9152Epoch 00192: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1997 - acc: 0.9114 - val_loss: 0.4191 - val_acc: 0.8584\n",
      "Epoch 194/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1846 - acc: 0.9161Epoch 00193: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1816 - acc: 0.9184 - val_loss: 0.4783 - val_acc: 0.8562\n",
      "Epoch 195/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2111 - acc: 0.9040Epoch 00194: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.2119 - acc: 0.9058 - val_loss: 0.4312 - val_acc: 0.8476\n",
      "Epoch 196/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2271 - acc: 0.8861Epoch 00195: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2234 - acc: 0.8891 - val_loss: 0.3766 - val_acc: 0.8455\n",
      "Epoch 197/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1990 - acc: 0.9153Epoch 00196: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2010 - acc: 0.9148 - val_loss: 0.4119 - val_acc: 0.8541\n",
      "Epoch 198/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1977 - acc: 0.9084Epoch 00197: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1999 - acc: 0.9075 - val_loss: 0.4697 - val_acc: 0.8476\n",
      "Epoch 199/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2075 - acc: 0.9141Epoch 00198: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2077 - acc: 0.9158 - val_loss: 0.3903 - val_acc: 0.8369\n",
      "Epoch 200/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1961 - acc: 0.9165Epoch 00199: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1977 - acc: 0.9133 - val_loss: 0.4184 - val_acc: 0.8648\n",
      "32/74 [===========>..................] - ETA: 5sTest loss: 0.344006631825\n",
      "Test accuracy: 0.783783787006\n",
      "Fold: 1\n",
      "Epoch 1/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.3295 - acc: 0.8717Epoch 00000: val_loss improved from 0.32819 to 0.23615, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 2s - loss: 0.3301 - acc: 0.8720 - val_loss: 0.2361 - val_acc: 0.8948\n",
      "Epoch 2/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2931 - acc: 0.8831Epoch 00001: val_loss improved from 0.23615 to 0.20815, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.2904 - acc: 0.8827 - val_loss: 0.2082 - val_acc: 0.9185\n",
      "Epoch 3/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2589 - acc: 0.8873Epoch 00002: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2594 - acc: 0.8875 - val_loss: 0.2183 - val_acc: 0.9013\n",
      "Epoch 4/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2854 - acc: 0.8686Epoch 00003: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2849 - acc: 0.8714 - val_loss: 0.2293 - val_acc: 0.8927\n",
      "Epoch 5/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2751 - acc: 0.8690Epoch 00004: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2781 - acc: 0.8686 - val_loss: 0.2392 - val_acc: 0.8906\n",
      "Epoch 6/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2356 - acc: 0.9063Epoch 00005: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2355 - acc: 0.9056 - val_loss: 0.2182 - val_acc: 0.9163\n",
      "Epoch 7/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2690 - acc: 0.8887Epoch 00006: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2660 - acc: 0.8895 - val_loss: 0.2415 - val_acc: 0.8927\n",
      "Epoch 8/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2452 - acc: 0.8996Epoch 00007: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2417 - acc: 0.9007 - val_loss: 0.2344 - val_acc: 0.9099\n",
      "Epoch 9/200\n",
      " 4/21 [====>.........................] - ETA: 6s - loss: 0.2468 - acc: 0.9018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.126250). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.251000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.214000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/21 [===================>..........] - ETA: 0s - loss: 0.2410 - acc: 0.8957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.177000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2530 - acc: 0.8904Epoch 00008: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2508 - acc: 0.8912 - val_loss: 0.2463 - val_acc: 0.9013\n",
      "Epoch 10/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2442 - acc: 0.9001Epoch 00009: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2447 - acc: 0.8989 - val_loss: 0.2570 - val_acc: 0.9034\n",
      "Epoch 11/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2426 - acc: 0.8941Epoch 00010: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2459 - acc: 0.8898 - val_loss: 0.2314 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2600 - acc: 0.8882Epoch 00011: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2625 - acc: 0.8876 - val_loss: 0.2338 - val_acc: 0.9163\n",
      "Epoch 13/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2331 - acc: 0.9048Epoch 00012: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2316 - acc: 0.9055 - val_loss: 0.2388 - val_acc: 0.8906\n",
      "Epoch 14/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2420 - acc: 0.8896Epoch 00013: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2371 - acc: 0.8925 - val_loss: 0.2109 - val_acc: 0.9163\n",
      "Epoch 15/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2427 - acc: 0.9027Epoch 00014: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2405 - acc: 0.9033 - val_loss: 0.2400 - val_acc: 0.8948\n",
      "Epoch 16/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2361 - acc: 0.9006Epoch 00015: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2354 - acc: 0.9001 - val_loss: 0.2259 - val_acc: 0.9120\n",
      "Epoch 17/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2283 - acc: 0.8976Epoch 00016: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2233 - acc: 0.8994 - val_loss: 0.2365 - val_acc: 0.9013\n",
      "Epoch 18/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2306 - acc: 0.9040Epoch 00017: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2351 - acc: 0.9006 - val_loss: 0.2223 - val_acc: 0.9099\n",
      "Epoch 19/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2198 - acc: 0.9011Epoch 00018: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2179 - acc: 0.9028 - val_loss: 0.2411 - val_acc: 0.9013\n",
      "Epoch 20/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2329 - acc: 0.8980Epoch 00019: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2319 - acc: 0.9004 - val_loss: 0.2397 - val_acc: 0.9077\n",
      "Epoch 21/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2219 - acc: 0.9048Epoch 00020: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2219 - acc: 0.9063 - val_loss: 0.2431 - val_acc: 0.8970\n",
      "Epoch 22/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2231 - acc: 0.9135Epoch 00021: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2261 - acc: 0.9117 - val_loss: 0.2440 - val_acc: 0.8906\n",
      "Epoch 23/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2270 - acc: 0.8955Epoch 00022: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2281 - acc: 0.8967 - val_loss: 0.2404 - val_acc: 0.8970\n",
      "Epoch 24/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2257 - acc: 0.9027Epoch 00023: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2220 - acc: 0.9043 - val_loss: 0.2446 - val_acc: 0.9185\n",
      "Epoch 25/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2465 - acc: 0.8852Epoch 00024: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2509 - acc: 0.8854 - val_loss: 0.2402 - val_acc: 0.9056\n",
      "Epoch 26/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2367 - acc: 0.9009Epoch 00025: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2335 - acc: 0.9011 - val_loss: 0.2416 - val_acc: 0.9120\n",
      "Epoch 27/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2256 - acc: 0.9059Epoch 00026: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2274 - acc: 0.9045 - val_loss: 0.2391 - val_acc: 0.9013\n",
      "Epoch 28/200\n",
      "15/21 [===================>..........] - ETA: 0s - loss: 0.2456 - acc: 0.8934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.192501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2433 - acc: 0.8904Epoch 00027: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2391 - acc: 0.8919 - val_loss: 0.2806 - val_acc: 0.8734\n",
      "Epoch 29/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2266 - acc: 0.8993Epoch 00028: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2267 - acc: 0.8982 - val_loss: 0.2482 - val_acc: 0.9077\n",
      "Epoch 30/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2334 - acc: 0.9044Epoch 00029: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2344 - acc: 0.9036 - val_loss: 0.2566 - val_acc: 0.9034\n",
      "Epoch 31/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9007Epoch 00030: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2225 - acc: 0.8981 - val_loss: 0.2449 - val_acc: 0.9013\n",
      "Epoch 32/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2174 - acc: 0.9054Epoch 00031: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2166 - acc: 0.9055 - val_loss: 0.2583 - val_acc: 0.9034\n",
      "Epoch 33/200\n",
      "11/21 [==============>...............] - ETA: 0s - loss: 0.2095 - acc: 0.9008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.293000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.147500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2148 - acc: 0.9024Epoch 00032: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2116 - acc: 0.9040 - val_loss: 0.2432 - val_acc: 0.9013\n",
      "Epoch 34/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1942 - acc: 0.9242Epoch 00033: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1937 - acc: 0.9255 - val_loss: 0.2588 - val_acc: 0.9034\n",
      "Epoch 35/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2171 - acc: 0.9090Epoch 00034: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2161 - acc: 0.9104 - val_loss: 0.2661 - val_acc: 0.8906\n",
      "Epoch 36/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2099 - acc: 0.9090Epoch 00035: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2096 - acc: 0.9103 - val_loss: 0.2478 - val_acc: 0.9185\n",
      "Epoch 37/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2042 - acc: 0.9075Epoch 00036: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2031 - acc: 0.9088 - val_loss: 0.2686 - val_acc: 0.8841\n",
      "Epoch 38/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2450 - acc: 0.8875Epoch 00037: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2437 - acc: 0.8884 - val_loss: 0.2877 - val_acc: 0.8991\n",
      "Epoch 39/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2178 - acc: 0.9006Epoch 00038: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2142 - acc: 0.9028 - val_loss: 0.2479 - val_acc: 0.9077\n",
      "Epoch 40/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2155 - acc: 0.9087Epoch 00039: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2135 - acc: 0.9107 - val_loss: 0.2346 - val_acc: 0.9077\n",
      "Epoch 41/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2274 - acc: 0.9019Epoch 00040: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2265 - acc: 0.9028 - val_loss: 0.2428 - val_acc: 0.9034\n",
      "Epoch 42/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2030 - acc: 0.9093Epoch 00041: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1985 - acc: 0.9126 - val_loss: 0.2872 - val_acc: 0.8906\n",
      "Epoch 43/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2054 - acc: 0.9018Epoch 00042: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2062 - acc: 0.9018 - val_loss: 0.2688 - val_acc: 0.9099\n",
      "Epoch 44/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2381 - acc: 0.8965Epoch 00043: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2385 - acc: 0.8977 - val_loss: 0.2707 - val_acc: 0.8820\n",
      "Epoch 45/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2656 - acc: 0.8817Epoch 00044: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2595 - acc: 0.8832 - val_loss: 0.2587 - val_acc: 0.8863\n",
      "Epoch 46/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2072 - acc: 0.9177Epoch 00045: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2044 - acc: 0.9185 - val_loss: 0.2724 - val_acc: 0.8948\n",
      "Epoch 47/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2356 - acc: 0.8981Epoch 00046: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2403 - acc: 0.8985 - val_loss: 0.2607 - val_acc: 0.8948\n",
      "Epoch 48/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2153 - acc: 0.9078Epoch 00047: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2148 - acc: 0.9084 - val_loss: 0.2763 - val_acc: 0.9077\n",
      "Epoch 49/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2197 - acc: 0.9110Epoch 00048: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2187 - acc: 0.9121 - val_loss: 0.2545 - val_acc: 0.9013\n",
      "Epoch 50/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1929 - acc: 0.9225Epoch 00049: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1939 - acc: 0.9218 - val_loss: 0.2694 - val_acc: 0.9056\n",
      "Epoch 51/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1891 - acc: 0.9259Epoch 00050: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1860 - acc: 0.9271 - val_loss: 0.2594 - val_acc: 0.9034\n",
      "Epoch 52/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1898 - acc: 0.9171Epoch 00051: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1917 - acc: 0.9160 - val_loss: 0.2460 - val_acc: 0.9077\n",
      "Epoch 53/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2077 - acc: 0.9117Epoch 00052: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2045 - acc: 0.9121 - val_loss: 0.2468 - val_acc: 0.8991\n",
      "Epoch 54/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1871 - acc: 0.9187Epoch 00053: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1886 - acc: 0.9181 - val_loss: 0.2658 - val_acc: 0.8948\n",
      "Epoch 55/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2042 - acc: 0.9147Epoch 00054: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2052 - acc: 0.9143 - val_loss: 0.2570 - val_acc: 0.8991\n",
      "Epoch 56/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2009 - acc: 0.8986Epoch 00055: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1974 - acc: 0.9007 - val_loss: 0.2707 - val_acc: 0.9077\n",
      "Epoch 57/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2152 - acc: 0.9017Epoch 00056: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2127 - acc: 0.9019 - val_loss: 0.2614 - val_acc: 0.9013\n",
      "Epoch 58/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2119 - acc: 0.9125Epoch 00057: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2040 - acc: 0.9151 - val_loss: 0.2682 - val_acc: 0.8991\n",
      "Epoch 59/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1863 - acc: 0.9250Epoch 00058: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1895 - acc: 0.9227 - val_loss: 0.2667 - val_acc: 0.9034\n",
      "Epoch 60/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2161 - acc: 0.8956Epoch 00059: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2133 - acc: 0.8964 - val_loss: 0.2546 - val_acc: 0.8991\n",
      "Epoch 61/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1742 - acc: 0.9251Epoch 00060: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1742 - acc: 0.9242 - val_loss: 0.2448 - val_acc: 0.9120\n",
      "Epoch 62/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1952 - acc: 0.9163Epoch 00061: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1988 - acc: 0.9137 - val_loss: 0.2712 - val_acc: 0.9077\n",
      "Epoch 63/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1977 - acc: 0.9155Epoch 00062: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1947 - acc: 0.9158 - val_loss: 0.2679 - val_acc: 0.9056\n",
      "Epoch 64/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1989 - acc: 0.9119Epoch 00063: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2013 - acc: 0.9118 - val_loss: 0.2806 - val_acc: 0.8906\n",
      "Epoch 65/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2097 - acc: 0.9089Epoch 00064: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2052 - acc: 0.9110 - val_loss: 0.2578 - val_acc: 0.9013\n",
      "Epoch 66/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1947 - acc: 0.9138Epoch 00065: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1992 - acc: 0.9113 - val_loss: 0.2597 - val_acc: 0.9163\n",
      "Epoch 67/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1781 - acc: 0.9226Epoch 00066: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/21 [==============================] - 1s - loss: 0.1770 - acc: 0.9233 - val_loss: 0.2916 - val_acc: 0.8948\n",
      "Epoch 68/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2044 - acc: 0.9090Epoch 00067: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2058 - acc: 0.9068 - val_loss: 0.2838 - val_acc: 0.8906\n",
      "Epoch 69/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1953 - acc: 0.9105Epoch 00068: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1922 - acc: 0.9111 - val_loss: 0.2700 - val_acc: 0.8948\n",
      "Epoch 70/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1931 - acc: 0.9194Epoch 00069: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1940 - acc: 0.9174 - val_loss: 0.2523 - val_acc: 0.9099\n",
      "Epoch 71/200\n",
      "11/21 [==============>...............] - ETA: 0s - loss: 0.1796 - acc: 0.9195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.494501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.248250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1947 - acc: 0.9107Epoch 00070: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1933 - acc: 0.9112 - val_loss: 0.2832 - val_acc: 0.8734\n",
      "Epoch 72/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1746 - acc: 0.9288Epoch 00071: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1817 - acc: 0.9264 - val_loss: 0.2647 - val_acc: 0.9142\n",
      "Epoch 73/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1883 - acc: 0.9205Epoch 00072: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1804 - acc: 0.9243 - val_loss: 0.2720 - val_acc: 0.9013\n",
      "Epoch 74/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1992 - acc: 0.9094Epoch 00073: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1978 - acc: 0.9106 - val_loss: 0.2680 - val_acc: 0.8970\n",
      "Epoch 75/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1929 - acc: 0.9131Epoch 00074: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1883 - acc: 0.9158 - val_loss: 0.2690 - val_acc: 0.9056\n",
      "Epoch 76/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1826 - acc: 0.9230Epoch 00075: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1827 - acc: 0.9237 - val_loss: 0.2785 - val_acc: 0.9034\n",
      "Epoch 77/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1751 - acc: 0.9291Epoch 00076: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.1752 - acc: 0.9281 - val_loss: 0.2920 - val_acc: 0.8906\n",
      "Epoch 78/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1922 - acc: 0.9180Epoch 00077: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1940 - acc: 0.9168 - val_loss: 0.2929 - val_acc: 0.9077\n",
      "Epoch 79/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1772 - acc: 0.9272Epoch 00078: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.1743 - acc: 0.9291 - val_loss: 0.2772 - val_acc: 0.9142\n",
      "Epoch 80/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1874 - acc: 0.9225Epoch 00079: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1922 - acc: 0.9196 - val_loss: 0.2784 - val_acc: 0.9099\n",
      "Epoch 81/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1957 - acc: 0.9148Epoch 00080: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1958 - acc: 0.9130 - val_loss: 0.2848 - val_acc: 0.9099\n",
      "Epoch 82/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2022 - acc: 0.9126Epoch 00081: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2037 - acc: 0.9117 - val_loss: 0.2909 - val_acc: 0.8884\n",
      "Epoch 83/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2036 - acc: 0.9053Epoch 00082: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2001 - acc: 0.9075 - val_loss: 0.3274 - val_acc: 0.8863\n",
      "Epoch 84/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2558 - acc: 0.8887Epoch 00083: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2495 - acc: 0.8939 - val_loss: 0.2819 - val_acc: 0.9034\n",
      "Epoch 85/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2078 - acc: 0.9166Epoch 00084: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2105 - acc: 0.9154 - val_loss: 0.2947 - val_acc: 0.8927\n",
      "Epoch 86/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2020 - acc: 0.9176Epoch 00085: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2027 - acc: 0.9178 - val_loss: 0.2540 - val_acc: 0.9013\n",
      "Epoch 87/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1860 - acc: 0.9246Epoch 00086: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1878 - acc: 0.9238 - val_loss: 0.2867 - val_acc: 0.8991\n",
      "Epoch 88/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2019 - acc: 0.9133Epoch 00087: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1995 - acc: 0.9120 - val_loss: 0.2980 - val_acc: 0.8884\n",
      "Epoch 89/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1806 - acc: 0.9230Epoch 00088: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1790 - acc: 0.9244 - val_loss: 0.2738 - val_acc: 0.8927\n",
      "Epoch 90/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2031 - acc: 0.9183Epoch 00089: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1981 - acc: 0.9206 - val_loss: 0.2556 - val_acc: 0.9034\n",
      "Epoch 91/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1818 - acc: 0.9317Epoch 00090: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1795 - acc: 0.9326 - val_loss: 0.2528 - val_acc: 0.9056\n",
      "Epoch 92/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1862 - acc: 0.9190Epoch 00091: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1867 - acc: 0.9198 - val_loss: 0.2586 - val_acc: 0.9013\n",
      "Epoch 93/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1913 - acc: 0.9213Epoch 00092: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1943 - acc: 0.9206 - val_loss: 0.2799 - val_acc: 0.9034\n",
      "Epoch 94/200\n",
      "11/21 [==============>...............] - ETA: 1s - loss: 0.1778 - acc: 0.9323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.389750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.654500). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.328250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1896 - acc: 0.9229Epoch 00093: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1887 - acc: 0.9235 - val_loss: 0.2696 - val_acc: 0.8991\n",
      "Epoch 95/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1815 - acc: 0.9248Epoch 00094: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1800 - acc: 0.9262 - val_loss: 0.2618 - val_acc: 0.9056\n",
      "Epoch 96/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1561 - acc: 0.9348Epoch 00095: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1570 - acc: 0.9349 - val_loss: 0.2962 - val_acc: 0.9077\n",
      "Epoch 97/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1754 - acc: 0.9233Epoch 00096: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1722 - acc: 0.9253 - val_loss: 0.2821 - val_acc: 0.9142\n",
      "Epoch 98/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1617 - acc: 0.9363Epoch 00097: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.1628 - acc: 0.9349 - val_loss: 0.2690 - val_acc: 0.9077\n",
      "Epoch 99/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1714 - acc: 0.9224Epoch 00098: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1696 - acc: 0.9245 - val_loss: 0.2681 - val_acc: 0.9099\n",
      "Epoch 100/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1878 - acc: 0.9163Epoch 00099: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1887 - acc: 0.9151 - val_loss: 0.2682 - val_acc: 0.9013\n",
      "Epoch 101/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1750 - acc: 0.9260Epoch 00100: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1715 - acc: 0.9272 - val_loss: 0.2871 - val_acc: 0.9034\n",
      "Epoch 102/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1505 - acc: 0.9332Epoch 00101: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1497 - acc: 0.9340 - val_loss: 0.2890 - val_acc: 0.8991\n",
      "Epoch 103/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1534 - acc: 0.9306Epoch 00102: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1515 - acc: 0.9316 - val_loss: 0.3062 - val_acc: 0.9099\n",
      "Epoch 104/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1711 - acc: 0.9266Epoch 00103: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1698 - acc: 0.9285 - val_loss: 0.2876 - val_acc: 0.8906\n",
      "Epoch 105/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1700 - acc: 0.9303Epoch 00104: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1701 - acc: 0.9283 - val_loss: 0.2893 - val_acc: 0.9013\n",
      "Epoch 106/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1701 - acc: 0.9270Epoch 00105: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1716 - acc: 0.9275 - val_loss: 0.2670 - val_acc: 0.8991\n",
      "Epoch 107/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1914 - acc: 0.9143Epoch 00106: val_loss did not improve\n",
      "22/21 [==============================] - 3s - loss: 0.1899 - acc: 0.9161 - val_loss: 0.2730 - val_acc: 0.9142\n",
      "Epoch 108/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1917 - acc: 0.9247Epoch 00107: val_loss did not improve\n",
      "22/21 [==============================] - 0s - loss: 0.1883 - acc: 0.9253 - val_loss: 0.2619 - val_acc: 0.9034\n",
      "Epoch 109/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1777 - acc: 0.9242Epoch 00108: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1751 - acc: 0.9262 - val_loss: 0.2816 - val_acc: 0.8948\n",
      "Epoch 110/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1715 - acc: 0.9266Epoch 00109: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1726 - acc: 0.9243 - val_loss: 0.2900 - val_acc: 0.8948\n",
      "Epoch 111/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1544 - acc: 0.9386Epoch 00110: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1554 - acc: 0.9386 - val_loss: 0.2848 - val_acc: 0.8970\n",
      "Epoch 112/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1554 - acc: 0.9280Epoch 00111: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1530 - acc: 0.9292 - val_loss: 0.3040 - val_acc: 0.9056\n",
      "Epoch 113/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1654 - acc: 0.9273Epoch 00112: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1645 - acc: 0.9285 - val_loss: 0.2659 - val_acc: 0.8906\n",
      "Epoch 114/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1607 - acc: 0.9288Epoch 00113: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1629 - acc: 0.9258 - val_loss: 0.3145 - val_acc: 0.9077\n",
      "Epoch 115/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1790 - acc: 0.9187Epoch 00114: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1771 - acc: 0.9196 - val_loss: 0.2794 - val_acc: 0.8863\n",
      "Epoch 116/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1527 - acc: 0.9348Epoch 00115: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1534 - acc: 0.9334 - val_loss: 0.2856 - val_acc: 0.9013\n",
      "Epoch 117/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1569 - acc: 0.9384Epoch 00116: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1559 - acc: 0.9383 - val_loss: 0.2938 - val_acc: 0.9034\n",
      "Epoch 118/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1835 - acc: 0.9152Epoch 00117: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1799 - acc: 0.9159 - val_loss: 0.2936 - val_acc: 0.8884\n",
      "Epoch 119/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1841 - acc: 0.9179Epoch 00118: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1868 - acc: 0.9146 - val_loss: 0.2850 - val_acc: 0.9142\n",
      "Epoch 120/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1662 - acc: 0.9288Epoch 00119: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1664 - acc: 0.9307 - val_loss: 0.3106 - val_acc: 0.8884\n",
      "Epoch 121/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1460 - acc: 0.9400Epoch 00120: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1483 - acc: 0.9392 - val_loss: 0.2998 - val_acc: 0.8991\n",
      "Epoch 122/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1475 - acc: 0.9405Epoch 00121: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1456 - acc: 0.9396 - val_loss: 0.3272 - val_acc: 0.8841\n",
      "Epoch 123/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1691 - acc: 0.9280Epoch 00122: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1706 - acc: 0.9278 - val_loss: 0.3429 - val_acc: 0.8906\n",
      "Epoch 124/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1855 - acc: 0.9173Epoch 00123: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1792 - acc: 0.9212 - val_loss: 0.2715 - val_acc: 0.8991\n",
      "Epoch 125/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1742 - acc: 0.9235Epoch 00124: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1750 - acc: 0.9221 - val_loss: 0.2849 - val_acc: 0.8970\n",
      "Epoch 126/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1669 - acc: 0.9323Epoch 00125: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1669 - acc: 0.9325 - val_loss: 0.3033 - val_acc: 0.8906\n",
      "Epoch 127/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1576 - acc: 0.9371Epoch 00126: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1558 - acc: 0.9371 - val_loss: 0.3022 - val_acc: 0.9034\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1589 - acc: 0.9288Epoch 00127: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1588 - acc: 0.9285 - val_loss: 0.2779 - val_acc: 0.9013\n",
      "Epoch 129/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1476 - acc: 0.9304Epoch 00128: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1435 - acc: 0.9317 - val_loss: 0.3180 - val_acc: 0.8927\n",
      "Epoch 130/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1350 - acc: 0.9404Epoch 00129: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1351 - acc: 0.9403 - val_loss: 0.3054 - val_acc: 0.9077\n",
      "Epoch 131/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1555 - acc: 0.9306Epoch 00130: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1549 - acc: 0.9302 - val_loss: 0.2896 - val_acc: 0.9034\n",
      "Epoch 132/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1450 - acc: 0.9325Epoch 00131: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1501 - acc: 0.9299 - val_loss: 0.3123 - val_acc: 0.9120\n",
      "Epoch 133/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2004 - acc: 0.9059Epoch 00132: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1958 - acc: 0.9064 - val_loss: 0.3623 - val_acc: 0.8884\n",
      "Epoch 134/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1739 - acc: 0.9193Epoch 00133: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1743 - acc: 0.9194 - val_loss: 0.2976 - val_acc: 0.8970\n",
      "Epoch 135/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1469 - acc: 0.9378Epoch 00134: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1437 - acc: 0.9393 - val_loss: 0.3261 - val_acc: 0.8948\n",
      "Epoch 136/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1581 - acc: 0.9277Epoch 00135: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1594 - acc: 0.9275 - val_loss: 0.2954 - val_acc: 0.8863\n",
      "Epoch 137/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1490 - acc: 0.9366Epoch 00136: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1503 - acc: 0.9368 - val_loss: 0.2962 - val_acc: 0.9056\n",
      "Epoch 138/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1603 - acc: 0.9399Epoch 00137: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1568 - acc: 0.9411 - val_loss: 0.3272 - val_acc: 0.8927\n",
      "Epoch 139/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1660 - acc: 0.9311Epoch 00138: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1633 - acc: 0.9335 - val_loss: 0.3159 - val_acc: 0.8906\n",
      "Epoch 140/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1590 - acc: 0.9324Epoch 00139: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1608 - acc: 0.9292 - val_loss: 0.3047 - val_acc: 0.9013\n",
      "Epoch 141/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1464 - acc: 0.9390Epoch 00140: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1442 - acc: 0.9403 - val_loss: 0.3010 - val_acc: 0.9056\n",
      "Epoch 142/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1500 - acc: 0.9388Epoch 00141: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1482 - acc: 0.9394 - val_loss: 0.3480 - val_acc: 0.8927\n",
      "Epoch 143/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1655 - acc: 0.9259Epoch 00142: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1676 - acc: 0.9228 - val_loss: 0.3115 - val_acc: 0.8927\n",
      "Epoch 144/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1717 - acc: 0.9286Epoch 00143: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1689 - acc: 0.9297 - val_loss: 0.3168 - val_acc: 0.8884\n",
      "Epoch 145/200\n",
      "11/21 [==============>...............] - ETA: 1s - loss: 0.1571 - acc: 0.9290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.889000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.446000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1656 - acc: 0.9278Epoch 00144: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1637 - acc: 0.9291 - val_loss: 0.2999 - val_acc: 0.8906\n",
      "Epoch 146/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1521 - acc: 0.9365Epoch 00145: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1523 - acc: 0.9380 - val_loss: 0.3085 - val_acc: 0.8991\n",
      "Epoch 147/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1663 - acc: 0.9267Epoch 00146: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1686 - acc: 0.9265 - val_loss: 0.3609 - val_acc: 0.8970\n",
      "Epoch 148/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1604 - acc: 0.9317Epoch 00147: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1593 - acc: 0.9326 - val_loss: 0.3060 - val_acc: 0.8927\n",
      "Epoch 149/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1700 - acc: 0.9277Epoch 00148: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1675 - acc: 0.9289 - val_loss: 0.3085 - val_acc: 0.8991\n",
      "Epoch 150/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1397 - acc: 0.9378Epoch 00149: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1436 - acc: 0.9380 - val_loss: 0.2998 - val_acc: 0.8927\n",
      "Epoch 151/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1474 - acc: 0.9394Epoch 00150: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1492 - acc: 0.9393 - val_loss: 0.3387 - val_acc: 0.8927\n",
      "Epoch 152/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1729 - acc: 0.9325Epoch 00151: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1707 - acc: 0.9335 - val_loss: 0.3180 - val_acc: 0.8884\n",
      "Epoch 153/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1537 - acc: 0.9317Epoch 00152: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1520 - acc: 0.9334 - val_loss: 0.2794 - val_acc: 0.9099\n",
      "Epoch 154/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1489 - acc: 0.9338Epoch 00153: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1505 - acc: 0.9339 - val_loss: 0.3319 - val_acc: 0.8841\n",
      "Epoch 155/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1494 - acc: 0.9437Epoch 00154: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1490 - acc: 0.9441 - val_loss: 0.2854 - val_acc: 0.8970\n",
      "Epoch 156/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1293 - acc: 0.9494Epoch 00155: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1299 - acc: 0.9474 - val_loss: 0.3052 - val_acc: 0.9120\n",
      "Epoch 157/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1441 - acc: 0.9383Epoch 00156: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1433 - acc: 0.9383 - val_loss: 0.2854 - val_acc: 0.9056\n",
      "Epoch 158/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1401 - acc: 0.9370Epoch 00157: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1390 - acc: 0.9384 - val_loss: 0.3262 - val_acc: 0.8970\n",
      "Epoch 159/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1486 - acc: 0.9349Epoch 00158: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1477 - acc: 0.9343 - val_loss: 0.3302 - val_acc: 0.8991\n",
      "Epoch 160/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1596 - acc: 0.9317Epoch 00159: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1586 - acc: 0.9319 - val_loss: 0.3267 - val_acc: 0.9013\n",
      "Epoch 161/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1490 - acc: 0.9364Epoch 00160: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1552 - acc: 0.9336 - val_loss: 0.2858 - val_acc: 0.9077\n",
      "Epoch 162/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1580 - acc: 0.9275Epoch 00161: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1585 - acc: 0.9281 - val_loss: 0.2834 - val_acc: 0.8884\n",
      "Epoch 163/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1547 - acc: 0.9301Epoch 00162: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1631 - acc: 0.9297 - val_loss: 0.3110 - val_acc: 0.8991\n",
      "Epoch 164/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1680 - acc: 0.9280Epoch 00163: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1672 - acc: 0.9276 - val_loss: 0.3057 - val_acc: 0.8863\n",
      "Epoch 165/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1345 - acc: 0.9474Epoch 00164: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1347 - acc: 0.9472 - val_loss: 0.2904 - val_acc: 0.8991\n",
      "Epoch 166/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1258 - acc: 0.9452Epoch 00165: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1238 - acc: 0.9463 - val_loss: 0.3469 - val_acc: 0.8948\n",
      "Epoch 167/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1434 - acc: 0.9421Epoch 00166: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1440 - acc: 0.9404 - val_loss: 0.3158 - val_acc: 0.8906\n",
      "Epoch 168/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1444 - acc: 0.9441Epoch 00167: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1470 - acc: 0.9424 - val_loss: 0.3143 - val_acc: 0.9120\n",
      "Epoch 169/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1479 - acc: 0.9385Epoch 00168: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1476 - acc: 0.9379 - val_loss: 0.3292 - val_acc: 0.9056\n",
      "Epoch 170/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1713 - acc: 0.9316Epoch 00169: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1686 - acc: 0.9326 - val_loss: 0.2962 - val_acc: 0.8863\n",
      "Epoch 171/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1555 - acc: 0.9282Epoch 00170: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1570 - acc: 0.9258 - val_loss: 0.3124 - val_acc: 0.9056\n",
      "Epoch 172/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1542 - acc: 0.9297Epoch 00171: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1535 - acc: 0.9307 - val_loss: 0.3144 - val_acc: 0.8927\n",
      "Epoch 173/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1068 - acc: 0.9568Epoch 00172: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1108 - acc: 0.9573 - val_loss: 0.3196 - val_acc: 0.8970\n",
      "Epoch 174/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1482 - acc: 0.9379Epoch 00173: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1482 - acc: 0.9378 - val_loss: 0.3210 - val_acc: 0.9013\n",
      "Epoch 175/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1337 - acc: 0.9394Epoch 00174: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1341 - acc: 0.9379 - val_loss: 0.3168 - val_acc: 0.8991\n",
      "Epoch 176/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1417 - acc: 0.9447Epoch 00175: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1403 - acc: 0.9458 - val_loss: 0.3155 - val_acc: 0.8841\n",
      "Epoch 177/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1514 - acc: 0.9347Epoch 00176: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1535 - acc: 0.9333 - val_loss: 0.2986 - val_acc: 0.9034\n",
      "Epoch 178/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1475 - acc: 0.9393Epoch 00177: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1445 - acc: 0.9397 - val_loss: 0.2865 - val_acc: 0.9077\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1445 - acc: 0.9339Epoch 00178: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1468 - acc: 0.9326 - val_loss: 0.2830 - val_acc: 0.9056\n",
      "Epoch 180/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1231 - acc: 0.9482Epoch 00179: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1217 - acc: 0.9492 - val_loss: 0.3036 - val_acc: 0.8991\n",
      "Epoch 181/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1193 - acc: 0.9509Epoch 00180: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1182 - acc: 0.9524 - val_loss: 0.3213 - val_acc: 0.8927\n",
      "Epoch 182/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1253 - acc: 0.9516Epoch 00181: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1246 - acc: 0.9509 - val_loss: 0.3128 - val_acc: 0.8991\n",
      "Epoch 183/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1245 - acc: 0.9401Epoch 00182: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1245 - acc: 0.9400 - val_loss: 0.3200 - val_acc: 0.8863\n",
      "Epoch 184/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1388 - acc: 0.9437Epoch 00183: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1408 - acc: 0.9420 - val_loss: 0.3212 - val_acc: 0.8991\n",
      "Epoch 185/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1464 - acc: 0.9332Epoch 00184: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1523 - acc: 0.9311 - val_loss: 0.3411 - val_acc: 0.8906\n",
      "Epoch 186/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1850 - acc: 0.9221Epoch 00185: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1887 - acc: 0.9207 - val_loss: 0.2933 - val_acc: 0.8820\n",
      "Epoch 187/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1694 - acc: 0.9290Epoch 00186: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1664 - acc: 0.9300 - val_loss: 0.3189 - val_acc: 0.9013\n",
      "Epoch 188/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1455 - acc: 0.9347Epoch 00187: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1467 - acc: 0.9340 - val_loss: 0.3288 - val_acc: 0.8863\n",
      "Epoch 189/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1381 - acc: 0.9461Epoch 00188: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1377 - acc: 0.9471 - val_loss: 0.3566 - val_acc: 0.8884\n",
      "Epoch 190/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1349 - acc: 0.9467Epoch 00189: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1315 - acc: 0.9484 - val_loss: 0.3176 - val_acc: 0.8970\n",
      "Epoch 191/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.1304 - acc: 0.9461Epoch 00190: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1386 - acc: 0.9398 - val_loss: 0.3398 - val_acc: 0.9142\n",
      "Epoch 192/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1248 - acc: 0.9519Epoch 00191: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1268 - acc: 0.9505 - val_loss: 0.3420 - val_acc: 0.8863\n",
      "Epoch 193/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1109 - acc: 0.9481Epoch 00192: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1137 - acc: 0.9495 - val_loss: 0.3231 - val_acc: 0.8906\n",
      "Epoch 194/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1267 - acc: 0.9467Epoch 00193: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1240 - acc: 0.9484 - val_loss: 0.3324 - val_acc: 0.9077\n",
      "Epoch 195/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1345 - acc: 0.9459Epoch 00194: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1363 - acc: 0.9458 - val_loss: 0.3163 - val_acc: 0.8991\n",
      "Epoch 196/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1244 - acc: 0.9511Epoch 00195: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1272 - acc: 0.9505 - val_loss: 0.3261 - val_acc: 0.9142\n",
      "Epoch 197/200\n",
      "12/21 [===============>..............] - ETA: 1s - loss: 0.1263 - acc: 0.9458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.987000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.546750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.106501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1250 - acc: 0.9459Epoch 00196: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1293 - acc: 0.9434 - val_loss: 0.3354 - val_acc: 0.9077\n",
      "Epoch 198/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1236 - acc: 0.9547Epoch 00197: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1270 - acc: 0.9503 - val_loss: 0.3277 - val_acc: 0.8906\n",
      "Epoch 199/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1434 - acc: 0.9391Epoch 00198: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1417 - acc: 0.9390 - val_loss: 0.3320 - val_acc: 0.8948\n",
      "Epoch 200/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1648 - acc: 0.9351Epoch 00199: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1625 - acc: 0.9367 - val_loss: 0.3153 - val_acc: 0.8906\n",
      "32/74 [===========>..................] - ETA: 5sTest loss: 0.315076809477\n",
      "Test accuracy: 0.851351348129\n",
      "Fold: 2\n",
      "Epoch 1/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2329 - acc: 0.9177- ETA: 0s - loss: 0.2574 - accEpoch 00000: val_loss improved from 0.20815 to 0.11009, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "22/21 [==============================] - 1s - loss: 0.2397 - acc: 0.9144 - val_loss: 0.1101 - val_acc: 0.9634\n",
      "Epoch 2/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2458 - acc: 0.9028Epoch 00001: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2438 - acc: 0.9016 - val_loss: 0.1785 - val_acc: 0.9419\n",
      "Epoch 3/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2074 - acc: 0.9076Epoch 00002: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2076 - acc: 0.9068 - val_loss: 0.1370 - val_acc: 0.9484\n",
      "Epoch 4/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2096 - acc: 0.9118Epoch 00003: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2156 - acc: 0.9094 - val_loss: 0.1409 - val_acc: 0.9419\n",
      "Epoch 5/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1935 - acc: 0.9205Epoch 00004: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1901 - acc: 0.9226 - val_loss: 0.1421 - val_acc: 0.9419\n",
      "Epoch 6/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2134 - acc: 0.9093- ETA: 0s - loss: 0.2183 - acc: 0Epoch 00005: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2113 - acc: 0.9099 - val_loss: 0.1378 - val_acc: 0.9376\n",
      "Epoch 7/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1826 - acc: 0.9276Epoch 00006: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1768 - acc: 0.9299 - val_loss: 0.1338 - val_acc: 0.9505\n",
      "Epoch 8/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9187Epoch 00007: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1950 - acc: 0.9202 - val_loss: 0.1289 - val_acc: 0.9527\n",
      "Epoch 9/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1740 - acc: 0.9209Epoch 00008: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1739 - acc: 0.9216 - val_loss: 0.1704 - val_acc: 0.9333\n",
      "Epoch 10/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1955 - acc: 0.9201Epoch 00009: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1965 - acc: 0.9202 - val_loss: 0.1481 - val_acc: 0.9419\n",
      "Epoch 11/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1895 - acc: 0.9184Epoch 00010: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1884 - acc: 0.9179 - val_loss: 0.1352 - val_acc: 0.9484\n",
      "Epoch 12/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1861 - acc: 0.9196Epoch 00011: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1842 - acc: 0.9204 - val_loss: 0.1975 - val_acc: 0.9011\n",
      "Epoch 13/200\n",
      "13/21 [================>.............] - ETA: 1s - loss: 0.2222 - acc: 0.9119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.986500). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.641250). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.296000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.149250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/21 [==========================>...] - ETA: 0s - loss: 0.2097 - acc: 0.9154Epoch 00012: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.2129 - acc: 0.9143 - val_loss: 0.1608 - val_acc: 0.9355\n",
      "Epoch 14/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1857 - acc: 0.9289Epoch 00013: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1843 - acc: 0.9286 - val_loss: 0.1533 - val_acc: 0.9312\n",
      "Epoch 15/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1824 - acc: 0.9177Epoch 00014: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1879 - acc: 0.9152 - val_loss: 0.1576 - val_acc: 0.9441\n",
      "Epoch 16/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1871 - acc: 0.9209Epoch 00015: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1868 - acc: 0.9209 - val_loss: 0.1468 - val_acc: 0.9398\n",
      "Epoch 17/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1662 - acc: 0.9289Epoch 00016: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1649 - acc: 0.9293 - val_loss: 0.1330 - val_acc: 0.9505\n",
      "Epoch 18/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1729 - acc: 0.9242Epoch 00017: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1733 - acc: 0.9248 - val_loss: 0.1466 - val_acc: 0.9462\n",
      "Epoch 19/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1748 - acc: 0.9187Epoch 00018: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1727 - acc: 0.9195 - val_loss: 0.2038 - val_acc: 0.9183\n",
      "Epoch 20/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1701 - acc: 0.9311Epoch 00019: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1696 - acc: 0.9314 - val_loss: 0.1552 - val_acc: 0.9312\n",
      "Epoch 21/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1932 - acc: 0.9227Epoch 00020: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1921 - acc: 0.9234 - val_loss: 0.1446 - val_acc: 0.9505\n",
      "Epoch 22/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1677 - acc: 0.9263Epoch 00021: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1668 - acc: 0.9274 - val_loss: 0.1499 - val_acc: 0.9462\n",
      "Epoch 23/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1977 - acc: 0.9067Epoch 00022: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1937 - acc: 0.9074 - val_loss: 0.1464 - val_acc: 0.9419\n",
      "Epoch 24/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1681 - acc: 0.9244Epoch 00023: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1664 - acc: 0.9250 - val_loss: 0.1517 - val_acc: 0.9419\n",
      "Epoch 25/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1670 - acc: 0.9291Epoch 00024: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1655 - acc: 0.9288 - val_loss: 0.1619 - val_acc: 0.9376\n",
      "Epoch 26/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1697 - acc: 0.9263Epoch 00025: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1685 - acc: 0.9260 - val_loss: 0.1569 - val_acc: 0.9398\n",
      "Epoch 27/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1719 - acc: 0.9226Epoch 00026: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1706 - acc: 0.9232 - val_loss: 0.1563 - val_acc: 0.9419\n",
      "Epoch 28/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1558 - acc: 0.9365Epoch 00027: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1549 - acc: 0.9381 - val_loss: 0.1534 - val_acc: 0.9505\n",
      "Epoch 29/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1473 - acc: 0.9390Epoch 00028: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1444 - acc: 0.9410 - val_loss: 0.1469 - val_acc: 0.9462\n",
      "Epoch 30/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1829 - acc: 0.9227Epoch 00029: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1813 - acc: 0.9237 - val_loss: 0.1549 - val_acc: 0.9419\n",
      "Epoch 31/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1642 - acc: 0.9216Epoch 00030: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1628 - acc: 0.9238 - val_loss: 0.1693 - val_acc: 0.9312\n",
      "Epoch 32/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1599 - acc: 0.9382Epoch 00031: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1599 - acc: 0.9375 - val_loss: 0.1766 - val_acc: 0.9312\n",
      "Epoch 33/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1596 - acc: 0.9277Epoch 00032: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1602 - acc: 0.9274 - val_loss: 0.1770 - val_acc: 0.9355\n",
      "Epoch 34/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1522 - acc: 0.9368Epoch 00033: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1516 - acc: 0.9353 - val_loss: 0.1939 - val_acc: 0.9204\n",
      "Epoch 35/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1709 - acc: 0.9229Epoch 00034: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1693 - acc: 0.9228 - val_loss: 0.1517 - val_acc: 0.9355\n",
      "Epoch 36/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1536 - acc: 0.9350Epoch 00035: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1559 - acc: 0.9330 - val_loss: 0.1742 - val_acc: 0.9290\n",
      "Epoch 37/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1842 - acc: 0.9215Epoch 00036: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1815 - acc: 0.9230 - val_loss: 0.1669 - val_acc: 0.9484\n",
      "Epoch 38/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1808 - acc: 0.9260Epoch 00037: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1827 - acc: 0.9248 - val_loss: 0.1882 - val_acc: 0.9247\n",
      "Epoch 39/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1633 - acc: 0.9358Epoch 00038: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1641 - acc: 0.9372 - val_loss: 0.2129 - val_acc: 0.9247\n",
      "Epoch 40/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.2064 - acc: 0.9032Epoch 00039: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.2043 - acc: 0.9041 - val_loss: 0.2117 - val_acc: 0.9075\n",
      "Epoch 41/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1781 - acc: 0.9195Epoch 00040: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1789 - acc: 0.9189 - val_loss: 0.1866 - val_acc: 0.9183\n",
      "Epoch 42/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1606 - acc: 0.9341Epoch 00041: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1573 - acc: 0.9357 - val_loss: 0.1691 - val_acc: 0.9226\n",
      "Epoch 43/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1822 - acc: 0.9312Epoch 00042: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1746 - acc: 0.9340 - val_loss: 0.1871 - val_acc: 0.9312\n",
      "Epoch 44/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1748 - acc: 0.9224Epoch 00043: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1750 - acc: 0.9245 - val_loss: 0.2021 - val_acc: 0.9161\n",
      "Epoch 45/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1668 - acc: 0.9296Epoch 00044: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1625 - acc: 0.9328 - val_loss: 0.1713 - val_acc: 0.9333\n",
      "Epoch 46/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1493 - acc: 0.9356Epoch 00045: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1496 - acc: 0.9350 - val_loss: 0.1993 - val_acc: 0.9161\n",
      "Epoch 47/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1537 - acc: 0.9287Epoch 00046: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/21 [==============================] - 1s - loss: 0.1496 - acc: 0.9312 - val_loss: 0.1688 - val_acc: 0.9226\n",
      "Epoch 48/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1590 - acc: 0.9369Epoch 00047: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1604 - acc: 0.9361 - val_loss: 0.1963 - val_acc: 0.9204\n",
      "Epoch 49/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1596 - acc: 0.9270Epoch 00048: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1561 - acc: 0.9289 - val_loss: 0.1708 - val_acc: 0.9312\n",
      "Epoch 50/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1458 - acc: 0.9423Epoch 00049: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1501 - acc: 0.9392 - val_loss: 0.1746 - val_acc: 0.9247\n",
      "Epoch 51/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1500 - acc: 0.9341Epoch 00050: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1485 - acc: 0.9350 - val_loss: 0.1610 - val_acc: 0.9269\n",
      "Epoch 52/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1414 - acc: 0.9395Epoch 00051: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1423 - acc: 0.9386 - val_loss: 0.1678 - val_acc: 0.9226\n",
      "Epoch 53/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1504 - acc: 0.9410Epoch 00052: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1495 - acc: 0.9423 - val_loss: 0.1799 - val_acc: 0.9183\n",
      "Epoch 54/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1454 - acc: 0.9373Epoch 00053: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1454 - acc: 0.9372 - val_loss: 0.1597 - val_acc: 0.9376\n",
      "Epoch 55/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1408 - acc: 0.9451Epoch 00054: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1424 - acc: 0.9447 - val_loss: 0.1494 - val_acc: 0.9355\n",
      "Epoch 56/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1373 - acc: 0.9413Epoch 00055: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1349 - acc: 0.9433 - val_loss: 0.1611 - val_acc: 0.9290\n",
      "Epoch 57/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1413 - acc: 0.9371Epoch 00056: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1436 - acc: 0.9357 - val_loss: 0.1672 - val_acc: 0.9355\n",
      "Epoch 58/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1293 - acc: 0.9435Epoch 00057: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1318 - acc: 0.9438 - val_loss: 0.1921 - val_acc: 0.9290\n",
      "Epoch 59/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1311 - acc: 0.9377Epoch 00058: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1325 - acc: 0.9376 - val_loss: 0.1817 - val_acc: 0.9290\n",
      "Epoch 60/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1391 - acc: 0.9490Epoch 00059: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1384 - acc: 0.9488 - val_loss: 0.1577 - val_acc: 0.9355\n",
      "Epoch 61/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1559 - acc: 0.9326Epoch 00060: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1601 - acc: 0.9321 - val_loss: 0.2238 - val_acc: 0.9269\n",
      "Epoch 62/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1643 - acc: 0.9335Epoch 00061: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1615 - acc: 0.9344 - val_loss: 0.1804 - val_acc: 0.9376\n",
      "Epoch 63/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1456 - acc: 0.9322Epoch 00062: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1470 - acc: 0.9310 - val_loss: 0.1898 - val_acc: 0.9247\n",
      "Epoch 64/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1317 - acc: 0.9430Epoch 00063: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1292 - acc: 0.9449 - val_loss: 0.2244 - val_acc: 0.9247\n",
      "Epoch 65/200\n",
      "12/21 [===============>..............] - ETA: 1s - loss: 0.1506 - acc: 0.9264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.985016). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.599008). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.213001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.107250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1431 - acc: 0.9314Epoch 00064: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1423 - acc: 0.9332 - val_loss: 0.1798 - val_acc: 0.9312\n",
      "Epoch 66/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1527 - acc: 0.9317Epoch 00065: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1511 - acc: 0.9327 - val_loss: 0.1917 - val_acc: 0.9226\n",
      "Epoch 67/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1706 - acc: 0.9268Epoch 00066: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1695 - acc: 0.9258 - val_loss: 0.2441 - val_acc: 0.9226\n",
      "Epoch 68/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1606 - acc: 0.9347Epoch 00067: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1612 - acc: 0.9355 - val_loss: 0.2082 - val_acc: 0.9204\n",
      "Epoch 69/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1377 - acc: 0.9408Epoch 00068: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1362 - acc: 0.9427 - val_loss: 0.1929 - val_acc: 0.9312\n",
      "Epoch 70/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1320 - acc: 0.9456Epoch 00069: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1313 - acc: 0.9460 - val_loss: 0.2007 - val_acc: 0.9290\n",
      "Epoch 71/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1249 - acc: 0.9450Epoch 00070: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1245 - acc: 0.9461 - val_loss: 0.2192 - val_acc: 0.9161\n",
      "Epoch 72/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1329 - acc: 0.9386Epoch 00071: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1314 - acc: 0.9399 - val_loss: 0.1973 - val_acc: 0.9269\n",
      "Epoch 73/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1525 - acc: 0.9392Epoch 00072: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1530 - acc: 0.9401 - val_loss: 0.1984 - val_acc: 0.9226\n",
      "Epoch 74/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1363 - acc: 0.9473Epoch 00073: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1347 - acc: 0.9469 - val_loss: 0.1993 - val_acc: 0.9269\n",
      "Epoch 75/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.1578 - acc: 0.9280Epoch 00074: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1622 - acc: 0.9278 - val_loss: 0.1969 - val_acc: 0.9140\n",
      "Epoch 76/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1509 - acc: 0.9414Epoch 00075: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1513 - acc: 0.9412 - val_loss: 0.1935 - val_acc: 0.9269\n",
      "Epoch 77/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1464 - acc: 0.9412Epoch 00076: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1469 - acc: 0.9403 - val_loss: 0.2064 - val_acc: 0.9183\n",
      "Epoch 78/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1340 - acc: 0.9398Epoch 00077: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1322 - acc: 0.9411 - val_loss: 0.2156 - val_acc: 0.9161\n",
      "Epoch 79/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1414 - acc: 0.9395Epoch 00078: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1385 - acc: 0.9415 - val_loss: 0.1720 - val_acc: 0.9398\n",
      "Epoch 80/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1248 - acc: 0.9392Epoch 00079: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1248 - acc: 0.9406 - val_loss: 0.1898 - val_acc: 0.9290\n",
      "Epoch 81/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1426 - acc: 0.9399Epoch 00080: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1419 - acc: 0.9398 - val_loss: 0.1978 - val_acc: 0.9247\n",
      "Epoch 82/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1316 - acc: 0.9435Epoch 00081: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1288 - acc: 0.9454 - val_loss: 0.1791 - val_acc: 0.9247\n",
      "Epoch 83/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1354 - acc: 0.9404Epoch 00082: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1350 - acc: 0.9417 - val_loss: 0.2278 - val_acc: 0.9161\n",
      "Epoch 84/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1416 - acc: 0.9382Epoch 00083: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1408 - acc: 0.9388 - val_loss: 0.2046 - val_acc: 0.9183\n",
      "Epoch 85/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1262 - acc: 0.9505Epoch 00084: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1302 - acc: 0.9492 - val_loss: 0.2224 - val_acc: 0.9140\n",
      "Epoch 86/200\n",
      "14/21 [==================>...........] - ETA: 0s - loss: 0.1297 - acc: 0.9407"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.125001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.217000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1294 - acc: 0.9458Epoch 00085: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1294 - acc: 0.9447 - val_loss: 0.2003 - val_acc: 0.9247\n",
      "Epoch 87/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1332 - acc: 0.9392Epoch 00086: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1323 - acc: 0.9398 - val_loss: 0.1893 - val_acc: 0.9290\n",
      "Epoch 88/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1225 - acc: 0.9511Epoch 00087: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1258 - acc: 0.9502 - val_loss: 0.2243 - val_acc: 0.9183\n",
      "Epoch 89/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1246 - acc: 0.9466Epoch 00088: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1245 - acc: 0.9476 - val_loss: 0.2079 - val_acc: 0.9312\n",
      "Epoch 90/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1730 - acc: 0.9191Epoch 00089: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1702 - acc: 0.9216 - val_loss: 0.2049 - val_acc: 0.9204\n",
      "Epoch 91/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1393 - acc: 0.9345Epoch 00090: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1405 - acc: 0.9353 - val_loss: 0.1955 - val_acc: 0.9247\n",
      "Epoch 92/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1548 - acc: 0.9363Epoch 00091: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1541 - acc: 0.9371 - val_loss: 0.2119 - val_acc: 0.9247\n",
      "Epoch 93/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1611 - acc: 0.9359Epoch 00092: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1606 - acc: 0.9367 - val_loss: 0.1921 - val_acc: 0.9269\n",
      "Epoch 94/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1273 - acc: 0.9435Epoch 00093: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1226 - acc: 0.9462 - val_loss: 0.2062 - val_acc: 0.9247\n",
      "Epoch 95/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1474 - acc: 0.9301Epoch 00094: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1463 - acc: 0.9298 - val_loss: 0.1839 - val_acc: 0.9290\n",
      "Epoch 96/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1283 - acc: 0.9410- ETA: 0s - loss: 0.1435 - aEpoch 00095: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1255 - acc: 0.9428 - val_loss: 0.2511 - val_acc: 0.9204\n",
      "Epoch 97/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1211 - acc: 0.9499Epoch 00096: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1182 - acc: 0.9515 - val_loss: 0.2176 - val_acc: 0.9097\n",
      "Epoch 98/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1384 - acc: 0.9341Epoch 00097: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1383 - acc: 0.9343 - val_loss: 0.2095 - val_acc: 0.9269\n",
      "Epoch 99/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1422 - acc: 0.9370Epoch 00098: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1393 - acc: 0.9385 - val_loss: 0.2110 - val_acc: 0.9204\n",
      "Epoch 100/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1254 - acc: 0.9419Epoch 00099: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1294 - acc: 0.9410 - val_loss: 0.2153 - val_acc: 0.9204\n",
      "Epoch 101/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1546 - acc: 0.9360Epoch 00100: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1581 - acc: 0.9355 - val_loss: 0.2180 - val_acc: 0.9226\n",
      "Epoch 102/200\n",
      "12/21 [===============>..............] - ETA: 1s - loss: 0.1522 - acc: 0.9388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.957001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.681502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.406003). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.204002). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1541 - acc: 0.9358Epoch 00101: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1545 - acc: 0.9344 - val_loss: 0.2364 - val_acc: 0.9161\n",
      "Epoch 103/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1420 - acc: 0.9401Epoch 00102: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1434 - acc: 0.9357 - val_loss: 0.2400 - val_acc: 0.9247\n",
      "Epoch 104/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1842 - acc: 0.9280- ETA: 0s - loss: 0.1401Epoch 00103: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1874 - acc: 0.9227 - val_loss: 0.2334 - val_acc: 0.9140\n",
      "Epoch 105/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1483 - acc: 0.9410Epoch 00104: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1466 - acc: 0.9399 - val_loss: 0.2304 - val_acc: 0.9140\n",
      "Epoch 106/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1479 - acc: 0.9341Epoch 00105: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1456 - acc: 0.9350 - val_loss: 0.2088 - val_acc: 0.9161\n",
      "Epoch 107/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.1391 - acc: 0.9415Epoch 00106: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1377 - acc: 0.9417 - val_loss: 0.2858 - val_acc: 0.9140\n",
      "Epoch 108/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1375 - acc: 0.9430Epoch 00107: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1370 - acc: 0.9428 - val_loss: 0.2341 - val_acc: 0.9140\n",
      "Epoch 109/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1384 - acc: 0.9360Epoch 00108: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1397 - acc: 0.9360 - val_loss: 0.2323 - val_acc: 0.9140\n",
      "Epoch 110/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1422 - acc: 0.9395Epoch 00109: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1453 - acc: 0.9386 - val_loss: 0.2215 - val_acc: 0.9140\n",
      "Epoch 111/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9457Epoch 00110: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1202 - acc: 0.9446 - val_loss: 0.2285 - val_acc: 0.9118\n",
      "Epoch 112/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1165 - acc: 0.9512Epoch 00111: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1153 - acc: 0.9513 - val_loss: 0.2617 - val_acc: 0.9075\n",
      "Epoch 113/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1668 - acc: 0.9349Epoch 00112: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1641 - acc: 0.9371 - val_loss: 0.2194 - val_acc: 0.9183\n",
      "Epoch 114/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1490 - acc: 0.9319Epoch 00113: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1476 - acc: 0.9328 - val_loss: 0.2476 - val_acc: 0.9097\n",
      "Epoch 115/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1525 - acc: 0.9302Epoch 00114: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1482 - acc: 0.9327 - val_loss: 0.2479 - val_acc: 0.9140\n",
      "Epoch 116/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1567 - acc: 0.9316Epoch 00115: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1581 - acc: 0.9312 - val_loss: 0.2581 - val_acc: 0.9097\n",
      "Epoch 117/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1241 - acc: 0.9466Epoch 00116: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1225 - acc: 0.9469 - val_loss: 0.2131 - val_acc: 0.9161\n",
      "Epoch 118/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1309 - acc: 0.9398Epoch 00117: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1274 - acc: 0.9426 - val_loss: 0.2728 - val_acc: 0.9161\n",
      "Epoch 119/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1263 - acc: 0.9511Epoch 00118: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1254 - acc: 0.9511 - val_loss: 0.2585 - val_acc: 0.9118\n",
      "Epoch 120/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1402 - acc: 0.9395Epoch 00119: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1393 - acc: 0.9397 - val_loss: 0.2529 - val_acc: 0.9011\n",
      "Epoch 121/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1246 - acc: 0.9430Epoch 00120: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1239 - acc: 0.9442 - val_loss: 0.2643 - val_acc: 0.9204\n",
      "Epoch 122/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1167 - acc: 0.9524Epoch 00121: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1153 - acc: 0.9524 - val_loss: 0.2113 - val_acc: 0.9226\n",
      "Epoch 123/200\n",
      "14/21 [==================>...........] - ETA: 0s - loss: 0.1109 - acc: 0.9505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.249500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1191 - acc: 0.9465Epoch 00122: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1192 - acc: 0.9468 - val_loss: 0.2481 - val_acc: 0.9118\n",
      "Epoch 124/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1177 - acc: 0.9468Epoch 00123: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1170 - acc: 0.9463 - val_loss: 0.2314 - val_acc: 0.9161\n",
      "Epoch 125/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1398 - acc: 0.9382Epoch 00124: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1361 - acc: 0.9395 - val_loss: 0.2461 - val_acc: 0.9097\n",
      "Epoch 126/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1410 - acc: 0.9365Epoch 00125: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1402 - acc: 0.9373 - val_loss: 0.2343 - val_acc: 0.9054\n",
      "Epoch 127/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1147 - acc: 0.9542Epoch 00126: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1166 - acc: 0.9527 - val_loss: 0.2256 - val_acc: 0.9204\n",
      "Epoch 128/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1325 - acc: 0.9545Epoch 00127: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1312 - acc: 0.9552 - val_loss: 0.2697 - val_acc: 0.9161\n",
      "Epoch 129/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1358 - acc: 0.9385Epoch 00128: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1353 - acc: 0.9385 - val_loss: 0.2371 - val_acc: 0.9118\n",
      "Epoch 130/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1303 - acc: 0.9444Epoch 00129: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1261 - acc: 0.9469 - val_loss: 0.2148 - val_acc: 0.9269\n",
      "Epoch 131/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1142 - acc: 0.9419Epoch 00130: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1144 - acc: 0.9417 - val_loss: 0.2175 - val_acc: 0.9312\n",
      "Epoch 132/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1359 - acc: 0.9386Epoch 00131: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1377 - acc: 0.9371 - val_loss: 0.2500 - val_acc: 0.9161\n",
      "Epoch 133/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1349 - acc: 0.9371Epoch 00132: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1315 - acc: 0.9394 - val_loss: 0.2077 - val_acc: 0.9204\n",
      "Epoch 134/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1185 - acc: 0.9475Epoch 00133: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1187 - acc: 0.9478 - val_loss: 0.2644 - val_acc: 0.9032\n",
      "Epoch 135/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1218 - acc: 0.9468Epoch 00134: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1200 - acc: 0.9479 - val_loss: 0.2292 - val_acc: 0.9226\n",
      "Epoch 136/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1173 - acc: 0.9475Epoch 00135: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1174 - acc: 0.9471 - val_loss: 0.2277 - val_acc: 0.9140\n",
      "Epoch 137/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1070 - acc: 0.9529Epoch 00136: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1096 - acc: 0.9515 - val_loss: 0.2336 - val_acc: 0.9226\n",
      "Epoch 138/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1067 - acc: 0.9474Epoch 00137: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1100 - acc: 0.9458 - val_loss: 0.2389 - val_acc: 0.9204\n",
      "Epoch 139/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1162 - acc: 0.9449Epoch 00138: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1141 - acc: 0.9460 - val_loss: 0.2757 - val_acc: 0.9075\n",
      "Epoch 140/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1119 - acc: 0.9525Epoch 00139: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1129 - acc: 0.9532 - val_loss: 0.2525 - val_acc: 0.9140\n",
      "Epoch 141/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1226 - acc: 0.9438Epoch 00140: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1191 - acc: 0.9456 - val_loss: 0.2874 - val_acc: 0.9032\n",
      "Epoch 142/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1310 - acc: 0.9462Epoch 00141: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1348 - acc: 0.9436 - val_loss: 0.2296 - val_acc: 0.9140\n",
      "Epoch 143/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1166 - acc: 0.9512Epoch 00142: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1181 - acc: 0.9506 - val_loss: 0.2444 - val_acc: 0.9226\n",
      "Epoch 144/200\n",
      "14/21 [==================>...........] - ETA: 1s - loss: 0.0991 - acc: 0.9503"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.305501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1001 - acc: 0.9493Epoch 00143: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.0982 - acc: 0.9509 - val_loss: 0.2490 - val_acc: 0.9118\n",
      "Epoch 145/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1185 - acc: 0.9496Epoch 00144: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1167 - acc: 0.9511 - val_loss: 0.2577 - val_acc: 0.9075\n",
      "Epoch 146/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1098 - acc: 0.9521Epoch 00145: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1080 - acc: 0.9536 - val_loss: 0.2471 - val_acc: 0.9054\n",
      "Epoch 147/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1428 - acc: 0.9408Epoch 00146: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1432 - acc: 0.9399 - val_loss: 0.2604 - val_acc: 0.9118\n",
      "Epoch 148/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1498 - acc: 0.9355Epoch 00147: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1473 - acc: 0.9399 - val_loss: 0.2262 - val_acc: 0.9204\n",
      "Epoch 149/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1175 - acc: 0.9492Epoch 00148: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1160 - acc: 0.9494 - val_loss: 0.2530 - val_acc: 0.9097\n",
      "Epoch 150/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1092 - acc: 0.9548Epoch 00149: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1075 - acc: 0.9555 - val_loss: 0.2291 - val_acc: 0.9204\n",
      "Epoch 151/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0906 - acc: 0.9600Epoch 00150: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0902 - acc: 0.9611 - val_loss: 0.2274 - val_acc: 0.9097\n",
      "Epoch 152/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1092 - acc: 0.9518Epoch 00151: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1063 - acc: 0.9540 - val_loss: 0.2373 - val_acc: 0.8989\n",
      "Epoch 153/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1120 - acc: 0.9502Epoch 00152: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1108 - acc: 0.9511 - val_loss: 0.2722 - val_acc: 0.9011\n",
      "Epoch 154/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.1413 - acc: 0.9412Epoch 00153: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1347 - acc: 0.9428 - val_loss: 0.2638 - val_acc: 0.9140\n",
      "Epoch 155/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1277 - acc: 0.9478Epoch 00154: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1270 - acc: 0.9481 - val_loss: 0.2812 - val_acc: 0.9140\n",
      "Epoch 156/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1014 - acc: 0.9564Epoch 00155: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1016 - acc: 0.9556 - val_loss: 0.2853 - val_acc: 0.9097\n",
      "Epoch 157/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1121 - acc: 0.9525Epoch 00156: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1118 - acc: 0.9518 - val_loss: 0.2475 - val_acc: 0.9161\n",
      "Epoch 158/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1044 - acc: 0.9477Epoch 00157: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1037 - acc: 0.9479 - val_loss: 0.2750 - val_acc: 0.9161\n",
      "Epoch 159/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1017 - acc: 0.9560Epoch 00158: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1047 - acc: 0.9552 - val_loss: 0.2428 - val_acc: 0.9183\n",
      "Epoch 160/200\n",
      " 8/21 [=========>....................] - ETA: 1s - loss: 0.1005 - acc: 0.9629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.718502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.360501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1077 - acc: 0.9570Epoch 00159: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1111 - acc: 0.9561 - val_loss: 0.2659 - val_acc: 0.9118\n",
      "Epoch 161/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1112 - acc: 0.9545Epoch 00160: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1116 - acc: 0.9545 - val_loss: 0.2597 - val_acc: 0.9204\n",
      "Epoch 162/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1261 - acc: 0.9477Epoch 00161: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1256 - acc: 0.9487 - val_loss: 0.2684 - val_acc: 0.9054\n",
      "Epoch 163/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1363 - acc: 0.9427Epoch 00162: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1370 - acc: 0.9431 - val_loss: 0.2782 - val_acc: 0.9183\n",
      "Epoch 164/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1178 - acc: 0.9535Epoch 00163: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1206 - acc: 0.9534 - val_loss: 0.3158 - val_acc: 0.9118\n",
      "Epoch 165/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1431 - acc: 0.9384Epoch 00164: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1432 - acc: 0.9386 - val_loss: 0.3443 - val_acc: 0.9011\n",
      "Epoch 166/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1469 - acc: 0.9358Epoch 00165: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1458 - acc: 0.9366 - val_loss: 0.2983 - val_acc: 0.9118\n",
      "Epoch 167/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1221 - acc: 0.9460Epoch 00166: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1220 - acc: 0.9471 - val_loss: 0.2967 - val_acc: 0.9140\n",
      "Epoch 168/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1055 - acc: 0.9545Epoch 00167: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1066 - acc: 0.9559 - val_loss: 0.2681 - val_acc: 0.9161\n",
      "Epoch 169/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1065 - acc: 0.9521Epoch 00168: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1062 - acc: 0.9529 - val_loss: 0.3099 - val_acc: 0.9011\n",
      "Epoch 170/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.0985 - acc: 0.9563Epoch 00169: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1034 - acc: 0.9516 - val_loss: 0.2566 - val_acc: 0.9140\n",
      "Epoch 171/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1023 - acc: 0.9540Epoch 00170: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1058 - acc: 0.9526 - val_loss: 0.2580 - val_acc: 0.9161\n",
      "Epoch 172/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1029 - acc: 0.9526Epoch 00171: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1009 - acc: 0.9534 - val_loss: 0.3336 - val_acc: 0.8882\n",
      "Epoch 173/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1243 - acc: 0.9484- ETA: 0s - loss: 0.1266 - aEpoch 00172: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1238 - acc: 0.9501 - val_loss: 0.2745 - val_acc: 0.9054\n",
      "Epoch 174/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1248 - acc: 0.9493Epoch 00173: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1297 - acc: 0.9459 - val_loss: 0.2809 - val_acc: 0.9097\n",
      "Epoch 175/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1178 - acc: 0.9505Epoch 00174: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1215 - acc: 0.9471 - val_loss: 0.2790 - val_acc: 0.9204\n",
      "Epoch 176/200\n",
      "19/21 [=========================>....] - ETA: 0s - loss: 0.1126 - acc: 0.9463- ETA: 0s - loss: 0.1073 - acEpoch 00175: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1193 - acc: 0.9411 - val_loss: 0.2173 - val_acc: 0.9269\n",
      "Epoch 177/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1085 - acc: 0.9592Epoch 00176: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1113 - acc: 0.9582 - val_loss: 0.2597 - val_acc: 0.9032\n",
      "Epoch 178/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.1171 - acc: 0.9513Epoch 00177: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1249 - acc: 0.9472 - val_loss: 0.2463 - val_acc: 0.9118\n",
      "Epoch 179/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1082 - acc: 0.9516Epoch 00178: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1094 - acc: 0.9510 - val_loss: 0.3128 - val_acc: 0.9140\n",
      "Epoch 180/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1069 - acc: 0.9469Epoch 00179: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1075 - acc: 0.9481 - val_loss: 0.3328 - val_acc: 0.8989\n",
      "Epoch 181/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1060 - acc: 0.9559Epoch 00180: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1050 - acc: 0.9565 - val_loss: 0.2894 - val_acc: 0.9247\n",
      "Epoch 182/200\n",
      " 2/21 [=>............................] - ETA: 1s - loss: 0.0877 - acc: 0.9453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.103000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1006 - acc: 0.9645Epoch 00181: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1002 - acc: 0.9646 - val_loss: 0.3246 - val_acc: 0.9118\n",
      "Epoch 183/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1112 - acc: 0.9573Epoch 00182: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1121 - acc: 0.9579 - val_loss: 0.2989 - val_acc: 0.9032\n",
      "Epoch 184/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1115 - acc: 0.9484Epoch 00183: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1133 - acc: 0.9472 - val_loss: 0.2891 - val_acc: 0.9097\n",
      "Epoch 185/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1115 - acc: 0.9473Epoch 00184: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1088 - acc: 0.9492 - val_loss: 0.2727 - val_acc: 0.9140\n",
      "Epoch 186/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0990 - acc: 0.9598Epoch 00185: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0973 - acc: 0.9609 - val_loss: 0.2782 - val_acc: 0.9161\n",
      "Epoch 187/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0915 - acc: 0.9605Epoch 00186: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0902 - acc: 0.9609 - val_loss: 0.3315 - val_acc: 0.9075\n",
      "Epoch 188/200\n",
      "11/21 [==============>...............] - ETA: 1s - loss: 0.1814 - acc: 0.9452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.314531). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.628061). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.315531). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1655 - acc: 0.9460Epoch 00187: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1665 - acc: 0.9428 - val_loss: 0.3027 - val_acc: 0.9032\n",
      "Epoch 189/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1105 - acc: 0.9542Epoch 00188: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1088 - acc: 0.9548 - val_loss: 0.3112 - val_acc: 0.9118\n",
      "Epoch 190/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0974 - acc: 0.9542Epoch 00189: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0954 - acc: 0.9563 - val_loss: 0.3078 - val_acc: 0.9075\n",
      "Epoch 191/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1098 - acc: 0.9514Epoch 00190: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1080 - acc: 0.9522 - val_loss: 0.3014 - val_acc: 0.9075\n",
      "Epoch 192/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1101 - acc: 0.9524Epoch 00191: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1086 - acc: 0.9531 - val_loss: 0.3038 - val_acc: 0.9118\n",
      "Epoch 193/200\n",
      "20/21 [==========================>...] - ETA: 0s - loss: 0.0961 - acc: 0.9566Epoch 00192: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0985 - acc: 0.9579 - val_loss: 0.3195 - val_acc: 0.9075\n",
      "Epoch 194/200\n",
      "10/21 [============>.................] - ETA: 1s - loss: 0.1045 - acc: 0.9516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.815750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.408875). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0986 - acc: 0.9600Epoch 00193: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0973 - acc: 0.9604 - val_loss: 0.2823 - val_acc: 0.9204\n",
      "Epoch 195/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1173 - acc: 0.9460Epoch 00194: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1155 - acc: 0.9459 - val_loss: 0.2987 - val_acc: 0.9011\n",
      "Epoch 196/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0957 - acc: 0.9616Epoch 00195: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0957 - acc: 0.9606 - val_loss: 0.2832 - val_acc: 0.9226\n",
      "Epoch 197/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.0875 - acc: 0.9602Epoch 00196: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.0875 - acc: 0.9591 - val_loss: 0.2876 - val_acc: 0.9183\n",
      "Epoch 198/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1108 - acc: 0.9578- ETA: 0s - loss: 0.1088 - Epoch 00197: val_loss did not improve\n",
      "22/21 [==============================] - 1s - loss: 0.1099 - acc: 0.9589 - val_loss: 0.2903 - val_acc: 0.9054\n",
      "Epoch 199/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1025 - acc: 0.9628Epoch 00198: val_loss did not improve\n",
      "22/21 [==============================] - 2s - loss: 0.1006 - acc: 0.9623 - val_loss: 0.3067 - val_acc: 0.9032\n",
      "Epoch 200/200\n",
      "21/21 [===========================>..] - ETA: 0s - loss: 0.1072 - acc: 0.9562Epoch 00199: val_loss did not improve\n",
      "22/21 [==============================] - 0s - loss: 0.1113 - acc: 0.9532 - val_loss: 0.2881 - val_acc: 0.9075\n",
      "32/74 [===========>..................] - ETA: 5sTest loss: 0.311956182525\n",
      "Test accuracy: 0.864864866476\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_initial, y_train_initial, test_size=0.05, random_state=142)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final test dataset size: {}'.format(len(X_test)))\n",
    "\n",
    "K=3\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=42).split(X_train, y_train))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range = 0.05, height_shift_range = 0.05, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "#train_tensors = X_train_new\n",
    "#train_targets = Y_train_new\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "optimizer_small = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "optimizer2 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "#res = model.fit(train_tensors, train_targets, \n",
    "#          validation_data = (X_valid, y_valid),\n",
    "#          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print(\"Fold: {}\".format(i))\n",
    "    \n",
    "    X_train_ = X_train[train_idx]\n",
    "    y_train_ = y_train[train_idx]\n",
    "    X_valid_ = X_train[valid_idx]\n",
    "    y_valid_ =  y_train[valid_idx]\n",
    "\n",
    "    datagen.fit(X_train_)\n",
    "\n",
    "    res = model.fit_generator(datagen.flow(X_train_, y_train_, batch_size=batch_size),  steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid_, y_valid_), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    \n",
    "    prediction_model = load_model('saved_models/weights.best.from_scratch.hdf5')\n",
    "    \n",
    "    test_score = prediction_model.evaluate(X_test, y_test)\n",
    " \n",
    "    print('Test loss:', test_score[0])\n",
    "    print('Test accuracy:', test_score[1])\n",
    "    \n",
    "  \n",
    "        \n",
    "#res = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "#                    steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid, y_valid), verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDoAAAQ8CAYAAABzdP6kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6HIZVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb266c3ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphs = [res]\n",
    "\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, hist in enumerate(graphs):\n",
    "    ax1 = fig.add_subplot(110 + i + 1)\n",
    "    #plt.setp([ax1], xticks=[], yticks=[])\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_ylabel('loss')\n",
    "    #ax1.set_xlabel('epoch')\n",
    "    ax1.plot(hist.history['loss'])\n",
    "    ax1.plot(hist.history['val_loss'])\n",
    "    ax1.xaxis.set_ticks(np.arange(0, epochs, epochs // 10))\n",
    "    ax1.yaxis.set_ticks(np.arange(0, 1, 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "prediction_model = load_model('saved_models/weights.best.from_scratch.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test.json'\n",
    "test_data = read_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test, y_test, ids = prepare_dataset_ignore_angle(test_data, global_min, global_max)\n",
    "X_test, y_test, ids = prepare_dataset_with_angles(test_data, ptocessing_lambda, ptocessing_angle_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = prediction_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 1)\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'is_iceberg']\n",
    "\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for prediction, item_id in zip(res, ids):\n",
    "        csv_writer.writerow([item_id, np.asscalar(prediction)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind-cv]",
   "language": "python",
   "name": "conda-env-aind-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
