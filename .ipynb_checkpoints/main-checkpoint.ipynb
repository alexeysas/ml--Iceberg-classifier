{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Capstone: Iceberg Classifier\n",
    "\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Icebergs presents threats to the ships navigation and various offshore activities. Especially, it as actual problem for the area offshore to Newfoundland and Labrador known as Iceberg Alley. The primary iceberg detection method for now is aerial reconnaissance using vessel-based monitoring data. Also, data received though satellites are widely being integrated now onto the monitoring systems greatly reduce monitoring cost. Additionally, Synthetic Aperture Radar (SAR) satellites can still monitor in various weather conditions such as clouds and fog.\n",
    "However, manual visual classification of SAR images to identify iceberg is very time-consuming process. So, C‑CORE company (https://www.c-core.ca/) has developed a computer vision system that analyzes SAR data to automatically detect and classify icebergs and vessels. Now it challenges ML community to build effective classification algorithm for their detection system [1]\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of the project is to build an algorithm which can reliably classify data to identify either it is iceberg or ship, based on given Synthetic Aperture Radar data. Also, the results are clearly measurable using prediction accuracy and it is important to have classifier with higher accuracy (ideally 100%).\n",
    "Additionally, analysis and classification SAR data is interesting problem. Even if it seems like standard image classification task it has some important differences which makes it challengeable to use pre-trained neural networks with transfer learning for the image classification such as VGG [2] or Inception [3]:\n",
    "• SAR data is not a three-channels regular image\n",
    "• Radar detected shapes are different than visually detected shapes.\n",
    "• Data set has additional incidence angle parameter of which the image was taken. So, it is additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "CORE provided dataset of satellite SAR images containing either a ship or an iceberg including 1604 training samples and 8424 test samples (5000 from them are autogenerated) Data was collected from SAR which bounces a signal off an object and records the echo, then that data is translated into an image. Two channels of image are provided: HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). See [4] for more details. Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format:\n",
    "\n",
    "Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format: \n",
    "* Id of the image \n",
    "* band1 – flatten image data of the HH channel (5625 elements, 75x75 image), each element is float value measured in dB. \n",
    "* band2 – flatten image data of the HV channel (5625 elements, 75x75 image) , each element is float value measured in dB. \n",
    "* inc_angle - the incidence angle of which the image was taken \n",
    "* is_iceberg – classification label of the image. 1 is for iceberg, 0 for ship.\n",
    "\n",
    "Training dataset the only dataset which has labels assigned – so it will be used for training and validation. Test dataset does not have labels and will be used for the model evaluation.\n",
    "For the model features we are going to use band1, band2 and inc_angle data as features and is_iceberg field as labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to load data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to find global dataset characteristics (signal strength min/max, angle min/max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_signal_minmax(data):\n",
    "    mins = [min(min(item['band_1']), min(item['band_2'])) for item in data ]\n",
    "    maxes = [max(max(item['band_1']), max(item['band_2'])) for item in data ]\n",
    "\n",
    "    global_min = min(mins)\n",
    "    global_max = max(maxes)\n",
    "    \n",
    "    return global_min, global_max\n",
    "\n",
    "def find_angle_minmax(data):\n",
    "    global_min  = min( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'  ])\n",
    "    global_max = max( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'])\n",
    "  \n",
    "    return global_min, global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to extract and display 75*75 image from raw SAR JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_image(data_item, key, image_size = 75):\n",
    "    image = np.array(data_item[key])\n",
    "    image = image.reshape(image_size, image_size)\n",
    "    return image\n",
    "    \n",
    "def extract_images(data_item, image_size = 75):\n",
    "    hh_image = extract_image(data_item, 'band_1', image_size)\n",
    "    hv_image = extract_image(data_item, 'band_2', image_size)\n",
    "    return hh_image, hv_image\n",
    "     \n",
    "def display_image(image, cmap='gray'):\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image pre-processing and normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_min_max_scale_sar_image(image, global_min, global_max):\n",
    "    image = (image - global_min) / (global_max - global_min)\n",
    "    return image\n",
    "\n",
    "def local_min_max_scale_sar_image(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    \n",
    "    image = (image - img_min) / (img_max - img_min)\n",
    "    return image\n",
    "\n",
    "def local_standard_scale_sar_image(image):\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    \n",
    "    image = (image - np.ones_like(image) * img_mean) / img_std;\n",
    "    \n",
    "    return image\n",
    "\n",
    "def flatten_image(image):\n",
    "    image = image.flatten()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 2-channel Images for CNN from HH and HV SAR channels ignoring angle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_ignore_angles(data, process_func):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        hh_image, hv_image = extract_images(item)\n",
    "        hh_image = process_func(hh_image)\n",
    "        hv_image = process_func(hv_image)\n",
    "        \n",
    "        image = np.dstack((hh_image, hv_image))\n",
    "        X.append(image)\n",
    "        if 'is_iceberg' in item.keys():\n",
    "            labels.append(item['is_iceberg'])\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            \n",
    "        ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3-channel Images for CNN from HH and HV SAR channels + angle data as separate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image)\n",
    "            hv_image = process_func(hv_image)\n",
    "            angle_layer = np.ones_like(hh_image) * angle_processing(angle)\n",
    "            image = np.dstack((hh_image, hv_image, angle_layer))\n",
    "            X.append(image)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create flat features for simple classifiers algoirithms like Logistic Regression, Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_flat_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image).flatten()\n",
    "            hv_image = process_func(hv_image).flatten()\n",
    "            angle_layer = angle_processing(angle)\n",
    "            x_item = np.concatenate((hh_image, hv_image, [angle_layer]))\n",
    "            X.append(x_item)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_image_part(data, margin):\n",
    "    return data[:, margin : -margin, margin : - margin, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1604\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train.json'\n",
    "train_data = read_data(train_file)\n",
    "print('Training dataset size: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum incidence angle = 24.7546, Maximum incidence angle = 45.9375\n",
      "Minimum signal strength (dB)= -45.655499, Maximum signal strength (dB) = 34.574917\n"
     ]
    }
   ],
   "source": [
    "image_size = 75\n",
    "\n",
    "angle_min, angle_max = find_angle_minmax(train_data)\n",
    "global_min, global_max = find_signal_minmax(train_data)\n",
    "\n",
    "print(\"Minimum incidence angle = {}, Maximum incidence angle = {}\".format(angle_min, angle_max))\n",
    "print(\"Minimum signal strength (dB)= {}, Maximum signal strength (dB) = {}\".format(global_min, global_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and run image workflow to  extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    image = cv2.bilateralFilter(image.astype(np.float32), 5, 80, 80)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "#X_train_initial, y_train_initial, _ = prepare_dataset_ignore_angles(train_data, ptocessing_lambda)\n",
    "X_train_initial, y_train_initial, _ = prepare_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "#X_train, y_train, _ = prepare_dataset_with_angle(train_data, global_min, global_max, angle_min, angle_max)\n",
    "print('Training dataset size: {}'.format(len(X_train_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_channels(data, index):\n",
    "    hh_channel = data[index, :, :, 0]\n",
    "    hv_channel = data[index, :, :, 1]\n",
    "    return hh_channel, hv_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK9OsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23613b98fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnV2sZmd13/9rZmxjDx8z4288bk0UBOIGk6IURFRRiCua\nILiJEDSt0giJm7QiaqoAuahaqZGSmyRcVEiIkFKJBihJVIQiUkRAbaXKBQptgg2FUFvY2J7xeIYP\nY8Az5+nFedc7v/PO+s+7z3y8wzl7/SXLz9mz9/O59/v813rWR4wx1Gg05oUD17oDjUZj8+gPv9GY\nIfrDbzRmiP7wG40Zoj/8RmOG6A+/0Zgh+sNvNGaIy/rwI+INEfG1iPhGRLz7SnWq0WhcXcSlGvBE\nxEFJ/1fSfZIekfR5SW8bYzxw5brXaDSuBg5dxrM/K+kbY4xvSlJEfETSmyXZD/+GG24Yhw8fvmil\nBw6cJyH8Ucry2bNny3/nddZx8ODB8npErK1na2trWT537twFfWUd7jrrPnToUHkP+8j7s31ec3UT\n7n6Oh9dZ5vjzuusr55NgO24+iXX1uPHwOc6t66Nrn2v77LPPXvRewvXluuuuK/vCPhLVmrt3j/OZ\nfWUdW1tbGmPULyZwOR/+XZK+hb8fkfR3L/bA4cOHdd9990nyi/mc5zxnWebAsnzmzJnltR//+MfL\n8okTJ3a0k3j+85+/LD/3uc9dlrkIrOfUqVPL8g9/+MNl+amnnrpgPNdff/2yzPFw4Xn96NGj5bPP\ne97zlmWOOfvF/vEF473u4+WLx/Gwj5yL06dPX3AP/51zyDLb/973vrcsP/nkk8vyD37wA1VgPUTe\nz/GwLzfddNOyfNttty3LXH/W/cwzzyzLXBeOOd+jp59+enmNH6HbVPie3XrrrcvysWPHluUjR46U\n4+D6Zrvs63e/+91l+YknnliWT548uSzn2vK5i+GqK/ci4h0R8YWI+MKPfvSjq91co9GYgMvZ8R+V\ndDf+Pr64tgNjjPdLer8kHTt2bOSvJ+kLdx/+YvEXNX8Vb7jhhuU1/hLy199RYP6y8lecO6fbIRPs\nN+vgDs4dite///3vL8vcCVz7uYveeOON5b0cM8dGsI+cZ4LjrNpyFJW7vKOjfNZR84pq83rF/KSd\nc+vGwM2G/eV7VomJnCv2j2Pgu8gy+0UGS1SiI+tx42Q7rDvHNnVzvZwd//OSXhwRL4qI6yW9VdIn\nLqO+RqOxIVzyjj/GOBsR/0zSX0g6KOmDY4yvXLGeNRqNq4bLofoaY/y5pD+fev/W1taSkpHSUunj\nlFRJsXgvqQ6vkxqRMjltM0GlTkU7SQFJ+9ypAimoo/28znlJKs85oSLQ0Wu2SRGA97jxs/28h/NA\nZRnbIcWstNQczyrYR9ZTKffcCQPnkFTfUWpS/UohxvqcSMfxvOAFL1iW+V5SKXzLLbcsy5yjSuyl\ngpLzw/ePa5FjpmL1YmjLvUZjhugPv9GYIS6L6l8KkqqRXjk6TjqUdIdUixSItMvV4TS5TiNOCpba\nVNI4d45PCkzttaPApP2kjPks22R9rINtOoMgzrOj5qwn54i0k1TSjWeKVp9w9+c4uD6k8c7Yi310\n88+1ZbmyneCakGpzXnj9scceW5Y5n+wXbTo4Do4vwfUk2GaWnVHZKnrHbzRmiP7wG40ZYqNUf2tr\na0m3STuddrjSZvJeUi0+R7rktNe8n2XSLlK8pIOOdpOO8R72dwoNq8QOihSEo7rOn4F1s8wTkcrI\n5jvf+c4F11b75Yx8KAJQC+207dWcOpNeZzRFkY7tsG53wlEZmPGkh5p54vHHH1+WaabMd4vmtjTf\nPX78+AX95btCUdC9c+49d+gdv9GYIfrDbzRmiI1S/XPnzi3t60kpSatIOyvXREcXK7vl1evO1dG5\nzjoNboJUk23yOdpWu9MDzkXliUe66Gi8OyVwxjxTbNgrt1zOP8fpxBh32sJ5cZr3nF/+O0VE50Hn\nTjjcdecrkHAGWe46+8Xrle+HtNMoK+t07tScn0oEcicAq+gdv9GYITZ+jl8FGuCvrPNmyl9C/qJR\niVedaa62w/IUZVQVuMIptFx8AZa5yznbhepZt8uxfe7Ubocg3I5S+Ym73ZF18Dk3t84GwcUyyDXn\nzurMpJ0S1ZnbOpPcZJwuEIdTtHJsfIfp60+PTO7ybCvX0cU6cCw0mZXzwFxF7/iNxgzRH36jMUNs\nlOpHxJI2Urnk4pJVJp5OFCAcNXWKD0clierc2Z2dOjo4RRyoPNiolOMYSCMdvXaeao4yV7TTeR4S\nLpaiU+I6hW5V/xRzYKdQdZ6PnHOKUvmss+1wc0iwHYYEY3guzldlssv+OSVyNc422W00Ghb94Tca\nM8TGTXaTQjnTQ5YrbbPTgJMOV2exq/dTZCB9JH2qNMxOe066Rpro7AWcR1ZlU8B+MLCDOxemqTHH\n5sbsaH9lJs2+OJGGGms3F44ycx3Xrbmz4+C75ewF3Pl61unEQjcGZyPAuSC9d+9i9teN052GMbz2\nFPSO32jMEP3hNxozxMa1+pUmdJ3JpnSeMjrzWmeE4sJLU3vuYreRbmX7LoAHr0+htC6hBftVxRl0\nZUc7nTeZo72VCOJMZknjSWOdaFBRU8mbIef9pOsudLkzB55icENk+06r7sQ7l6WJfaniKUp1cBHn\nPcn5YXj5PDGYmhJv7Y4fER+MiBMR8de4diwiPh0RX1/8/+jF6mg0Gj9ZmEL1/72kN6xce7ekz4wx\nXizpM4u/G43GHsFaqj/G+K8Rcc/K5TdLeu2i/CFJn5P0rikNJg1yQSScPXtS2XW0dPUep8kmfSNN\ndfHqKmMdl5nH5bpzduNE5StASs2xkQLyHmr16alH++9Ke75azrlwBj4uniENVXg/59Z5B1bhsynG\ncN5cSHVHd51hVxWO3VH9SvyUvK2+C4fN65UHqfOw5FpwnjNYigtwsopLVe7dPsbIiIKPS7r9Eutp\nNBrXAJet1R/bP69Wo8CkmbsND9RoNK4OLlWr/0RE3DnGeCwi7pR0wt3IpJmHDx8eFX10tH+d3Tbv\nJe0idXK0m5SINJVUztliJ1wMN2rSnWhCww7SPtZTnWQQTnvuUjk7UccZ+eRcUxRyiSqdjwX7RZdi\ntuMSpeb4nYhAuL64EwmC46/CW3P8zt/Ctelcx6dkFUo4kYZjTtq/7p1NXOqO/wlJv7Io/4qk/3yJ\n9TQajWuAKcd5fyzpf0h6SUQ8EhFvl/Q7ku6LiK9L+vnF341GY49gilb/beafXr/bxsYYSyozxS20\nKk9xhXTiQhVpRfK52tfRTj7nTgZ4fUrO80qDz/khRbz55pt3VbczpnF0ONtyEXLYF2dY5fruKDu1\n2bl2FBE4t84Vl21ynd09lWjEe6cYjTmDLBd23Z12VBGq1tF7abo2P9Emu43GDNEffqMxQ2zUVn+M\nsaSSzr3TUbAsk3aROjlK5dxi3dGiEzXyWWqgOQYnXlSJN1fvcTnf8x72g1pid3rhjElIewk+W4UD\nX2dgs1p2AUEpRjg790rbz2tu/Z2PReVvsdqvqi+Orrs6eI9b5yl+BlX7BOl9Fbnpamv1G43GHkZ/\n+I3GDLFRqn/w4MGl4YqzmyZNIlLb61xemYTQ2Vk7quX6UrlAOrrs7OCp1XdaYMJFj6n65GzIKQ6w\nL7yHdNwZQuX1KQFOqW12ueqdGOWofK6jmxNHqd2pAuGMlqqoRy7wJuFOhlz0IIo9lXjHa3y3nIiU\n898ReBqNhkV/+I3GDLFRqn/o0KGl0QkNNUhNSXGoQU84KkM6RkMRR/tJKV0Odcatz764pJ68fvTo\n+bgkpNR0l3VGPpVtv7MP53UnUjjaSyrrgnbmnDpa7pKAci5cHnqX5qqKauTEst2eDDnjq8pFllTc\nBXJ1BlEcmzNOc4lIU3xx76pLlLpb9I7faMwQ/eE3GjPExuPqJw10VMZRsyzz3ykKkII6jamzZ3b2\n/ERlW872qWGmGEP3W95PccS5i2a/KqOaVZBeOzdS56Lrxp/PugCjjl4TTsPNNs+cOVOOI+Ei11SR\nc1bbcWKPEx9yzKzPiStcT2dww/fGrX/l0usMvJybd47nigXbbDQa+w8b3fGlOnywM18kcudwQQlc\n4sUpCRedYowsInedjG0m+RznUzKssB4qN6tAIM5TyynLqFBcl6Vm9TpR7fguJz3nyoWX5rxwJ+Ta\ncVfMXdd5HjolmjvTdyyrMrF27NApggmX7ca9l5zHHBPfCRd8plqLPsdvNBoW/eE3GjPExr3zkuK5\nJJOkw6SAaTLrsrGQdjmzWhd8wmXSoSLp2LFjFzznlF6k2lUMPcl7kFVUknTZhdeectbvYvu5uHg5\nL44+OsWpSw7JeyrPMqk2j3XiCutzmWcojvEeKgwrkWVK9h5nC+DCYTtzX65R9pFzzjVx3pHOBNyh\nd/xGY4boD7/RmCE2TvWT+kzJec7z8CpppTvTddSQlMrRJ5ZZf9IqmuM6OubChbu4gI6mZ/tTcrK7\nuHRT4uK5Z3POnfbaeQe6kxEXl46oxCeKDgQpPU9J3MmD09ST9mffuQ4UF1xfWTfH7DI5uXWsTq9Y\n5lrRIzXn6Eomzbw7Ij4bEQ9ExFci4p2L6504s9HYo5hC9c9K+o0xxsskvUrSr0XEy9SJMxuNPYsp\n4bUfk/TYovy9iHhQ0l26hMSZW1tbpScY6bij5kkTXYACxiJzgSVcPnuKFERl/MF2XGw7aoSnBF9w\nCRSzHheKmnAnBpwLiinOaMdlFaquUYxwVN9lqeGYnXdmlUmIFJl1O9GpyoyziipAisuM405A2C/e\n404vKs9T6fzplTsl4jqfOHE+gVX20a3rKnal3FtkzX2FpPvViTMbjT2LyR9+RDxX0p9I+vUxxnf5\nbxdLnNlJMxuNnzxM0upHxHXa/ug/PMb408XlSYkzmTTzxhtvHElFpmT+qAInkLqR6pNGke6QOjtq\n5gxxqM3N9p1BjKN0zlafcEkm87rL5c76OC+kkXz2qaeeWpbTIGm1j0SKJs6XwJWdTwQxxYMwaW/+\nf7VugmKMo93MPOTs5pNKuySkpNouG44L+MF3xAVlSTzxxBPlcy50e67VldTqh6Q/lPTgGOP38E+d\nOLPR2KOYsuO/RtI/kfRXEfHlxbXf0naizI8tkmg+LOktV6eLjUbjSmOKVv+/Swrzz7tKnOkMeJzr\nZOVeSY056ZKjd05jTg27C7pQZc2pXChX+8WxkV6ybkf7WWeKHVPECxfbjcYffPbJJ59clp1LbRUU\nwgWzIFzY6yma/yrUNME14b/zvXG+F4TLcFOJPS6pKeFOAXabVSfFUYouLiw628l3ZMoJkNQmu43G\nLNEffqMxQ2w8As+yYWP84gxb8n5q8glnKMN2KBq4WGikUpUxC23CKV5MiadH7fiUJJv5LPvKzDRO\nXHHhrV2kIXc6kOviqKtLFDklyaYzoKrcbtedOkg+w5A7VXDvSPbLufm6BJ/OzduJSc6lOO/nOrg6\nKpGGJzcXQ+/4jcYM0R9+ozFDbJzqJ/VzbrTOdTOpjws26DTDU4xcSHtJO0kHaUSSIAWkCOJopDPm\ncO0nxaVNNkURUkR3SuCMk9hflunqmXCimNNYu0CRFFk4n9WYpfPz6EJNu9MIZ0Dj3IV5PfvOuZoi\nRroMO47Su8Cjla29c2F37t9T0Dt+ozFD9IffaMwQG0+amVRySiSdKoOMM1ohvXTa0yqijuSNcipt\nsosl7+L9O/dL3uOCM6a9Nv+dZWcQQ+29c4XlCcedd965LPNEIu8nFXVGUC4IKe8npefpCK9XwSmd\noRThThKcFt5dr2zeXRQfZgByJzlOC8/3gvQ+23I+CS7qU9bhsjKtonf8RmOG6A+/0ZghNkr1Dxw4\nsHR1dTHGSUcrCkja5ZJJOjGCcJrf6iSB97NNF3XH2YqTPnJsbJMGOglSbVJ0Z7QyhdLeddddyzL9\nCajVr2K8uwg97oSBdNjlkHca6ayf88P1d/4eVR2SF/Uqeuw07Vw3J3byxMIZ+bDMtnIunBEWwbnI\n5zqFVqPRsNi4ci93F+edxB2iOid1yjengCL47Gq/EtzxKk9B1l2deUteWelYBueiyvzCHdkpurhr\ncrfgbsa6Wb711luX5SocNHc5js2ddXMNnWl05QW4Wn+Ob0oSTsJ5/k3JdpNlF89vikeks6mY8r5U\nymCuoWNWfY7faDTWoj/8RmOG2LjJblIS0mhHJatYc47GETzfJE0ijSLtYpntV0ofnpE7LzCXscaZ\nuBJU7iXFd3H7XCw4Ny9T8rlX9JkmvaSaXMPHH398Web4XZAT57VIVGvuFKEuzh+v03ZgXSAQzjPX\n3Ik6Lqw1lbHu3Tl9+vQFfXdmys40Oetznoyr6B2/0Zgh+sNvNGaIjVP9pDCOpjmtZUXBCGqyWUdF\nnVfhzFqrc1IXQINw3mzOO4/1VOf0Loag89piH6m9d/H6OHdEiiZ8jrSf59XujJx9cZTdafirul3W\nIydeOG03RaPqfJ1ijBORKDqyXxTpOP8UKdnHylOQIhfXh3VXoqM7uVrFlPDaz4mI/xkR/3uRNPPf\nLK6/KCLuj4hvRMRHI6KOathoNH7iMIXq/0jS68YYL5d0r6Q3RMSrJP2upN8fY/y0pNOS3n71utlo\nNK4kpoTXHpKSx1y3+G9Iep2kf7S4/iFJ/1rS+y5W19bW1pISOYMTokqg6bS0vNeZhjoxwmWEIbKP\nrj5SYCcOkF5OMT5JijslyASvU1xwzzpqXuWzJ+0lXCAKR+mdyFLFGZTqpKEuIaqbc0d93Tym5p1j\ndkE7WIdrh5p8F3OQlD3Hz/lhotapSTHXYZJyLyIOLpJpnJD0aUl/I+nMGCN7/4i2M+g2Go09gEkf\n/hjj3BjjXknHJf2spJdObYBJM6/Ur1Wj0bg87EqrP8Y4ExGflfRqSUci4tBi1z8u6VHzzDJp5pEj\nR0ZSMmpBp3g8JZz2nhp4F8PPUV2WnRY666yoqLSTdpIm8rozZnF24UlBnTaeFJVJMF0YbcLFyGM5\nx8ExsOzCYjuxywUOITgXaYjlTiBoqFXZ+EvemIfXKaYlreccOpHGxbxjX9gmRRo+W4mmLjPQupDi\nV1Krf2tEHFmUb5R0n6QHJX1W0i8tbuukmY3GHsKUHf9OSR+KiIPa/qH42BjjkxHxgKSPRMS/lfQl\nbWfUbTQaewBTtPr/R9Iriuvf1La8PxnPPvusTp48KcmHV3aJENe5S5KaEc44h8+Sgjlta9JXZ8vv\nDGtI02hMROMP3lOFo2Y/SEt5neO/5ZZblmWXvcdp6llnzgUpOsdA2u3CktOAxRk8UXzgXOR1F5+P\ncIY6Luw656UKrjLl1MWJbnzWiV18/yka5T1cH/67W8+psfYSbbLbaMwQ/eE3GjPERm31n3nmGX35\ny1+W5PPJu3hxVbJMUp0q2ePFQPpE2k3KXsWrc8kxHb13VLOid9J6W3XSSN5LP4QXvvCFy7KLKOTs\n+akVzgw+Dz/8cDked5LC+jhmGqK4DDOc36TDzp2WZRd2250kuESUeY8LY024UwLOC0UjFzGK4ku+\n5+70iO1U0XqumFa/0WjsP/SH32jMEBul+mOMJYWiRthlviG9T5roNMOk+qR3zg9gSgDDym7fUbop\nmVRcZBxStipijzPaII2//fbbl+W77767vIciA69znkk7M2imCx1NOH8HF3XIUXOuVxVNxoVU53vD\nd8GJRkQldpCKV7b0q/U5sXNKmG4i59edurB99jvLU4Nu9o7faMwQ/eE3GjPERqn+9ddfr+PHj0va\nSeNIH6kRJt2paC/haFwVJ/5iIB2tjHkcLecYaDdPjbDT5Loc8knNKX4cPXp0WXY57lnmfFKrznoo\nAlQx+alVd1TfZYxx/hGcR2q+GXgy56JKpLlaHw1lpuRYcO7XlQafFJ0uzy7rEsfD/nJuOS9sM/vr\nTiDcaUhT/UajsRb94TcaM8RGqf51112nO+64Q9JO+uiMdkiHUzRwNI50kFTXJc0k1iVQlM7TNEeL\nSc0yr720czxPPfXUssyxkfaSMiaVc3HnnaGQKzu3UIpd7GPGymdKLM65O5lhme1wzfks6T3LOaeO\n6nKdeY8bmzMaqwyoXFJT0nL6RLg8ARyzO5GiCJZwUX9cotJOodVoNNaiP/xGY4bYKNWPiCVVcXne\nSV8qLTgpFSkotdeEEwdI+0gv12XaZZsucy/htNfpnix5V8+kgJwHghp2UnSXRdelsCKtffTR84GU\ncl5c8FJS7Sk53EmT2V+XdTevO5+N3WaxrQxeVuvP/pJSu6Ce3/72t8t+ccxspzqxWUWKFc73w0WO\nyjE4cWIVveM3GjPExjPp5K8xf6F5Xu8CXeT9UxIycifkjuuCb7BNKnKqIBIuqwt/oam4Yr9crnje\nQxuA7K9TqJFBMGklf/XZL97P3Yp9eeihh5blnEdnI8F2uGtz/jnPLs89x18FmpgS0tqxEtdfgn3P\nsbogK5x/p0R0tiAuzh7fyxyfyy51pdA7fqMxQ/SH32jMEBul+ltbW6W3HKkkvcaqrCmkq1Piorkw\n3u7c9dZbb12W2ValPHQmu07R5ZRUpK+kwBlQg+aozjSVdNCdHZ86dWpZJjV15tOpgKOoQaUXx+ls\nGlg354tn1xT1WE+2S4Wqy5LDPjoa7ah2NSYXOt2JosSU9Xf9qt7pdeHXL9amw+Qdf5FN50sR8cnF\n3500s9HYo9gN1X+ntuPpJzppZqOxRzGJ6kfEcUm/KOm3Jf2L2OZYl5Q0M6mcC3XsAjrk/e7s3Hm4\nOY+4ygtN2kkZK5NM9pvacKftJVzGGpcRJ+mwE39YB+eFY6b23mmhSe+rOa20zpL3PHRzsZtMNtJ5\nUcOdkVNccHELXeAKZxpeecdxnnmvC9denRKs3sN3uzo1cWKEC8WeouiVPsf/A0m/KSlbulmdNLPR\n2LOYkkLrjZJOjDG+eCkNMGnmbhUQjUbj6mAK1X+NpDdFxC9Ieo6k50t6ry4haeaNN944kqqQ0pG+\nsFxRM6cBJwVzOdRZdnnrXTmptAud7LLBkKaS0nH81NQzQEZS/Cmhlp0m22nYXVaXyvSVY3MGOe60\nxWnPnVEKr+f9FKmqMOuS17a7kx+KI1VSTq6DEyOmeD4SXHNnwJZz7bzt2A7fhUoUvhjW7vhjjPeM\nMY6PMe6R9FZJfznG+GV10sxGY8/icgx43qVtRd83tC3zd9LMRmOPYFcGPGOMz0n63KK866SZBw8e\nXNJX52VEqlIZNrgEly4nuQuX7TSvzssr6aMzmiG9dPHSqJ13VJ+GQtUJiDM84thI6V1QDsL5AiTF\ndnboDi50NMUBUnaXQLTySmTdFUWWvKgzRbzIOtknZ7TkjJOcYRGpuet7Xuc6VxlzVpHr0pl0Go2G\nRX/4jcYMsVFb/YMHDy4pnouRRpC2JJVyWlUX0rkyAlotkzLzOpF0jNSRcHW75IwuySfpY1JQp1V2\n2V6c0Qznk6KR0/xXVN9lpnFiBI182D7t7xmUpXK1pu8B583F/HN28LyH9L6aC+dmzfHzHo6fhkV8\n/yjqudiBeZ1r69awinPYVL/RaFj0h99ozBAbj7mXlNTFgluXK54acJeZxcUzc8Y/1LCuC2tMKsrn\nXL9ddCGXfLOi+pwfl5DSabWdCML5cjH/kqaTrjtK7wyvCNbNdXTiQ86FM84hRSbFdeKaOxGqxIsp\nGnu3ngRPadimM2CrMkZVIddX++Lad+gdv9GYIfrDbzRmiI1S/QMHDiypKqmpo8kVrXLGMc5og3U4\nwwrSZ9ZZJVZ09M65mZKOOyMj0jcauVQGJy6o5JRQ326cLiBolp1BEOECfDp6y3u4dlXf3YmNo7pu\njZw4VBliuTrcKYGzvXch3d37n+OfcjJE0aUz6TQajbXoD7/RmCE2SvXHGEtK4mzyCdKaKoGiM8Lg\nPS4LiaPjpGmVBtVRQLpxsj5qZJ32ngYivJ500NG+KVF/CGrynaFHdZLBOXEGOS5OPuEo67qEn47S\nO6Mll0OeINWuRBknFjn/iCmGZc51l+JdrrVzbeZ4qnfBGZitonf8RmOG6A+/0ZghNp5Cq6Lsjp6Q\nyiatItV0ttfUcDrtPWkibcWdkUfSQdJOGqEQjvY6Cuoi3FRU1xk7cT6ryCySP/lg2eWQT0xxf+a8\nOO29M/ipIvw4jb1zl+XcuhRq7v3L685QiGIP++XELhe008Xqryh7lYtCqml/U/1Go2HRH36jMUNs\nPIVWZaBBKksqTzqYkVcclSFFJZwxBbXwbMcZq2S7ztiFNI5jcFp4pxEnHa18DpyW2qWQYh0uJr7T\nmlc51x1FdScpHJsTozimygWVa+jECMKl2aoi+kg73WizLxQjOLcsM0ipc6O9+eabyzY5L5V7MSMN\nOZGC48+5nWrI0zt+ozFDbPwcv9rxnXKnOvfmryl3AsL9mvJ+p2hyXmtVn6j0cWfRztaAO4o7G85d\nhLsJ++d2UAZ8cJ53U3bLLDs24WwxnFmtYzbO9Dh3Orf7EhwP58itizMfzjV1QVNc3W6HJrNy7yvf\ni3y/nLedSzCaY5gaiGNqCq2HJH1P0jlJZ8cYr4yIY5I+KukeSQ9JessY4/SkVhuNxjXFbqj+3x9j\n3DvGeOXi73dL+swY48WSPrP4u9Fo7AFcDtV/s6TXLsof0nbY7Xdd7IGtra0lVXLnyE4ZVlGgKc85\nLzTSN8Z0I1WqbAOoXCJddeKKU8axHmdfkMojVwfbIXV1eeungNSUQSSqdkhvnYLQKU6dmFJ5PDra\n62IrOlsHJyZU9bssRU5Z6DwMaSPCvriYh+uUdJXHqORDmjtM3fGHpP8SEV+MiHcsrt0+xnhsUX5c\n0u27arnRaFwzTN3xf26M8WhE3Cbp0xHxVf7jGGNERPkTtfiheIfkf60ajcZmMenDH2M8uvj/iYj4\nM21n0HkiIu4cYzwWEXdKOmGe3ZE0Mz2RnPmoMxlN6uW880gdSaNcCOpTp05dULe001Oq0vw7DbR7\nzsWCc+fhFa10lNZlI3KeYs5M1lHQ7MsUs1932kHRgXDn0ZU23QUZYV84b07scfYNFVzMPXcaUmXD\nkfw8sy9n9clAAAAQH0lEQVTVSdeUdSPy3b5i4bUj4nBEPC/Lkv6BpL+W9AltJ8uUOmlmo7GnMGXH\nv13Sny1+SQ5J+o9jjE9FxOclfSwi3i7pYUlvuXrdbDQaVxJrP/xFcsyXF9dPSXr9bho7d+7cDjPH\nhAsBXWlqp4TiJnVifdTek+pRI+pOG5J2sj4napDGOQ0372f7VYAI523n6KozJXamses85dyJBUF6\nTbHHiW4uLh/7kvdwPlnmOJ044sx3ef+6jEBsk2D7LrafMyByBmSVR6Y71XEi5RS0yW6jMUP0h99o\nzBAb985LWkuaSprotPNJpdZRpNXrzvjC2bwTNLJJTa0zCKoy4KzWPSWH/enT562ec15YHw1CWJ8z\n+KBW3dFroppfroMLcsI+UrygGONECkfHsy0Xq5DgejpxzPkQVMFC2D8nRhDOmMz5ELhMQjl3nLcp\nAUfyenvnNRoNi/7wG40ZYuMx9xKkKY6yVhTQ2VCzDucK67SwBO3TaYiSFIxUi9prYrehnknvq6Ak\nLq+6o5fsI+klx++MXCo6zjmkcQppN7X3bJNzeObMmbX9rZJJViG/V59zpzTufXEiWPbLZUly4pLL\n6uTcwp1/Qq4R54Hz7MKoZ7+a6jcaDYv+8BuNGWLjEXiSyrjc8i7JZFJglzd8N9FVsi9VO6S6vD/p\nIOn6uvhnkg8B7oxPeE+2T3rHsbnEjy5ijzP4YZ0VxXX0kvNDccXZ0PN+GnKtc7Wm+DUl2abzvXAG\nX+vgxE/3nvG9cG7JFC+qEwnXVyci5HNTMipJveM3GrNEf/iNxgyxUap/4MCBJcVfR6mlWhxw2Uic\nK2QlLkiePjnaX93rklk6F1lXD1HNBY09XMBM5yvA+hxNZbnqO7Xkzt7fheAmXAQkzmPVF66tc21m\nfTw9cCcc64yGnCuwW1vnQ+GMvJxomv2ieMFyFf5c2n2wzd7xG40Zoj/8RmOGuGZUn9SMVNYZq+T9\njt655ISO9lOkoIaZdKwy1nABNpmNxWleScGdn0GlQXYZeJwBD9t0ce05z5wvtp9uzC4JpYtG5Og4\n19lFD+I92S+Ok+1QBOAaOjrufEIqUc/5hDgDHs4h+8i149xVhkp81hniVHkPJG8c5tA7fqMxQ/SH\n32jMEBul+ocOHdJtt912wXXnoujcPhPUNtPIo8qxLu2kT3yWRh5sh+XKLZd0mSClZjsuJj3FC1Lj\nLDtNbqUNXr3fnZ6Qmjo32hRN2CfOrTOCcmvIeeGas78UwZJWuxMYF42J9LpKwrna90oTPsXmnX11\nfXQ+CU7bXxniuISwletwa/UbjYbFRnf8iCizoziTyErRxjNaPueUSM5rjyamrJO7f5WemLs8d8c7\n7rhjxzgTvJ8x/9z4OY7sL+tzSjHurLzf5TLgPVN20aqvLgMQlZjOU5Fry/WqzsydQs3F4nO7qTu7\nr+Ay5lSefKtlri3H5mLxVbYmjs0RlVLwiu74EXEkIj4eEV+NiAcj4tURcSwiPh0RX1/8/+j6mhqN\nxk8CplL990r61BjjpdqOuPugOmlmo7FnsZbqR8QLJP09Sf9UksYYP5b044jYddJM6Tw9cfTSmT7m\nOW2VSFPaqbhy59hsxwXIINXnPbfccoskn9eez1XJJlef5Zkuz6NJ63LMlT2D5ENEsx0XuMOZ3lam\nv868eEqGF2cXwXFyLirvTEd1+X6wDsJ50K0Ltc17nV2CUwBSvOP76uIVcny5dm5t2e8qbuGVpPov\nknRS0h9FxJci4gOLjDqdNLPR2KOY8uEfkvQzkt43xniFpKe1QuvH9k+ZTZoZEV+IiC+4naPRaGwW\nU7T6j0h6ZIxx/+Lvj2v7w9910sybbrppZBy5KRSwCkbAcNEudLbLg+7ECNJLFxctNfjuLN6Z5jrv\nQLbjztErM+HqnHu1TDjvQEfNK80zxQLnhee095wLF+qa1zlHKTJxDAwvTvA5zqfzmuScVicFTizj\n+rvYgu5ddGf6RN7j7CUqj1HWd8Vi7o0xHpf0rYh4yeLS6yU9oE6a2WjsWUw9x//nkj4cEddL+qak\nX9X2j0YnzWw09iAmffhjjC9LemXxT7tKmnn27Fk9+eSTknZSKRf2uDJVnWLMQFQaU2kn7XP54Um7\nE6R3rO/UqVPLsjthcIYd6wxLqBkmvZ5ihMPxsB2nkWedeT/roKi1Lj7iapsuNDbFvip8N+m9E29c\niHbW52IOckzZX15zGZg4fp7qTPEO5ZiroBvOaMkZZOX9TuRbRZvsNhozRH/4jcYMsfFMOkl9SKOd\nBrPSzjrqTHrNMo1pHNV0sfiqnO+kyBwD+0qwTfbL0TfWn9cZZMIZ2zhvP1JdYjeBO5ytOukwA5G4\ngCfsO59191e0licALtS282bjmCleVuHAeY00nu+Ey5LjxD53ClQlcHVZn3hvtW5N9RuNhkV/+I3G\nDLFRqn/27NmlCyzp2BQ30qSJpFq8l9dJ7939LhAD6SDddROka84IxMXcI011Bj+VPb2zvSftc3bg\nDs5WnetShTQn7WWbFEdoq+406W79Of/ZrkvI6QyIXDhql1uedeZ1F0yEYhTnxY3fGS25eH3VO+cy\n8FRr2IE4Go2GRX/4jcYMsfEIPElJSKVozEI6VIWjdjSacHnbXXQZV2b92V9qaZ2G2Wnp2T6poaO9\nCZco0+Wbd5FunLabVJP35Dgo3jhKzX5Xhk+Sj0XnRA2+F+vad/4EjHrkjGKqsqPUPL2gVp9GRuwX\n73ciCOfr6NGjF9Tt5q0ycGuq32g0LPrDbzRmiI1S/YMHDy4p0Tr78NVyanAdXXe236RMpOmODlNr\nTSQFdckm17lZXqy/Tkyo8tOTLpIKO9dWBxcos8pOxHG6UN8uGpKjqbyf9VfZdqow0pIfJ6kz59ZF\nz6lCljtDMqeNd+vCUw1q+ynGVoFKOYd8h93JUFP9RqOxFv3hNxozxMa1+kl9XJQUF3s8y1OCR5JG\nkVJSC05KRA27i56T2laXkNFpzJ1xjqP9nIu0/3aRg5xm2Bm2OJruTkey786XgXAnM8523Il0Vax8\n55/hjHNIu0mT3fxX2X5oe+8i4Dh7etbt3K/5/vPZKnuSi1ZUnXo11W80Ghb94TcaM8Q1M+CpjDMk\nn+c7Ka6LtOPoJekQqdmU66wnxQRqZkkvSQ3dKYWzD2ffq1MDXnPaYBc5yAV4JH2l+FSddlBcYh3O\nJ2BKEFDnk0GDm+wL3xVnBOP6wmc5L2yf9+Q6UqvuEnI6UcMle3XigBNvq/q4znwXXF4Bh97xG40Z\noj/8RmOGmJJC6yWSPopLPyXpX0n6D4vr90h6SNJbxhinV5/f0dihQ7r99u2EO47Sk2JVBjJTYvC7\nPOikxi5opwtgmSAtc9FY2I4zFHLRWCojI4oCzpjEiTpO8+xOPjhfed0ZG7nIME6z7MSedf4RnFsX\nUYlzzrlgpB8+605Ksk1H0d0cupMJd8LixId17ucutVaOzblBr2JKXP2vjTHuHWPcK+nvSPqBpD9T\nJ81sNPYsdqvce72kvxljPHwpSTMjooydxx2FyrPqzJT30rzWKf3cL6A7D3dhorPsgm84810XI8/F\n/KvO19knNx7HCqYEH3HsK++fMk4XrMLtco4VVZl33G7OHdxlRnJ2HyyTrWTfpzBLx4ScObqLLUlW\nkO2zT2lDIu2c5ynmyw67lfHfKumPF+VOmtlo7FFM/vAXWXTeJOk/rf7b1KSZ/DVvNBrXDruh+v9Q\n0v8aYzyx+HvXSTMPHz48qMhIOCpbKThI70nRqcRxZpJTFHqOMmVfXAw1lzGGtNcplNjfylPOBSch\nHXQJRN1Zt6OJldLNedsRbJ/jcUpPl8CyCrvuwnJPSZRJcC64LtWGxH64YC6uPocqn/1q+znnLuAK\nA35UwVdcMtRV7Ibqv03nab7USTMbjT2LSR9+RByWdJ+kP8Xl35F0X0R8XdLPL/5uNBp7AFOTZj4t\n6eaVa6e0y6SZ586dW5pkOkrvPN6qTCFVLnlpp5aUprSOMruc4lUADGqSCVJNp2F2IbB5D081sk72\ng1SX5q0cs/M8Y784F87cM++h6MJxOtGF4pjLpOO8/KpTC46NIL12dgEUUwgnmqVoUnnMXey6M43m\n3Lr55zua72UV8luSTp8+bypTnUB1Jp1Go2HRH36jMUNs1DtvjFGaoTpvMiIpFoNmkPY6zzMX3prt\nkFY5ylZ5B1J7TQpI6ujMUV2e+SrUNSkly5w30n6C4Z3ZF3ciUZXdSYcTozjOkydPLssUr9x6VSax\nLnQ4TwzYR+ep6IzDiGyL9Npl8qniE66OgZgSly81+OtMeqXaUOpqaPUbjcY+QX/4jcYMsfHw2kk9\nHb13OccrTzFSHVIcF0ab9I7PkiaSSpPK5T2k8VPi1jmjGad9XZdVh9dInSk6OErpfAicMU/lK+DG\n4BJ7OlGL97i5y744rz6Chi3Ohp5rztOZKhy3O41wgTCcDwX77gzFqqAgLuCH84PI6+6E6oJ+Tbqr\n0WjsK/SH32jMEDGVGlyRxiJOSnpa0pMba/Ta4Rb1OPcT9so4//YY49Z1N230w5ekiPjCGOOVG230\nGqDHub+w38bZVL/RmCH6w280Zohr8eG//xq0eS3Q49xf2Ffj3LiM32g0rj2a6jcaM8RGP/yIeENE\nfC0ivhER+yYcd0TcHRGfjYgHIuIrEfHOxfVjEfHpiPj64v9H19X1k46IOBgRX4qITy7+flFE3L9Y\n048uYjPueUTEkYj4eER8NSIejIhX76f13NiHHxEHJf07bcfue5mkt0XEyzbV/lXGWUm/McZ4maRX\nSfq1xdj2Y+6Bd0p6EH//rqTfH2P8tKTTkt5+TXp15fFeSZ8aY7xU0su1Peb9s55jjI38J+nVkv4C\nf79H0ns21f4m/9N2/MH7JH1N0p2La3dK+tq17ttljuu4tl/410n6pKTQtlHLoWqN9+p/kl4g6f9p\noQPD9X2znpuk+ndJ+hb+fmRxbV8hIu6R9ApJ92v/5R74A0m/KSk9X26WdGaMkV4s+2VNXyTppKQ/\nWog1H1jEndw369nKvSuIiHiupD+R9OtjjO/y38b2NrFnj1Ai4o2STowxvnit+7IBHJL0M5LeN8Z4\nhbbNzHfQ+r2+npv88B+VdDf+Pr64ti8QEddp+6P/8BgjoxE/scg5oIvlHtgjeI2kN0XEQ5I+om26\n/15JRyIi/Ub3y5o+IumRMcb9i78/ru0fgn2znpv88D8v6cULLfD12k7H9YkNtn/VENvO1X8o6cEx\nxu/hn/ZN7oExxnvGGMfHGPdoe+3+cozxy5I+K+mXFrft6TEmxhiPS/rWIlO0tB1N+gHto/XctHfe\nL2hbTjwo6YNjjN/eWONXERHxc5L+m6S/0nn597e0Led/TNLfkvSwtlOJP3VNOnkFERGvlfQvxxhv\njIif0jYDOCbpS5L+8Rijjoe9hxAR90r6gKTrJX1T0q9qe6PcF+vZlnuNxgzRyr1GY4boD7/RmCH6\nw280Zoj+8BuNGaI//EZjhugPv9GYIfrDbzRmiP7wG40Z4v8Dgx+8XbsVce0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fbd9080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fc63e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 24)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 25)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 26)\n",
    "display_image(hh_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fc7add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fc7a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+f+Hjo1pb1kJEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fbdc940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnW3MZVd13/9rnrEZG8bzYgZjeVzsKBaIL5gUpSCiikJc\n0RTBlwhBkyqNkPiSVkRNFSAfqlZqpORLEj5USIiQUokGKAkqQhGJRUBtpcq8lKQJNhRCsTAav41n\nxi9gjGd2Pzx33/nPnfWbs+/MM3d4nrP+0mj2Pc+5++2cc89/r73Wf0VrTYVCYV7Yd607UCgUNo96\n8AuFGaIe/EJhhqgHv1CYIerBLxRmiHrwC4UZoh78QmGGuKIHPyLeHBHfjIhvR8T7dqpThULh6iIu\n14EnIrYk/V9J90h6SNKXJb2ztXb/znWvUChcDey/gu/+rKRvt9a+I0kR8XFJb5OED/7W1lbbv//i\nJiMiLe/bd56QbG1tXXTMy/49h/+wPf/882n57NmzaZ1e7v3u/VgtO86dO7cs//jHP55s08+fanNk\nnA5vx/vix73ObHwj1+e6665L63DQvNAcZe142eHj9z76/UbXLqvf/+5103Uj+Pk+NroWva3s3l8t\nZ/fCs88+q+eeey6/SQxX8uDfJul79vkhSf/gko3t36/jx49fdNwHef311y/LN95447J85MiRi469\n4AUvSMuO5557bll+9NFHl+UnnnhiWX7qqafS9l/0ohctyy9+8YslSYcPH07/7hfh6aefXpYff/zx\nZfmxxx5L2//Rj360LL/whS+8qM1Dhw4tj/mN7Dee3zxePnPmzLLs43/yySeXZb+ZDh48uCz3sfqc\n+DzfcMMNy/KxY8cu+p504bX1eX7kkUeW5YcffnhZPnny5LLcx+fX3Ofc4Q+Vz1G/b1b7ddNNN6V1\n9vn36+B1+xh+8IMfXNRX6cIfimeeeWZZ9uvvYz516tSy3H8Qsnt/ta/+Y9vn+Utf+pJGcCUP/hAi\n4t2S3i1deEEKhcK1w5U8id+XdLt9Pr44dgFaax+S9CFJOnDgQMvoif9aEh3/4Q9/eNH3iN76Wymj\njtKFb8UDBw4sy/6G8uMZ7SX4D5y/Lf24l30c2a+4g+aK3v4OosxE2ftx76uP38/19p1l+flOb2ke\nva0+Dmck3o7X4XPo4yEm5H30cr/mdN84RpZOU0vX1eP9fvG59TkZuYYjuBKr/pcl3RURd0bE9ZLe\nIekzV1BfoVDYEC77jd9aez4i/qWkP5e0JekjrbWv71jPCoXCVcMVLbpba38m6c9Gz4+IJYUhC7fT\nLqd1zz77rKQL6bcbYNxY089drcOpmdfjNNENVpnRx5cRROOc3nsf3XBGlmyvv9fj7fi5bhQkqu/H\nR5Yg3n6fIzLuOfy6ueGQzvG+O631+vs4aFlASxq6t/y+IPS58D5R3X4NfTy01PB5pPus34t+f9LO\nRNbHkaWoVJ57hcIsUQ9+oTBDbHx/LXNKcTh98j3QTmGcrhGlcxrlNN7LI7Qr2zMlCys5jXjdvhzx\nvviYMtpNVJ/mwusmS7r3yyljtgTIdjdWy0Sj/bo4HfZ+OTJr9ogDk5/jY/Dzifb7fPWx+j66XxPa\ndaLdlswhS7rQN8P72JdDtIycurajnrj1xi8UZoh68AuFGWKjVH/fvn1LKkXWZnfUIQeNDqJdRCMd\nbj12Cu5unU5xO8hRg7wSvd++dPDxk9NOp6wjTituMSenFYdTfafGXu79onES7fZrOLI08/E5Ze/n\nkKOKn+v9IrducqvNnI+8r+SQ4yB6T310ePvdJdiXRe4a7Mez5TJd71XUG79QmCHqwS8UZoiNUv2I\nSC215EBDVtOOEScUsryTtd0dbjK/cbKaet3eL2+fHIgoLLfXQ04g3uaIr77Piy8vvC/kuJK1Q+P0\n4yM+7z6n2a4BLYtoCUI7NrQ08vp72f9Ood1E+0fCdckXv9dJzkGEXsdI21K98QuFWeKavfEd5J6Y\nGWDWFYIgkQlv08u0Z97L/jYd2V8eeStMnUORXOTK6f2m75KWQcZQKPLNQfPvb1+KiPO3W2aAowi/\nkWg3uv5k3Ov1u4HSy/72JV8Ar899UYhNZWOm/Xov+3j6fNYbv1AoIOrBLxRmiI1S/dbaksIQNaQ9\n7YwCkXGH6qB2sihAKfcNoDa9DqeGVLfvzTrtzSg7CVHQXDiov0TZszGPRPiNuEOTfwG50vZ2SVaL\nDHAjhk5aMvY6T58+vTxGyygyPpMxkIx0mcGQliu0FM3avhTqjV8ozBD14BcKM8TGXXa7OyXt+1LU\nXqdsI3pytC88osXm9In2erO6yQXV63Aa68sBcvfM9NdIwIPoOFHzzCK8Wn+nlUQv/Xu02+Bl3z2h\nOfexdmrs51Ld3u91l0CO/l3aR6f26ZpTNCX1MdvJyfxJVtvs5aL6hUIBUQ9+oTBDbJTqb21tLSPU\niDI5nA536kUuk+TMk9Gh1fPJqpxloSG3T6eGXgc5rZB12PvVLeLk+OH1OYhe03KILM9Z9iKKgqT6\n/LhHzTnI6SQbNy0pRhy4KLIw2xEgZx9yTvJ+UTQdXf9s54kcjxxT37sUJt/4EfGRiHg0Iv7Wjh2N\niHsj4luL/49cqo5CofCThRGq/58kvXnl2Pskfb61dpekzy8+FwqFXYJJqt9a++8RccfK4bdJesOi\n/FFJX5T03qm6tra2lpFw5JRAftv9OPlqk3MKacRRIkSH07R+Dvlqe0410pajBJZe9nH0HHxOY0eS\nU7rgh/vhk0be1BKE6PKIlXokOSlZ+LNMPg7asXBMJUFd7W8WH0D3E0VK+vk+R55TkazzWft0D4/6\n5We4XOPeLa21E4vyw5JuueweFAqFjeOKrfpt+ycKNw8j4t0R8ZWI+MpIQoNCoXD1cblW/Uci4tbW\n2omIuFXSo3SiJ808duxYyyy1TnXJCt5/NJxGO9Wi0Fq3JJMjjJ9P/tS9fdoBIKs++ZCTP3mWcJKc\nncgn3scwoq3nVujMUk+6dbSrQUsQ8rmfEjchqWvyj3dMOcpI+XJoJKko3U+0BPF7lDQSO2jXh5ZF\nm3Lg+YykX1mUf0XSf7vMegqFwjXAyHbeH0v6X5JeHhEPRcS7JP2OpHsi4luSfn7xuVAo7BKMWPXf\nCX9607qNtdaWVIUoo9NOt4J2q7mf6/p45BAxYsklep/583ufyFeeLMy0k0DW9iwUmBJl+jhH9Pyc\n3tI8dozkaqdlzFTI8er5Ptbex2x3ZfV7tDNDSwofk3+3HyfnHBrzFAVfBYVFZ+HnJLWdhSWXAk+h\nUEDUg18ozBAbT5qZyVQ7ZXH/fKc1vUzJJqnslI1oop/v4ohktc/qICcLWl44HSRre+a05CArNUla\n+/lOdclqnvXJzyXZa8oVTzscdP7UuSPZk3xJQ/OcLYdGlIZGst2QQxo5k02BdoAytaRLod74hcIM\nUQ9+oTBDXDOxTVI4IQtup2DktEE+0WTVJocbKve2yIHEQX0kHXzKApQp0FDSTmqfBCmdmlPmm95H\n3yXw/pFyjtNe7y+F7k757fv3vDzleCRx6K6XfVejzx19j5ZrFMMxknxzypnJ559CnnsdvlS+FOqN\nXyjMEPXgFwozxEap/rlz55b0jGgSWT47HR0RdSSrqoPoGPll9+PkKEK03ykdiUY6fc7OoTbJn3vk\nHErymSXWPHjw4PKYXxO6bhQ6PTJ32S4ILdEol8GIMpGXM3rs8zCyM+Og5Rjt/GTXnJymvOxLlI4H\nH3ww7dMq6o1fKMwQ9eAXCjPExql+d5BxekOU0elODy8l6kR0bCTPOFEpd1bpdWZhs6v9HokbICWd\nzOFlSkt9tc11LOYSZ87tc05+6xTaTD755EA0RYdHnLbIIYtiIkjVp4/Zlzd0LoUl084PZTHOLPm0\nFHR1JUe/LidOnEj/ftH5Q2cVCoU9hY277GaJEMno4790fS9zJOsOGYsoImpEOKG/lUhFaOSN7/VR\nYs8M9KamaL8RSfGRc/qYaf+ddAOnNPSkC8efGRT9fBobXX8HGdromvf2RwRUSKvQv0uMj8qZzqHf\nNy6skjGrqXtp2ebQWYVCYU+hHvxCYYbYONXv9ITEF8ittVMwirZy0H45RVll7VBfaF+eaDQZDkfc\nQDt9Js09As3tyD5yFilHSwraOyffiRGNvGxpNhK9RhLYZLjsMu+SdOTI+Xww3ahHEYuUSJXmaOR8\nmq+sPlq6jFL85flrnV0oFPYE6sEvFGaIjVL9iFjSJqJv5G6ZWTsdIzSe9rpHLLgdTvucLpLLLsk7\nr2ORJ0pHOxy0jBihpplPBenTjej/retu7Lsm3ZpNOwYO2jEgvwzfD/fj/Zr6MbLA064GuYxTRGjm\n6+BjINGa7J7bMXntiLg9Ir4QEfdHxNcj4j2L45U4s1DYpRih+s9L+o3W2islvVbSr0XEK1WJMwuF\nXYsRee0Tkk4syk9FxAOSbtNlJs60epdlp0CkebfsMLjmkvWc2qGkkY4sao2sxORMQjTe6aBbxzO3\nYtoN8PadmpKTB9F0n/MsnztF21EfaTlGrrQjWXCy7zncyWVqZ2i1nLlJ073l8Hn2a0jLKKL3mVux\n/92j8Lzf2RLhqmTSWWTNfbWk+1SJMwuFXYvhBz8iXiTpTyT9emvtSf/bpRJnetJMerMWCoXNYsiq\nHxHXafuh/1hr7U8Xh4cSZ3rSzMOHD7dOd/1HgDT3HJnscRa9J435VlNue6e1TqW7Y8dU1hfvq8Ry\nzN4mZefpY/Ulh4/f6a1TfY8s83nx/nqbjqnsNCSE4iA67nPr1JgcnrLoPHJIohgLStTpfXTtvl4n\nSXHTUst3CUiUhCS9s6URXXPasejjoWXWKkas+iHpDyU90Fr7PftTJc4sFHYpRt74r5f0zyX9TUT8\n1eLYb2k7UeYnF0k0H5T09qvTxUKhsNMYser/T0nEH9ZOnJmB/KyzcNARxxuyZJOf+Yi1NQOF4hKl\nPH369LLslnSnml5np+kjYZ5OAZ3qkqCE01SyBPflCF0f2skgB56RnZdMgpqWH1N0WWI5cLr+vS9E\nqX1uffy0vPC58KWmn+/Lrn4+7TrQ8qqfX2G5hUIBUQ9+oTBDbDwst4MSMTqynPNO3cj3najpSJJL\nokr9u2SNdgrqYyOVFsq8klm43TJ8uXVI7H9OOxydPtKSh/TsnKZ6X2h5Qc4y/fi6GWucRtM1oowz\nmf+798+XTuQcRnM+Itnd+0XZgxxeB0l9E+qNXyjMEPXgFwozxDWj+k5T3MmE0Omj02gKV80EI6UL\nKSBZm50CZmo7ZLElaythRCY8U6AheW+fFwqXJXlr6ld2jPz2yT+eHIic9k6F9Pqc+w7IqVOn0rpH\naD/tyGTLGj/3zJkzyzItF0n1iYRX/Zr2746EkHu5t7NjDjyFQmHvoR78QmGG2CjVb62lVlOnQET7\np4QX19W1d2rm1mHK855ltXHaS7SPKCD1xdHpMKmx0FzQEsjboSw42e4InTviLOJjdn92yrCTqfGM\naNlTlhqac1LS6eP3dpze0w4EicB6SC3582e6+pTLwMu0XBtBvfELhRmiHvxCYYbYuFU/o8yOKSUb\np0CkekLOQV52Kkd+2V5n96deVzAz8z1fbZN2Ifo4RhR4fInk9NIFQd35hHYVvM4eT0DOJuQcRfkG\nyKrv85hZ+2l5QX0ZyavgtN8xtaSj9GAOCql1TDmNUaoyOr7urlK98QuFGWLjabL7W5p+radcZkmK\n2cuHDx9elv2XnTTqSCzB0ftLWU/I0DKiJ0cutpkhlMRHDh06tCy/9KUvTct+vtfz5JPnBZWeeOKJ\ni84f0Q0k8QkynI4IZ/Q6R/wFnMGR1Dr5jmT105uVjIsUHUn9dWQMgXwLfM4zlkMy76uoN36hMEPU\ng18ozBAbpfpnz55d7ok67c7y0Eu5+ypppTl1IwpKUVMjMt29vyQYSvvrGXVdLdP5vV+UPccNdy95\nyUuW5VtuuSU9TrqAjz/++LKcGUCd6vqcEwXN9sV9PNKFyzGip32Z4PWNuNqShiMJYWQy6X4/rZMB\nZxXrZv7p8z/ir5G5+l4Vee1CobA3UA9+oTBDbNyq36ki0UGy/GZCGOR2SRZZomN+fGoJQHngR/Z3\nycWX+tXpqFP6m2++eVm+/fbbl+Xjx4+n57i139t0/T8S0ejHXR+Q9redAvtyIBvPavsUQdfn2uuj\nZRzlpKfrReVM0pt2FWjpRtc5o+ZSHkFK/aMlYp//nUyaeSAivhQRf71ImvnvF8fvjIj7IuLbEfGJ\niMj3wQqFwk8cRqj+jyS9sbX2Kkl3S3pzRLxW0u9K+v3W2k9LOiXpXVevm4VCYScxIq/dJHX93+sW\n/5qkN0r6Z4vjH5X07yR9cKKuJd1xykIOD071O5VzKk5RS04NPSKKdNbWEaUgENX34yMuu25hPnJk\nO/O4W+lf9rKXLcu33Xbbsnzs2LFl2am+z6FTdpL3zvK8U+JHove08+HzQgknM/25Ecv4iBAGuQl7\nv3rfyXXb4X0lN+0s8avELsN9/mlJRbsa2RLlUhgy7kXE1iKZxqOS7pX0d5JOt9Z6yw9pO4NuoVDY\nBRh68FtrZ1trd0s6LulnJb1itAFPmjmVoKJQKGwGa1n1W2unI+ILkl4n6XBE7F+89Y9L+j58Z5k0\n88CBAy1LSkiyw5kPP4lcOKUieku7ABRNlVlqvR3Ka59lZpHYUcjb7/ReOk/x77zzzuWxu+66a1l2\nP3y3/JOYiGvUnTx5cll2C7uLTnTa6+NxGu/jp2WXn0+S3uSrn0XKjegm+vh9Xkjq2pHFioyIrHib\ntGNF0XSZg5Rn16GlUyYssmO++hFxLCIOL8o3SLpH0gOSviDpFxenVdLMQmEXYeSNf6ukj0bElrZ/\nKD7ZWvtsRNwv6eMR8R8kfU3bGXULhcIuwIhV//9IenVy/DvaXu8PY2tra0llSSyDrN0ZhaFMLmRJ\nJeEMp0yZA4t0noJ63b6M8Aw0I5Zct2qTA1N3vnGLvfve33rrrcuyz6EvO5y6O+334071M6v9iMiG\ngxxoKKuOU93s+hMtJ7pM8QwuUDIVRk3LPx9z5kAjsbahf9fvnUzfz6m+15HFFVxqPIRy2S0UZoh6\n8AuFGWKjvvrXXXfd0lJNOm9OsTLKRN8jZRSnt+SQQ5Z30nfL2iGfcALRx+wcnxMvk/4eWbtJvcj7\n7sf7OMgaTTLmPp+kGDOietSXQ+TXTgo4NEe0jPP56uf4fNJSg3QbKZ6AQnp92dXVkJzqk4x3FkJe\nmXQKhQKiHvxCYYbYKNV3qz4p3RBVyYQKM79yiS3JTrVIjceRSRa7ZZ4oJfnnk9gnqRFlSwBaUviS\nhmIFKHTUHXG83OskYVAHHSdq7rsaJIGeOXs5yDmIfOj9fHLEycZB/u8+HrfSO0333R76braTki0/\npAvnIhNPHcluJNUbv1CYJerBLxRmiI1S/X379qUWUtLYp+Md7pxBvvIkiEjLC7K2T8UYUMinn++Z\nbDxc2MfhxzOBT3e28Xac9jlN9PO97BTUy35On1OfQ8rYQk4+Dqfx3l+PT5ha6hDtpjZJhJNCejtl\n9nuI6nZK73EQLl5K91/mHObHaSno94c7JPVziuoXCgVEPfiFwgyxcV39bv0kMUNysujnkPChU0en\nYERTpwQmV9vq/SWBRcp3ThZ7p2leznYYnIq79Z6SfXr7/t0TJ04syyNhub0tCnklJxhv38fsYzt6\n9Oiy7EugTOyUHIwo3wCJoFIfp1JR+RLBQbEP5IdP/v/Zfel98jnxe8Xv+V5HUf1CoYCoB79QmCE2\nTvW79ZPyxpPffKc7pHRCgoQOEkQcyQDbz3eqSRTckQmGrrbv9MzP6UsWp+LktOR1UDizU1C3QpPw\nZiaMSj75Ph6noJSzwMtO9bPrS9eT6Dqp3oygf5ecfWiXwEFCmd4vSuHV54ucvXxuR7JME+qNXyjM\nEBvPpJNJXI9EtvU3gf/iOUgIg1w2/S3nIENj1k4WVbVah8P7Ttp1WV/8Lexvf3JNptzz/ibyNum7\nq/2QLnzLkEQ0yYjTm4t8A3p/KYEk+V84yL+A9tR7f8lNmdgHuQB7PVOuwdL5Nz25NJNQzais9vK7\na51dKBT2BOrBLxRmiI1S/dbacl+douZIsrqDsu5QtBOB9NrI6NbrJz07MrR5f52ak0BIZlwcyYDj\nNNZpordPCRwp+quPw+eKohPJBXoqmkya9o2gRJG0BPH26drSmPpxElkheL9GIj/Jv6CPg3QgKVJx\nVFa7Y/iNv8im87WI+OzicyXNLBR2Kdah+u/Rtp5+RyXNLBR2KYaofkQcl/RPJf22pH8d25xp7aSZ\n0nlaQznsneo7rc32V0mfb0rDzuu7VF+yyCoS/PCy0zTaUyaqne1wEC2kqDHvo4/BMSIc0udxXc0/\nWl543SQQko2DtP3oeo74SJBVv5dpN2BEOpz6RRb+KRl5WhZl13ZHk2ZK+gNJvympt3qzKmlmobBr\nMZJC6y2SHm2tffVyGvCkmet6URUKhauDEar/eklvjYhfkHRA0k2SPqDLSJp54403tk7fiAIT1c6O\njUTVjViVSaNunUw+pBVIx2lXY0oemaK2iIJ62ek45bPPljdk1SanFYoUpOtM0XT9HiGHIActgUaE\nOzI6TvqEvvykpdY60ukj55Pbb7YDtGNJM1tr72+tHW+t3SHpHZL+srX2S6qkmYXCrsWVOPC8V9uG\nvm9re81fSTMLhV2CtRx4WmtflPTFRXntpJkRsaQ1lExwyhfcqbuLEoyIHIw4kPj5GWV2KuzUmej6\nSKQgUcaM9pFPOi0R/Hyit2QpzhyhyD+friElM/WlBsVN9GtEbRJobD4Xfr9k14J2HWiuyGmJdp7o\nmvd7imTJyQmsn0P1rqJcdguFGaIe/EJhhtior35ELKmP02TyRXarZac+maSwxEIUZG32dqacORxE\nI0lYhKy6Pmaidb08koeeklCOJAp1epjJbtPyhqz9tNQiERMKo+7j8LH59XeMaCh6mZaGGdWne4gE\nSkhnkMJys90hcjyi3ZD+rOyYVb9QKOw91INfKMwQG0+aeejQIUkX0henQ+Rb3inWlFqLxCo+5DdO\n1Dyjb06lnHZ6m041fSfBz6d2fPxZfMLIbgCFiJJ/PoX39jE5Ffe+EI2n5KQ0Dlqy9LgNUl3y+fRr\nS2HJtARxZGHjI9+j3SaKW/Dr7EvaLJaFnN1IH3IE9cYvFGaIevALhRnimln1HSNOFpkyCglGEh2m\nusmfO3O4cBpHfv0kNU0hrbS8yWjfSDYYB1n1R+TIs+xFJPboGJHDpvP9mvbrQmHGXh85Z1GSSQoR\n7rsNI7s+DkqgSs5HU5Z62nXx+ckczMqqXygUEPXgFwozxEapvpSHWhIFzxx0SC3HE2U6NRyh9FMW\nXq+HVGcc5KtPjjqk8JJlkhmh995HHzNZ52kXoI+PnK18N4YSktLcjujgd1pLSwSn7n7Np0K7V9tf\nR8nGj9O94GXS2PdrnsVHkNWflrr93tppBZ5CobCHUA9+oTBDbJzqd0oyoliTqaA47XHHE6f6RF3J\nUYfEETPLrluPidJRaCv5pE/lcx+JJRhRppkSEpXyZI60M+Ljp2SODtodIP/33keiy051/V7wflHd\ntATs14icwCjMlpJg+jj93qW4gb4EG0kDl+1MFNUvFAqIevALhRlio1R/3759S0pEVu0pZxand06H\nRqywlIppxJmlUzZKveSgsRG9J034TgFJj53CmSklF1n7KYy110N+494OWawd6855putPDkTeJqVQ\noxgCR3af0RKF+kLCr74EoDmaShvmc+Fz2MdDOyCrqDd+oTBDbNy413/pSLqaDFCZ7PGI7DJlzHGd\nNzLMZRF0lJOc3IRHMt9QZFk2Zge9fUZcTEnS2VlMP+4GJX9rjuyd0365G2PdMOfob0jy83A/An+b\nkgQ1Mctsfkf8D8hA7XNE0Xne9+wtPdJXn4vMEHwpjKbQ+q6kpySdlfR8a+01EXFU0ick3SHpu5Le\n3lo7NdRqoVC4pliH6v+j1trdrbXXLD6/T9LnW2t3Sfr84nOhUNgFuBKq/zZJb1iUP6pt2e33XuoL\n586dU5ZJZyQLSi/THr2DjGGZnt1qeYoqZfnjJZZdHsntTpFl/RzaUyaBiJGEmE7faT+6H/e/k1GS\nDJp0nZ3ek6Gvj4Oi3Uhwg5ZjlPDUqXlvy6k45aR3kJt4Vrc07ZtBfg7kJt7b3GnjXpP0FxHx1Yh4\n9+LYLa21E4vyw5JuGayrUChcY4y+8X+utfb9iHiJpHsj4hv+x9Zai4h0P23xQ/FuiSWUCoXCZjH0\n4LfWvr/4/9GI+LS2M+g8EhG3ttZORMStkh6F7y6TZh46dKh1WjcVZbT2QAYSFRIdp0ioLOJsRF6Z\n9u5pD56ivHqdfows/JS9hXYSiGpmdNTnh3YJiK5TFhiKDsxor4tpeJnG7PPp7ZAPiCObL9qvJz8S\nb9PPp3sruxfJTZq0GrNdp0thJE32CyPiYC9L+seS/lbSZ7SdLFOqpJmFwq7CyBv/FkmfXvz675f0\nX1prn4uIL0v6ZES8S9KDkt5+9bpZKBR2EpMP/iI55quS4yclvWndBjsNpEwllJ2lUxjS5yMrKFFw\n0q4jx4leJms0USyKCCR572w5QDsDjhEtvBFrf7Y0oeXXSGJJBzk5UaRkp+8k8kHWfpqjkX5lO0U0\nn7RjRPPp9dCyx5cDWZ8oqq9fW1qKraJcdguFGaIe/EJhhti4r36nIk7NyLEjEx0YiYiaokPShRTM\nrbN+TiaQ4VQso2Wr/aL+EgXMREFGsv6QY42DIusIUznXSczC+0KZd0byzJODVtYmOQqR85ODrkuH\n7wZQ4kvqC9VNS8p+nJYflJCzqH6hUJhEPfiFwgyxUaq/f/9+HT58WBL7eZOFeyqcd8ThYcRpxiWb\nM003ymQyskvh/SKdvSzPPclVE+2nvozosU3tIJBzEPnze3+ffPLJtB3HlChLllRz9XsUkzBC+3u/\nKJGog66L03Fvn3ZVKM5hqp1MQGXUAa7e+IXCDFEPfqEwQ2xcc69bIknHjehQdi6BKB0p9njwEDnl\ndAqYJTgWJHzwAAANf0lEQVRcBdFYCtclOe4+V6SJ50sU0txzZHp60rRKEO2SjDhHnTlzJj2f/Nkz\nZxVaftBSa8oJa7XNbCeHdpdIXchjCBx0L4zKYK+2Tz7+Uzsgq6g3fqEwQ9SDXyjMEBul+q21lPqQ\nYoojs1pSEkp3rBkR5xzx4e79IvlvklQmek0OHxmVHaHaU6G9q+MZUQnKdlJI7NLh1nty2iFlHLeI\n9+/6MRIVpcSSdP3JsavDxVgp/JacwMjyTuHN2TUieW+H19HHTzsQq6g3fqEwQ9SDXyjMEBul+ufO\nnVtSmBGrckYxyT/cqR6FRTr8fKJvbtntZbISk5PFiLWVrPa9PBW2uvo9Ou67EKSJn80LhTxTJiGf\nw6eeempZ9nHQkilz/qGMQeRARA5h5MCVOUX5Tg/tElByTlre0bLH0effqb7PCe1AZQlOL4V64xcK\nM0Q9+IXCDLFxqt+tpUTHSXixW5BJaYXoIIWrEu10ZwmnqZ3qk0LQSDgkUUNHFmdAuxc+tszCu3rc\nz3crtFuwM8eakWtF8+/Xi1R6yJ+994uSfXo7IzsZdH7m8DSiCO1jo/YphZaf4/Pfr6/fe0773SHq\nyJEjy3IfW1H9QqGA2Pgbv785SYiCJJMzeWsyojhIjtjb9zekG/QyJkBukvQ2I/GLEX+ErA6v298E\nlFWG3qxe9jFnEtAjoiEkUOFvIL+elCj15MmTF33Xx9+jO1f74mM+ePBg2q8RZPPvEZvEGmguiGVS\nZGH3gXAW4G9/Sg7aGfGOvvEj4nBEfCoivhERD0TE6yLiaETcGxHfWvx/ZLqmQqHwk4BRqv8BSZ9r\nrb1C24q7D6iSZhYKuxaTVD8iDkn6h5L+hSS11p6T9FxErJ008+zZs0vaQvurRJM6hRnJ2EL7uCRc\nQXuqjk7l1s3Y46CIRHI37mMd0byjZJKky0f7xFkEG1H9TChltT66RiP77tm9QtqKPn4q0/Iik0yn\nMZNrNCWz9Hlxg6ofz7QbyYjsx31sve6Re1kae+PfKekxSX8UEV+LiA8vMupU0sxCYZdi5MHfL+ln\nJH2wtfZqSc9ohda37dcXJs2MiK9ExFfIW6tQKGwWI1b9hyQ91Fq7b/H5U9p+8NdOmnnDDTe0TquI\nJjoySz3lXh/5UaHILqJvjmxfl+gaST17m0S7fZ+21+ljo+g4EqXwc8g1mmhlp42UeNLhNNbpplun\n6Tq7FT5z2fa5evrpp5dl2gEa2Tun8fdx0LKMXMopSxJFB05Js990003LYz6H5LLd79sdS5rZWntY\n0vci4uWLQ2+SdL8qaWahsGsxuo//ryR9LCKul/QdSb+q7R+NSppZKOxCDD34rbW/kvSa5E9rJc1s\nrS2pYpYxRpq21Du9IQeakYgoitRyZNZx0scjGkeRev5dtyqT40YHuZpS1Bpp21GUYzYmcgIi0RRy\nB/Z+HTp0KD3u6MfJOev06dPLMlnsXQuPqL5fr4zqe90+/+tm0hlZJnSK7/ekL4W8TV/S9DmqTDqF\nQgFRD36hMENsXHOv0zCiRuTP3TGVS15iSzrRIEpsmfVxxEFiJGqLohAdGcWlGIORXQ2ioLQ0yqg8\niVJklvHV497mSGxBr5/iCvz4448/np7jyyW/txy0lOnwefC6KauRHyexED/f56L3xeMDRhzSeptF\n9QuFAqIe/EJhhtgo1Y+IlG6R/3fmoEGZZIhGTSWBXD3HkWVQIXpLohQkDU3CGe4g0+skuki7BKQn\n52Xqe0YlSTSEREFoh8WvM/m5Z/NCYhpkmXc6Tn77JFOe7d6QUwyNzZ2m6LtO77PlEC2FHZlDWlH9\nQqGAqAe/UJghNp40s1srSaONaGo/7g4ZRBHJCYgs0qQYk1FJUuhxkJMRacc57XMf7U77yHpPlnly\nMiKrNlmke/0Uiky7FNQXap/iL/o1dQs39YWWGm7V93OcjmdKOjQGv898WUYOYbTUo3nsbZFTm48/\nS5q6Y776hUJh76Ee/EJhhtgo1d+/f7+OHj0q6ULa51SXKHs/TpSKLKxOI0kQk6h+Jk5JWWcob/nI\nrsKIHHUHjY2cmabmc7Xv2fySpZjEJn05RMsun0cvZ+07LXe/dffVp2XciDIQCYh2UAg51U2Zb2ge\ns10QCpWmJUImhX4p1Bu/UJgh6sEvFGaIjVL9ra2tpS76CDXNaBVROkrgOJLPnBRWaJmQYSrH/Cpo\n/D6OThn9GImEUjLLLHRzFeTk06k8LReIOvuYKc/8SH6C3r5b9cnxh2IYHKTGk1F9co5y0NLFzycr\nPO2CZH/3uh3Zs0DLyVXUG79QmCHqwS8UZoiNUn1p2vpIFKeXya+fQnvJz3xK13y1rQxE1ymd0khf\nplIgUUwCpXmiJZCDllpZqCfRW1reOO0loU6n6W4F79TYx+AWfp9PV/TxXQW6hhQ6nTkZ+d+zBJcS\nj9/P8XpIELRfU4o3oXwA/RrupK5+oVDYY6gHv1CYIUZSaL1c0ifs0E9J+reS/vPi+B2Svivp7a21\nU5eqy7Plkt86+UhnYptTfv29zQ5KhURW09W+r2Kk/RFRT6KJvU2fE3fsII31EacRb4fiJrL+UR2U\nhsvhVNfPcfruS5YsroPuFZ8Lj3dwkNhoRru9bl+u+HzSLgWFbrvzkY+DlmkdtCzNruGOUf3W2jdb\na3e31u6W9Pcl/UDSp1VJMwuFXYt1jXtvkvR3rbUHLydpZmtt+Qs4ItaQ6ZLRnjfJTtNe87qJIPvx\ndetYNz+7o9dJLId020gLzt9EI5Fi/bvkAjsiFuHXiPwuyHjXQRGG/jalPXqP5nTQnPa+0Hj8fqK+\n+Dk+/54FyPvr125KZ5GM3+toQkrrr/HfIemPF+VKmlko7FIMP/iLLDpvlfRfV/82mjRzZC1dKBSu\nPtah+v9E0v9urT2y+Lx20syDBw+2TvdGxBqy/OdThhDpwv1i33f1fU/KakLSyFN7+kSXHWQM8/56\nOYvQIhdQMjTSGGh5lUXw0Z4yuTSTHwMtzTJ5aen8XFDk24hcNUX+kWE087WgJQ3NIcHv82wP3vtL\nYipe9jb7XF0Nqv9Onaf5UiXNLBR2LYYe/Ih4oaR7JP2pHf4dSfdExLck/fzic6FQ2AUYTZr5jKSb\nV46d1JpJMyNiSaWmXFP7+R2d1pD13CmQ66y5WANF5PnygfbdMypFfSEXS0pOeerUefeHM2fOLMud\n7jn9dCs1LXUcIyISRFl7f4nqjmTvGdk98OVNtoPgY3CrP2kV+jmkV0haeB3kRky7UdQm+Tf4mKds\nX34uXcN1niupPPcKhVmiHvxCYYbYeCadTttIIIIcJ9ZJ4Eg55t2SSg4clHwz68e6Gn5edmcOp/on\nT55cljsFdAu4wymoU02i92S9XwdTywKJoxAdvmMxRY0pqs1dYH0uvC8UTUfLtA66t7LsTqtt+nj8\n/iOBkszJLMvitFp3Ns+jW+b1xi8UZoh68AuFGWLjQhwZrSJ6nVlQyW+ZRBEogmrEySZra8SZhazX\npMvmS5ApBx4aj4NETkZo+lQ9lJ+dlhd+zoiff0b7vY6RyEef50zDcLUvjj6OkSxJDlo6kaMWOXP1\n8fu19+9NRQ2WEEehUEDUg18ozBAxKse7I41FPCbpGUmPb6zRa4cXq8a5l7Bbxvmy1tqxqZM2+uBL\nUkR8pbX2mo02eg1Q49xb2GvjLKpfKMwQ9eAXCjPEtXjwP3QN2rwWqHHuLeypcW58jV8oFK49iuoX\nCjPERh/8iHhzRHwzIr4dEXtGjjsibo+IL0TE/RHx9Yh4z+L40Yi4NyK+tfj/yLXu65UiIrYi4msR\n8dnF5zsj4r7FNf3EQptx1yMiDkfEpyLiGxHxQES8bi9dz409+BGxJek/alu775WS3hkRr9xU+1cZ\nz0v6jdbaKyW9VtKvLca2F3MPvEfSA/b5dyX9fmvtpyWdkvSua9KrnccHJH2utfYKSa/S9pj3zvVs\nrW3kn6TXSfpz+/x+Se/fVPub/Kdt/cF7JH1T0q2LY7dK+ua17tsVjuu4tm/4N0r6rKTQtlPL/uwa\n79Z/kg5J+n9a2MDs+J65npuk+rdJ+p59fmhxbE8hIu6Q9GpJ92nv5R74A0m/KalHgtws6XRrrUe0\n7JVreqekxyT90WJZ8+GF7uSeuZ5l3NtBRMSLJP2JpF9vrT3pf2vbr4ldu4USEW+R9Ghr7avXui8b\nwH5JPyPpg621V2vbzfwCWr/br+cmH/zvS7rdPh9fHNsTiIjrtP3Qf6y11tWIH1nkHNClcg/sErxe\n0lsj4ruSPq5tuv8BSYcjood375Vr+pCkh1pr9y0+f0rbPwR75npu8sH/sqS7Flbg67WdjuszG2z/\nqiG2g6T/UNIDrbXfsz/tmdwDrbX3t9aOt9bu0Pa1+8vW2i9J+oKkX1yctqvH2NFae1jS9xaZoqVt\nNen7tYeu56aj835B2+vELUkfaa399sYav4qIiJ+T9D8k/Y3Or39/S9vr/E9K+nuSHtR2KvEnrkkn\ndxAR8QZJ/6a19paI+CltM4Cjkr4m6Zdbaz+61Pd3AyLibkkflnS9pO9I+lVtvyj3xPUsz71CYYYo\n416hMEPUg18ozBD14BcKM0Q9+IXCDFEPfqEwQ9SDXyjMEPXgFwozRD34hcIM8f8BbwctRvbtUigA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2366fbdc278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnV3MbVV1ht/BORyQA/JfQjy0YCQabgRLrETTWJGGtka9\nMERqG2tIuLENpjYqXjRtUhO9UbloTIw/pYlVKGpKiNESxLRNGgoWW5UjBSmEQ/ixFiMe4RyB2Yu9\nV7/Bdow937H3/vbH/tb7JCdnfmuvPddcP3Ovd4455hjWWoMQYlwcs9MNEEKsH3V8IUaIOr4QI0Qd\nX4gRoo4vxAhRxxdihKjjCzFClur4Zna5md1rZveb2YdW1SghxPZiizrwmNkeAP8F4DIAhwDcCeDK\n1to9q2ueEGI72LvEd18L4P7W2gMAYGZfAvA2AGnHP/54ayedxB/A/yZFv0/LOB2aLf7diKwtzHGy\n8xzKxzhdlpWz4/SuIcMy16p6/N6x/OdZmWnL88/z7aoeJzumL/t6esfPPo+2Hz4MHDnSuq1cpuO/\nDMDD7u9DAH5j3hdOOgl4+9sn5WOSQYY/mWef3Sr/4heT/597bmtbdvN8Oescfjuz/1DOjuPb5bfv\n2RPX7Tl6NC4P57dv39a2/fu3yiecsFU+9ti47uG6zZazhyn6YcmuW3Y+/lpE9xB44b3z9e91T6S/\ndtE2f87+Gvl2+eMcObJVfuaZ+e3K2uSP6dvi9/fn7MvZOUc/SH7f7JmPnv9bbwXFthv3zOxqM7vL\nzO7yF1sIsXMs88Z/BMA57u8D020voLX2aQCfBoAzz7QWvTkZ+TTsk719sl9TX47eILP1eKJf9Kpc\nzNqbHT/aJ3s7MG3x2z1+e9auCEa6Z8fP2tg7P6Z9zDCCuXbRG9+/cbN71RuusW3pfc+zzBBsmTf+\nnQDON7PzzGwfgHcCuHmJ+oQQa2LhN35r7Vkz+2MA3wCwB8DnWmvfX1nLhBDbxjJSH621rwH4WuU7\ng5RiZFIkk/22TK5mxjVGdmfGuOG7mdHFfy8bdmRDjWxoMtSfSeHMWJYZHTPZ35PAWfsyOZrJe0bq\nR+1ijrPMEMiXvTFuwF/DqH0ANxxjypFxLzM+R7DyX557QowQdXwhRshSUn8ZmDlNzyBxGHnFWPWZ\nuelIVmXt9nIwqzsbRmRDk2FOP5OoXupHEnVeu6ryPfo8+142vKhaoXvXn5nVqTpWDftnlvzqM8dY\n+KPrVX2eo8/noTe+ECNEHV+IEbJjUt9TkUyMdMrkUGYRZaz9veNXHXi8G2jvnDPrvd/OnBsjk6N6\nsvvDWNWztlSGV55sVqXnag288Fnw198Pk4Y6mZmE7PgM2fAtmkHw7c7K0azXPPTGF2KEqOMLMUJ2\nTOpXV9ANMP7xjLzPVtP1fNuz7zGr3Rb128+cdrKZgWwYwRyz5zRV9cNnJHhleOVhZiyyZ8Gvsovq\n9/I/O59lrP3ZcxatCcmOyTznGXrjCzFC1vrGby12A2XWr0er8xjX2GyuveemGbV9tg5PdR634u7J\nrLDzb/nMiOjJ3laZ8WjAnz/jDpzVl90XT2/VWqZ4em/T2e9GsQyyupk3OKNyKr4TTCCQapAVvfGF\nGCHq+EKMkLUb9yKDTSZHV0E2RxrN3c7u3/MjyOSdl7SMMaxnMMrq82TyPhsCeLIwU5FxkVnhxxja\nmNWEkaxlpG52P5nVkcP5Z1Lfw7gJM/4KjJE4Oqan7A5d210IsRtQxxdihKxd6les85EEqlrGPYwP\nQE+CZVKLsfZnkj2zNvvIsdG+mYytRsXtUQ1ysUzgjshPgpkZYIYLTBuj4RXjpuxhrn82kxU9Z9kM\nSHStsudzFr3xhRgh6vhCjJC1Sn2zvhuut7ZHEosJeMFI2swK7/FybGgXY9VnpLGn52KaWWwzx6Oq\nFd7TC5yRrRT0MNefics3UHVOYY7Te3aY2YNqPEHGfbvnJh09k4vQvUVm9jkze8LMvue2nWZmt5rZ\nfdP/T128CUKIdcNI/b8BcPnMtg8BuK21dj6A26Z/CyE2hK7Ub639k5mdO7P5bQDeOC1fD+BbAD7I\nHLAXO6+yao4JBMEEwsiGCZGUr6ywmlc308aeA022PZOpi4a6XtUqvOo9mtemecfxVGd4hu3Z8C9z\nAsuGqIzU90T33NMLisIOixY17p3VWnt0Wn4MwFkL1iOE2AGWtuq31hqA9HdGSTOFePGxqFX/cTM7\nu7X2qJmdDeCJbMcsaaYns2pHmWqqFl4mmaZfltlz+MiGAhlV2V+pww9XMmeeqoW957SUyfteLLhZ\nsu9G8pWZselJZ1/f7P7RTBEzFM3KjN98b91A1fe/cmxg8Tf+zQDePS2/G8A/LFiPEGIHYKbzvgjg\nXwG80swOmdlVAD4K4DIzuw/Am6d/CyE2BMaqf2Xy0aXVg5ltSZxMdjHLKKN9Gcs3M0zo+VMzEr0S\nXWV2n8jnP5N6zMxEtj4gW5ZcaUu2nDeL+VeNZNOLuce0Kyv3ov5UHXUYpykP40zUg5klSL9b210I\nsRtQxxdihOxYeO3M2spkmBnIpCsjDXtLQbNjVsJCz9vODE0iGHldTc6ZSVnvlx99ng07mFkSJmLO\nsL06dOvdw9ntkTRfZmlv1bEqOm41LHrlGIDe+EKMEnV8IUbIiyLYZsUime3LSNdMgjHBNiPHCg9T\nNyMNo/oZi32WEJI5Tia7o1wCVQeebFYhSzgZXS8/5GBkdOXazm6PpDYD41jE5FiIhh3ZvVXSTCFE\nCXV8IUbI2lNoRZbijMiyWU3CmMnIaiqs6PiZRMucY7I1CZkVfihXI90w8pZxFOnJXub6M7H0e5bv\nbAaoGtGIuS6Rg1lWdzUhKiP7e59nqb+GdkvqCyFS1PGFGCE7JvUZJ5tIsjE+4YzcqQZH7MX19/Le\nD2d8OXO+8ZItWmqbDS961vBZqkFIIypyFeDkfS+dWTbrktWXyfts7Ud0fav5AJbJMRA5n2XDFf98\n+LwLsuoLIbqs/Y0//HovGn+NSXyYvbWqWVAqQT8YF1TmrRAdM3ONZeaFs4SkTEjxedvm7cP4NBw9\nulXODL7Dd1dxDWfpZR5iniFmhSOjRHuBS5gkqHrjCyG6qOMLMULW7rI7SKWqMa4iZaqukRWZWJ3H\n7RkLgb5MrK7qy6SxNyJ6spVq0b3KvpfVkQW/6Bn0MpiYh8z97AU3yYZIjP8H066e30M1aMm6wmsL\nITYYdXwhRsjapX60yq3ieluRPUAufRZNcplJPW+ZzSR1FsSCuRa9tmbDhaq/hJfg0fkzQxomKEVl\nqJNZ4KuS3p9bzzdimZDeDMxzGe3rr1UvUMo8mCi755jZ7WZ2j5l938yumW5X4kwhNhRG6j8L4P2t\ntQsAvA7Ae83sAihxphAbCxNe+1EAj07LT5nZQQAvw4KJMyPHEWZlXWThriTYnK3PU3W+6dXNZLth\nJHOv7qrUZQJhVIJCMG1ZZmgSDe+yVY3ZvcociHpuwJnVn3lWmPP39IZ3mdOQl/qRs9M8Ssa9adbc\niwDcASXOFGJjoTu+mZ0I4MsA3tda+6n/bF7iTJ8088iRpdoqhFgRlFXfzI7FpNN/obX2lelmKnGm\nT5p5xhnWIseIijMPI78rq+3mtSWTo71jZjDW4UjWM9bzrD4mBHfPwswEsKj66lcCcWTyPpvVYUJj\nZ/J52IdZ7eepOlNlQ4noGcnWOPhyFIp8HoxV3wB8FsDB1trH3UdKnCnEhsK88V8P4A8BfNfMvjPd\n9mFMEmXeOE2i+RCAK7aniUKIVcNY9f8FQCbAS4kzzfqxwSrONJlc8jCOOoykjyRgdZnvKtYnMLMB\nWeCG446Lv5tZpyOpz8weMDnkGSt4NAO0TKJIfy0yR6Vo2TgzdPAwswqeyFc/G3L2rqd89YUQKer4\nQoyQHUuayUjGnkxi5D2TMaayVqC65DaDGQJE8deqlnwv7/0aguw8esMXZialSk8yZ9KZGcYx16vS\nJma4yMj7igPXMmsCMvTGF2KEqOMLMULWHmyz52DQS2bJyN6qD7k/ZhbMcqiHsSoz0VOq0Xt6ZBlW\nvNT34ZgzKZstXY22Mc4iWVadbKgV3X/fJg8zjGP85pmhQUT2rEY+9LNEobF9OTu3bK1ENCych974\nQowQdXwhRsiOWfU9TK76qpQZYKy6jA99FDmIOSazpLbq2BPV5yX98cdvlU88cavs5aW/zocPb5Uj\nv/SqHzpzzZl7HiXtZIZrleXUQHxfsjUBHiYCFBN1KXK4ys6ZiQzEoDe+ECNEHV+IEbJWqW/Wlyc9\naVaNmc/I68wRpBL4shIzfbbcO04mNf0MxEteslU++eSt8ktfulX2w4Fnntkq9xxOMv9wpo2ZY1El\nYg7j15/53vei+8yWhzZW1174++nbwkQP6vnqZ+fZi2Y1D73xhRgha3/jR7+olXDMTAAHJq00kxTR\nExm6PFW3WsYAOJSz1XZ+vt4b8U45Zat8qot97L+bvfF74bWZlWKZymFiEUbHYlb1eaoZk3qrD5lQ\n2Nn5+LZnb/xI/WXPR7bCUEkzhRBd1PGFGCFrn8ePgjt4eivVmMws1VV72fE9UXszQ9ei9QFxRh5v\nlMvK+/dvlb3UP+mkX65v9jhe9j/99FZ5cD3NzjNzU/Xn7H0HMkOXP49IynpJm5Uzqi7TURj3LP4e\nI8erPh0DTNxIfz+ZZ9GjN74QI0QdX4gRsnapH0l2JojDsA8TQKMaC49x2R3qiUIxz6uvOrzw+w+u\ntyecsLXNl71rrpf6fh8/v+/r9qGZs5mCyH00s+Rnq/qyeedKOZsZyKi6VUfz4dWQ4lndWUhzxiV4\ngJlJ6sWynIUJr328mf2bmf3HNGnmX063n2dmd5jZ/WZ2g5nt69UlhHhxwEj9IwDe1Fp7NYALAVxu\nZq8D8DEAn2itvQLAkwCu2r5mCiFWCRNeuwH42fTPY6f/GoA3Afj96fbrAfwFgE/Nryu2xFZDVldg\n4p8xFvYo5l4mBysybt4xB4u4d87xkt5LfS/pfTkLLOJZdKUgk+8+C77BWMeHOplVbRVX63nHHKhm\naWKGjn4YlcU/HGBi+FWHyx7KuGdme6bJNJ4AcCuAHwL4SWtt6MaHMMmgK4TYAKiO31p7rrV2IYAD\nAF4L4FXsAXzSTD9fLITYOUpW/dbaT8zsdgCXADjFzPZO3/oHADySfOcFSTN7Ur8nRxnZl/lkZ7HQ\netlbZuuMtlXlciYZo9DYXrpnVn2/T+YQk51/T9YysQ0zslkA35bsmvcCgVRmg4BaPMNsGFFNjho5\nZAF5zL3humTyPhsCVNoHcFb9M83slGn5JQAuA3AQwO0A3jHdTUkzhdggmDf+2QCuN7M9mPxQ3Nha\nu8XM7gHwJTP7KwB3Y5JRVwixATBW/f8EcFGw/QFMxvs0mVWfsZQOkqkXQMHvOwsTAnlvckWGOpnA\nDlX8eXo5OEj5LJ5eJu8zGZk5H1WkLOPslM2e+GvOZDXqJYJkYu5l+1eGdEzGHmaJeCUQC5NRaplZ\nL7nsCjFC1PGFGCFr99XvxdTrOUUwSyGzbC+ZTMoi0FSswNUQ2Zm1N1p2m8n4yK8eWC6TUMWBioln\nWM0PX1mKzYTozo6TDdMin3fm3Jjr7OkNtbJzztgWBx4hxO5CHV+IEbJjy3I9TMjiSOoz4Yoz54dl\nHG56x2QkcJbDPip7ed9zdgFyOXzkyFbZL8v11vYo2GY1Gw2zv6c3vGCGKFVpnEnz6HMGJux3LyEp\nEM9kMM+qpL4Qoos6vhAjZK1Sv7U4z72nIvWZOPnVGPez7Z0tZ8fsZUaZPY6X75mlPpL1mWU8k/Re\nXv7851vln/1sq+wDbEbDge0IcOnJpPGiiVIzGGv/ABOYs+pAlAUk7Tk/VaJFKa6+ECJFHV+IEbJj\nDjxMJJXIsslYNTM/8EVj+c/7bsQy1n4vEwep7aV4Frwyq8PLy8OHt8pPPRVvj2S/nwGoJtBknKB6\n1nmmvsw5h1nSG93bTLozsyrZ85etm4hmRFa1DiRDb3whRog6vhAjZO1SP4JxxBjKmXOOh5F6jKU0\namM2FKj6xPu2e0u6t7wPeLmeOfMwUYd83V7e+5BoXtZHVn0mVROTQqo3pPPlqgNLVd4zMfSjuj2M\nNZ1pY68+5hoy6I0vxAhZ6xvfbOttwMRCi37dmCALjPsmY1CMyoyxxpP9cjOJIIc3rn8je5fe3pz/\nbH3Z/H4v531mIM0CiFTnwHv3a1VuqkywjMhfhDl+1QDZg/E/WSYUvd74QowQdXwhRsjapf4gCRm3\nzp40qq7OyupmXC+jOpkYfh4mY09kXPMSPVvJ56V2dsxM3ldW0zFuylUXV19P1JaK8Xe2zIRgjwK6\nZEMnD/PcMuXe8Cbbtxr8xUO/8afZdO42s1umfytpphAbSkXqX4NJPP0BJc0UYkOhpL6ZHQDwewA+\nAuBPzcywQNJMs6156EUDYWSyNLM8M6vpMgleyVpSdev0+2ezA8M+TE76LOaeJxua9M6TiXOYzd17\nmJV9PQnMWLKZ2HqLDi+ZDEjVZy66/tX+Ec1GzIN9438SwAcADJf9dChpphAbC5NC6y0AnmitfXuR\nAyhpphAvPhip/3oAbzWz3wVwPICXArgOCyTNPPNMaz0HiRd+95fLyyQwZKz6kdT2ZWblVRbbL2t7\nz9rMSOdMavYk5Ww92RAgah/jtMJY5CuurB5Gdi8aiIW5z9kxs7Kn51a8irDc8+i+8Vtr17bWDrTW\nzgXwTgDfbK29C0qaKcTGsowDzwcxMfTdj8mYX0kzhdgQSg48rbVvAfjWtFxOmmkWJwVkypHUroYg\nZqzdmdV82M5YkpmAG4zPeTSTkTkK+bYyTjOZ7I9mR6qBSqor5Zh9ouMzDjmVYaQvM1I7u+dZhiPf\nFu+oFQ2NmOxG8tUXQpRQxxdihKw9vHYkHzPJ0ouXV10KmclUxrFlkNKMg0S1Xb2MQNXAIlWp3wuu\nwfiN93LczytXZmqYWRpmxqjXrszZJjsOE149u1/RkC0b0vWu1cqs+kKI3Yc6vhAjZO3LciPLOuP8\nEm1j5B3jWMKUe+3OnDmYpat+n8gRJ2u3hxmCMFb9SNZWw5ivIkS5pzpcYjL29I7FhGJnlu5m9yu7\nF4PEz0KaZ0SzZfPQG1+IEaKOL8QIWXt47Shkck9SA/FsACO1PIzU9UT1V+U9s1yzN2Rhos5UnWyY\nJI8VX/lVhbeOvss4QVWj0fSCulaTsGbrPZgl35GjmLfqM5Gb5MAjhOiiji/ECFm71I+spotKNsYh\nYpl4+9Gwgoncki3LrMrHKAJPJTBmVh/AScNe0krGYl2V+r36PMyQholl33OEWWZ9QmaRzxx0epGe\nMmevbQu2KYTYPajjCzFC1i71o2WPi0rzZZZ8MkkJI6eQZazKjA99JBOZFFbV5acVGOnKyP5ljjvA\nnAOz5Ll3vRinqeoSbSYa0nBcxtkrSqHGpunSG1+IEaKOL8QIWfuy3F5s9V40GMZ6Wo3Sk0mpyqxC\nVdL6ujNnjqFcyfI6r43MUKcXeJKxsGftzfbvXedVSe2MaKaiGumGGcZlUj+qJ4vc4+W9L29XXH0h\nxC5i7ca93kqjXqyzXi73ecdg5kB7rq/Mr7aHURk9FcO88TOqxkBP5F6d1V3NcFNZTZhd5+zcGHfo\nrJ4ozmF2nlX11TPi+jqZQC3LzOOzKbQeBPAUgOcAPNtau9jMTgNwA4BzATwI4IrW2pO1wwshdoKK\n1P+t1tqFrbWLp39/CMBtrbXzAdw2/VsIsQEsI/XfBuCN0/L1mITd/uC8L7QWG088PclUCQs9S5bY\nsjIfzxxnFXPnGVVJ51lU9i8Tz8/DrKbsuc8yxkomLl7Pj4Lxs2Di7GVDhmhFnt8nGwoyww4G9o3f\nAPyjmX3bzK6ebjurtfbotPwYgLNqhxZC7BTsG/8NrbVHzOxXANxqZj/wH7bWmpmF77bpD8XVALB/\n/1JtFUKsCKrjt9Yemf7/hJl9FZMMOo+b2dmttUfN7GwATyTf/f+kmWecYa234qtnhfUyqmrtzfbx\n9GQtE66boWIFr4YL9zDhpRn35d4xmVmVahsrdVQzz2Tfjaz6vTl3gJs9iHw0gDjoRrba0xOtQlxZ\nzD0z229mJw1lAL8N4HsAbsYkWSagpJlCbBTMG/8sAF+1yc/YXgB/11r7upndCeBGM7sKwEMArti+\nZgohVkm340+TY7462P5jAJdWDma2JVsYx5bZ7wJcnLnMIsrI3oyeSyQjnRkJHEk1RkZW3UqzdvWG\nV1m7shDp2bXNcs737kV16JZ9NwvWUckqlB2fme3w8t6H0h6+y8RqZGaYMuSyK8QIUccXYoSsPZPO\nccfx+0dWYya2GpPVhTlmJLEZuVh1bOnJ9GoAj0wuZ8OontU6i/nm8dKUGTplMwLR/a3OXmROO0xW\npWG7t7pXn6EoCea8cm+olQ1Xq+HlPXrjCzFC1PGFGCFrlfrHHAMcf/ykXLV8RvHnPIy1v5phJcqU\nU/VVZ6y9veAW1WwszBCEWU8QDa+Y6+kDRGQBRyo53z3VwB7RPZylZ9XvxccD6s9Cr72ZpK8kWJ2H\n3vhCjBB1fCFGyNqt+oM8Yfy8I+sokzTSk1mymdzqvZhmmSzNZDfjTx61cZmluJ5F62GWpTJrL5hZ\niEgOL+OcldFbOludmWCGLtmzFT1TXsZncfZ8OaprHnrjCzFC1PGFGCFrD6/dy4jTk4CMMwPjZMNY\nSqMhQCbvmCWXTFafylJYZo1DNoypBpCM9s1YZomyp5dVZpnl19Fx/P7M8JMJKZ5dzygLjid7JnvP\np6S+ECJFHV+IEbJ2qT9I36o/+yBlvERiorEw1v5KBCBft19OeeRIXO6dz+xxojLjn54NdRh/7t7M\nB+Oo4mFmLDKZHg3NsiFaVl9GxZkqcySrOOHMkg0vI5hZp2qeghfUX9tdCLEbUMcXYoTsWNJMxvmh\nImUYCephLOJRnV72+bKX95k0XHR4saq48oxPfNQuJlVTdR0AMwQb9mFmJphhH9PGSOozTmNMObPU\nL+qrv4xjl974QoyQtbvsVn6lFn3jM2qiGqAhehMwhrssTHIWUy1q+zIpuJlrkX235zLrWcbQl2W4\n6akfJkuNh1FC0XkwhrtqgJCeYTZ7hpYJvuGh3vhmdoqZ3WRmPzCzg2Z2iZmdZma3mtl90/9PXU2T\nhBDbDSv1rwPw9dbaqzCJuHsQSpopxMbSlfpmdjKA3wTwRwDQWjsK4KiZlZNmTurjtk2O9cv7MIa4\nqpsms+IuSqaYBXnwvgbZdkZqeiNhtG81yAhjGOzFvKvGFswMY1UX46juavaeytCRWW3IPGfV5zXy\nXciGhRGrdNk9D8CPAHzezO42s89MM+ooaaYQGwrT8fcCeA2AT7XWLgJwGDOyvrXWAIS/f2Z2tZnd\nZWZ3Pf30ss0VQqwCxqp/CMCh1tod079vwqTjl5NmnnXWVkbdbGVbFqNskIA9KTj7PWZ7L7yx36e6\n2i+T/Z5sNV8vpDcjRz3ZjIUnksOMJT2L/5dZ2D3ZirPoONlzk/lOZOfZi0vIuENnLOM+PJSzZ8tf\nq14o9nl03/ittccAPGxmr5xuuhTAPVDSTCE2FnYe/08AfMHM9gF4AMB7MPnRUNJMITYQquO31r4D\n4OLgo1LSTL86z8s0v8rNb4/itXl5k7k9MllaPJk8iizSjNRmHHU8i66yqs5eMMOBnkxmLPlVJ6ce\nTNzCbAjihwAVpxjGBZpxZlrUlZx5niozZLPIZVeIEaKOL8QIWauv/vPPA4cPT8qZb3VmYd+3b/J/\nFq45y9hSzWTSc0TJrMEe/z0mcEQv5ly2LTtnZkUgs2ovWivAOD5VA1R4ouEL43jkJbC/FtVMQtE9\nZa5hNgTInJZ6Q8ZKoJbZ+hj0xhdihKjjCzFC1i71h4AVzHLNyLedsaQy0rkqO6NluYyV3pM58PTC\nbveyvsyWs9zuTEAL/93oOjLLmTOLdFaPpydZs+P7tmZOLtkxe0uRmWeFCSzCBD8Z6sli/vUcktgl\n3HrjCzFC1PGFGCE7Fl47Y7DeZ+WqVdnDOLBk1vFKBqDMP51ZltpbityLEDRbXsZvPHK+YRyoGIck\nJqpMb6iRwQz7FnUgYpYzM9b2ntMYE3I+grXu640vxAhRxxdihKxV6gPxskdvvffy/rjjtsqRdZiR\nQExGEk8mpYfvVkI0A/naAybJZ/R5NlypXotKAMmqjPUwTi694QCzPiL6HlAPtT6Q3c9qSOtKGHFf\nZtabRGVZ9YUQKer4QoyQtcfVjyLpZFI/Wo6YSdqKvzuw+CwA42+9TFaf3ufV5aJVeR05wlRzFjAy\nmbGUR59n26tLZz3RPtXhTTWXQy+SU3VIIV99IUQXdXwhRsjapf4g6xkf8sjCzkR9WUbe9xwumNkA\nRvYy340+92TXLZsxYKz6UduZoJrZvcjuF3PPKxFmsiW6jNOWp3fOTM4AZp9eJB1m1mdbg20KIXYf\n6vhCjBAmhdYrAdzgNr0cwJ8D+Nvp9nMBPAjgitbak/36Jv8z/ueRfM/kZSajsyFAVbIPMDHWGX/6\njF4KK8aZp2rVrzjwZG1lAkwyjk2RTGdSeGVDRz9jVAkwyqTkYoad1XUD0bVmIidFn8+Diat/b2vt\nwtbahQB+HcDPAXwVSpopxMZSNe5dCuCHrbWHFk2a2aO3Kmk7flkrK7Wyt081tl4v+EZ2zKzuZbLH\nZMeK3ECZcOVZfVWD6tDGTGVl2YuYcNQVX4tVZQ/KXImZbENRHT1D7Nx6+EMCAN4J4IvTspJmCrGh\n0B1/mkXnrQD+fvYzNmnmM88s3E4hxAqpSP3fAfDvrbXHp3+Xk2aefrq1iktmLxCEhwk+wYSJ7hlP\nsmAajAEqk3pZnZEEZYyijMssk/M9MnQxwxXG6JbN43tpPhjmMrmezd1nSTgZw1x0zozvAnM/s3vX\nW1nJBHCpUqnmSmzJfEBJM4XYWKiOb2b7AVwG4Ctu80cBXGZm9wF48/RvIcQGwCbNPAzg9JltP0Yx\naSawePjiXoaVTPb22jGvHMlBJoYaI6MzCdybnWAs0z6uYWaFz9xao1DbWe555nwyGc9Y5Aepz4TF\nZkKHV5IhGd5zAAADv0lEQVR8ZjKeuf/ZeWZDluieZ2HOe/1GMfeEECnq+EKMkLXH3Itcdivx56pJ\nCJkVeYxjSU/qM7nXmdmDXrAMZsbCy8SjR+P9/T5egkb1+DqYWQJfnw+swuR89y62w3bGks0EFunJ\ne19mHKyY+5ydpyeaKcgs/b1nWFJfCJGiji/ECFl7II5eQIvZ/QeilWqMjK/UzdTDxJBjnHyY9vbk\nGyP7GcmaycpBgvZWTLLtqsYOjGaAsuP0AlQAtWxD1ZiMGdnsRTa86zmtMUMaBr3xhRgh6vhCjBBr\n1bi8yxzM7EcADgP4n7UddOc4AzrP3cSmnOevtdbO7O201o4PAGZ2V2vt4rUedAfQee4udtt5SuoL\nMULU8YUYITvR8T+9A8fcCXSeu4tddZ5rH+MLIXYeSX0hRshaO76ZXW5m95rZ/Wa2a8Jxm9k5Zna7\nmd1jZt83s2um208zs1vN7L7p/6fudFuXxcz2mNndZnbL9O/zzOyO6T29YRqbceMxs1PM7CYz+4GZ\nHTSzS3bT/VxbxzezPQD+GpPYfRcAuNLMLljX8beZZwG8v7V2AYDXAXjv9Nx2Y+6BawAcdH9/DMAn\nWmuvAPAkgKt2pFWr5zoAX2+tvQrAqzE5591zP1tra/kH4BIA33B/Xwvg2nUdf53/MIk/eBmAewGc\nPd12NoB7d7ptS57XAUwe+DcBuAWAYeLUsje6x5v6D8DJAP4bUxuY275r7uc6pf7LADzs/j403bar\nMLNzAVwE4A7svtwDnwTwAQDDkpDTAfyktTasKN8t9/Q8AD8C8PnpsOYz07iTu+Z+yri3QszsRABf\nBvC+1tpP/Wdt8prY2CkUM3sLgCdaa9/e6basgb0AXgPgU621izBxM3+BrN/0+7nOjv8IgHPc3wem\n23YFZnYsJp3+C621IRrx49OcA5iXe2BDeD2At5rZgwC+hIncvw7AKWY2LO/eLff0EIBDrbU7pn/f\nhMkPwa65n+vs+HcCOH9qBd6HSTqum9d4/G3DzAzAZwEcbK193H20a3IPtNauba0daK2di8m9+2Zr\n7V0AbgfwjuluG32OA621xwA8PM0UDUyiSd+DXXQ/170673cxGSfuAfC51tpH1nbwbcTM3gDgnwF8\nF1vj3w9jMs6/EcCvAngIk1Ti/7sjjVwhZvZGAH/WWnuLmb0cEwVwGoC7AfxBa+3ITrZvFZjZhQA+\nA2AfgAcAvAeTF+WuuJ/y3BNihMi4J8QIUccXYoSo4wsxQtTxhRgh6vhCjBB1fCFGiDq+ECNEHV+I\nEfJ/InHJkYH6DxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x236785d6da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1250\n",
    "\n",
    "hh_image, hv_image =  get_image_channels(X_train_initial, index)\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "print(y_train_initial[index])\n",
    "\n",
    "import cv2\n",
    "\n",
    "hh_image = cv2.bilateralFilter(hh_image.astype(np.float32), 5, 80, 80)\n",
    "hv_image = cv2.bilateralFilter(hv_image.astype(np.float32), 5, 80, 80)\n",
    "\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "\n",
    "image = np.dstack((hh_image, hh_image, np.zeros_like(hv_image)))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "X_flat_initial, y_flat_initial, _ = prepare_flat_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "print('Training dataset size: {}'.format(len(X_flat_initial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat_initial, y_flat_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 1176 samples.\n",
      "{'log_loss_test': 6.9078393044748019, 'log_loss_train': 9.9920072216264148e-16, 'acc_test': 0.80000000000000004, 'pred_time': 0.014539957046508789, 'train_time': 3.4170689582824707, 'acc_train': 1.0}\n",
      "AdaBoostClassifier trained on 1176 samples.\n",
      "{'log_loss_test': 8.3128281638591055, 'log_loss_train': 1.3815643824202646, 'acc_test': 0.7593220338983051, 'pred_time': 0.2411363124847412, 'train_time': 76.1302285194397, 'acc_train': 0.95999999999999996}\n",
      "SVC trained on 1176 samples.\n",
      "{'log_loss_test': 8.898258258600988, 'log_loss_train': 8.9802097982656104, 'acc_test': 0.74237288135593216, 'pred_time': 8.644875764846802, 'train_time': 19.407894134521484, 'acc_train': 0.73999999999999999}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "clf1= LogisticRegression(random_state = 3)\n",
    "clf2 = AdaBoostClassifier(random_state = 3)\n",
    "clf3 = SVC(random_state = 3)\n",
    "clf4 = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    start = time() # Get start time\n",
    "  \n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    X_train_pred = X_train[:300]\n",
    "    y_train_pred = y_train[:300]\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train_pred)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train_pred, predictions_train)\n",
    "    results['log_loss_train'] = log_loss(y_train_pred, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    results['log_loss_test'] = log_loss(y_test, predictions_test)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, len(X_train)))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "print(train_predict(clf1, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf2, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf3, X_train, y_train, X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-8e87bc4f6966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# TODO: Fit the grid search object to the training data and find the optimal parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrid_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Get the estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 for train, test in cv)\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1675\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "clf_default = AdaBoostClassifier(random_state = 3, base_estimator = dtc) \n",
    "clf = AdaBoostClassifier(random_state = 3, base_estimator = dtc)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = { 'n_estimators': [5, 10, 20, 50, 100], \\\n",
    "              # 'base_estimator__criterion' : ['gini', 'entropy'], \\\n",
    "               'base_estimator__max_depth': [2, 3, 5, 10, None] \\\n",
    "             #  'base_estimator__min_samples_split': [2, 5, 10, 50], \\\n",
    "             #  'base_estimator__class_weight': [{ 1 : 0.75}, { 1 : 1}] \n",
    "             } \n",
    "\n",
    "scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring = scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def_model = clf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unoptimized model\n",
      "------\n",
      "Accuracy score on testing data: 0.7254\n",
      "log_loss on testing data: 9.4837\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.7254\n",
      "Final log_loss on the testing data: 9.4837\n",
      "{'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = def_model.predict(X_valid)\n",
    "best_predictions = best_clf.predict(X_valid)\n",
    "\n",
    "print (\"Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\n",
    "print (\"log_loss on testing data: {:.4f}\".format(log_loss(y_valid, predictions)))\n",
    "\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\n",
    "print (\"Final log_loss on the testing data: {:.4f}\".format(log_loss(y_valid, best_predictions)))\n",
    "                                                                     \n",
    "print (grid_fit.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_initial, y_train_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range = 0.05, height_shift_range = 0.05, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#X_train_new = np.copy(X_train)\n",
    "#Y_train_new = np.copy(y_train)\n",
    "\n",
    "#for i in range(40):\n",
    "#    X_batch, y_batch =  next(datagen.flow(X_train, y_train, batch_size=2000))\n",
    "#    if i == 0:\n",
    "#        X_train_new = X_batch\n",
    "#        Y_train_new = y_batch\n",
    "#    else: \n",
    "#        X_train_new = np.concatenate((X_train_new, X_batch), axis=0)\n",
    "#        Y_train_new = np.concatenate((Y_train_new, y_batch), axis=0)\n",
    "    \n",
    "    #plt.imshow(X_batch[24, :, :, 0])\n",
    "    #plt.show()\n",
    "                                    \n",
    "    \n",
    "#print(X_batch.shape)\n",
    "\n",
    "\n",
    "#print (X_batch.shape)\n",
    "##X_train = cut_image_part(X_train_new, 10)\n",
    "#print (X_train_new.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexeys\\AppData\\Local\\Continuum\\Anaconda3\\envs\\aind-cv\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"av...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_initial, y_train_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "\n",
    "features_train = []\n",
    "features_valid = []\n",
    "\n",
    "for x in X_train:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_train.append(flat)\n",
    "\n",
    "for x in X_valid:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_valid.append(flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-4f38bc28e175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_train'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_loss_train'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_test'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_loss_test'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "lrc = LogisticRegression(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(features_train, y_train)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(features_valid)\n",
    "predictions_train = lrc.predict(features_train)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_valid, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_valid, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(learner.__class__.__name__, len(X_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "   \n",
    "\n",
    "entities_count = 20\n",
    "\n",
    "def getModel10(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def getModel20(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel30(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel32(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    " \n",
    "\n",
    "    \n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel31(num_layers):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel40(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False,\n",
    "                     kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def getModel50(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 75, 75, 16)        448       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 38, 38, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 38, 38, 32)        4640      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 19, 19, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 949,921\n",
      "Trainable params: 949,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel20(3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.5747Epoch 00000: val_loss improved from inf to 0.62897, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.6796 - acc: 0.5751 - val_loss: 0.6290 - val_acc: 0.6542\n",
      "Epoch 2/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.6654 - acc: 0.6057Epoch 00001: val_loss improved from 0.62897 to 0.62651, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6656 - acc: 0.6000 - val_loss: 0.6265 - val_acc: 0.6983\n",
      "Epoch 3/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.6319Epoch 00002: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.6453 - acc: 0.6380 - val_loss: 0.6327 - val_acc: 0.6271\n",
      "Epoch 4/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.6476Epoch 00003: val_loss improved from 0.62651 to 0.60634, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6366 - acc: 0.6549 - val_loss: 0.6063 - val_acc: 0.6949\n",
      "Epoch 5/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.6394 - acc: 0.6552Epoch 00004: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.6346 - acc: 0.6660 - val_loss: 0.6302 - val_acc: 0.6644\n",
      "Epoch 6/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6427 - acc: 0.6207Epoch 00005: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.6425 - acc: 0.6166 - val_loss: 0.6086 - val_acc: 0.6847\n",
      "Epoch 7/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6325 - acc: 0.6528Epoch 00006: val_loss improved from 0.60634 to 0.60262, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6297 - acc: 0.6577 - val_loss: 0.6026 - val_acc: 0.7186\n",
      "Epoch 8/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6242 - acc: 0.6806Epoch 00007: val_loss improved from 0.60262 to 0.58640, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6272 - acc: 0.6798 - val_loss: 0.5864 - val_acc: 0.7220\n",
      "Epoch 9/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6180 - acc: 0.6892Epoch 00008: val_loss improved from 0.58640 to 0.57499, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6127 - acc: 0.6945 - val_loss: 0.5750 - val_acc: 0.7525\n",
      "Epoch 10/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6052 - acc: 0.7075Epoch 00009: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.6057 - acc: 0.7075 - val_loss: 0.5787 - val_acc: 0.7017\n",
      "Epoch 11/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.6962Epoch 00010: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.6104 - acc: 0.6947 - val_loss: 0.5755 - val_acc: 0.7153\n",
      "Epoch 12/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6007 - acc: 0.6806Epoch 00011: val_loss improved from 0.57499 to 0.55612, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6101 - acc: 0.6756 - val_loss: 0.5561 - val_acc: 0.7525\n",
      "Epoch 13/1000\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 0.6246 - acc: 0.6394"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.635718). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.319360). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.6139 - acc: 0.6554Epoch 00012: val_loss improved from 0.55612 to 0.55548, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.6135 - acc: 0.6538 - val_loss: 0.5555 - val_acc: 0.7695\n",
      "Epoch 14/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.6038 - acc: 0.7014Epoch 00013: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.6013 - acc: 0.7060 - val_loss: 0.5762 - val_acc: 0.7153\n",
      "Epoch 15/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.7057Epoch 00014: val_loss improved from 0.55548 to 0.55016, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5942 - acc: 0.6974 - val_loss: 0.5502 - val_acc: 0.7424\n",
      "Epoch 16/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5792 - acc: 0.7188Epoch 00015: val_loss improved from 0.55016 to 0.53727, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5738 - acc: 0.7288 - val_loss: 0.5373 - val_acc: 0.7559\n",
      "Epoch 17/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.7300Epoch 00016: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.5787 - acc: 0.7289 - val_loss: 0.5412 - val_acc: 0.7525\n",
      "Epoch 18/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5923 - acc: 0.7135Epoch 00017: val_loss improved from 0.53727 to 0.53410, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5792 - acc: 0.7218 - val_loss: 0.5341 - val_acc: 0.7695\n",
      "Epoch 19/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5661 - acc: 0.7422Epoch 00018: val_loss improved from 0.53410 to 0.51732, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5690 - acc: 0.7383 - val_loss: 0.5173 - val_acc: 0.7831\n",
      "Epoch 20/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.5718 - acc: 0.7393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.335762). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.5694 - acc: 0.7439Epoch 00019: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.5693 - acc: 0.7421 - val_loss: 0.5352 - val_acc: 0.7763\n",
      "Epoch 21/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5824 - acc: 0.7248Epoch 00020: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.5850 - acc: 0.7155 - val_loss: 0.5205 - val_acc: 0.7797\n",
      "Epoch 22/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5484 - acc: 0.7604Epoch 00021: val_loss improved from 0.51732 to 0.49769, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5494 - acc: 0.7599 - val_loss: 0.4977 - val_acc: 0.8068\n",
      "Epoch 23/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.7517Epoch 00022: val_loss improved from 0.49769 to 0.48787, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5355 - acc: 0.7580 - val_loss: 0.4879 - val_acc: 0.8000\n",
      "Epoch 24/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7465Epoch 00023: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.5496 - acc: 0.7446 - val_loss: 0.4941 - val_acc: 0.7763\n",
      "Epoch 25/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5356 - acc: 0.7708Epoch 00024: val_loss improved from 0.48787 to 0.47473, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5353 - acc: 0.7719 - val_loss: 0.4747 - val_acc: 0.8000\n",
      "Epoch 26/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.7648Epoch 00025: val_loss improved from 0.47473 to 0.46965, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5320 - acc: 0.7598 - val_loss: 0.4696 - val_acc: 0.8102\n",
      "Epoch 27/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.7674Epoch 00026: val_loss improved from 0.46965 to 0.45587, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.5146 - acc: 0.7707 - val_loss: 0.4559 - val_acc: 0.8373\n",
      "Epoch 28/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.7804Epoch 00027: val_loss improved from 0.45587 to 0.45161, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.5024 - acc: 0.7810 - val_loss: 0.4516 - val_acc: 0.8068\n",
      "Epoch 29/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5016 - acc: 0.7726Epoch 00028: val_loss improved from 0.45161 to 0.43998, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4984 - acc: 0.7735 - val_loss: 0.4400 - val_acc: 0.8068\n",
      "Epoch 30/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.7812Epoch 00029: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4905 - acc: 0.7775 - val_loss: 0.4540 - val_acc: 0.8169\n",
      "Epoch 31/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.7509Epoch 00030: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4998 - acc: 0.7551 - val_loss: 0.4474 - val_acc: 0.8102\n",
      "Epoch 32/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4911 - acc: 0.7639Epoch 00031: val_loss improved from 0.43998 to 0.42566, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4915 - acc: 0.7632 - val_loss: 0.4257 - val_acc: 0.8102\n",
      "Epoch 33/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4761 - acc: 0.7721Epoch 00032: val_loss improved from 0.42566 to 0.41830, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.4772 - acc: 0.7724 - val_loss: 0.4183 - val_acc: 0.8237\n",
      "Epoch 34/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.7856Epoch 00033: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.4619 - acc: 0.7838 - val_loss: 0.4372 - val_acc: 0.8000\n",
      "Epoch 35/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.7795Epoch 00034: val_loss improved from 0.41830 to 0.41768, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4735 - acc: 0.7844 - val_loss: 0.4177 - val_acc: 0.8136\n",
      "Epoch 36/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4838 - acc: 0.7708Epoch 00035: val_loss improved from 0.41768 to 0.40966, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4805 - acc: 0.7761 - val_loss: 0.4097 - val_acc: 0.8136\n",
      "Epoch 37/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4588 - acc: 0.7847Epoch 00036: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4595 - acc: 0.7808 - val_loss: 0.4217 - val_acc: 0.8034\n",
      "Epoch 38/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.7873Epoch 00037: val_loss improved from 0.40966 to 0.39469, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4617 - acc: 0.7812 - val_loss: 0.3947 - val_acc: 0.8373\n",
      "Epoch 39/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.7786Epoch 00038: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4518 - acc: 0.7793 - val_loss: 0.4019 - val_acc: 0.8136\n",
      "Epoch 40/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4603 - acc: 0.7767Epoch 00039: val_loss improved from 0.39469 to 0.39344, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.4520 - acc: 0.7857 - val_loss: 0.3934 - val_acc: 0.8237\n",
      "Epoch 41/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.7743Epoch 00040: val_loss improved from 0.39344 to 0.38115, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.4470 - acc: 0.7773 - val_loss: 0.3811 - val_acc: 0.8339\n",
      "Epoch 42/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.7812Epoch 00041: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4410 - acc: 0.7839 - val_loss: 0.3828 - val_acc: 0.8339\n",
      "Epoch 43/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.7839Epoch 00042: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4573 - acc: 0.7757 - val_loss: 0.4181 - val_acc: 0.7932\n",
      "Epoch 44/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4604 - acc: 0.7786Epoch 00043: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4583 - acc: 0.7814 - val_loss: 0.4290 - val_acc: 0.7763\n",
      "Epoch 45/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4442 - acc: 0.7840Epoch 00044: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4484 - acc: 0.7797 - val_loss: 0.3938 - val_acc: 0.8136\n",
      "Epoch 46/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4458 - acc: 0.7711Epoch 00045: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4438 - acc: 0.7722 - val_loss: 0.3930 - val_acc: 0.8237\n",
      "Epoch 47/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4428 - acc: 0.7858Epoch 00046: val_loss improved from 0.38115 to 0.37502, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4476 - acc: 0.7830 - val_loss: 0.3750 - val_acc: 0.8441\n",
      "Epoch 48/1000\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 0.4404 - acc: 0.7969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.807695). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.404848). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.7804Epoch 00047: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4394 - acc: 0.7873 - val_loss: 0.3897 - val_acc: 0.8203\n",
      "Epoch 49/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.7891Epoch 00048: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4551 - acc: 0.7934 - val_loss: 0.4112 - val_acc: 0.8034\n",
      "Epoch 50/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4502 - acc: 0.7721Epoch 00049: val_loss improved from 0.37502 to 0.36411, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4497 - acc: 0.7680 - val_loss: 0.3641 - val_acc: 0.8542\n",
      "Epoch 51/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.7873Epoch 00050: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4228 - acc: 0.7939 - val_loss: 0.3845 - val_acc: 0.8508\n",
      "Epoch 52/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.7760Epoch 00051: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4462 - acc: 0.7705 - val_loss: 0.3725 - val_acc: 0.8441\n",
      "Epoch 53/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.7995Epoch 00052: val_loss improved from 0.36411 to 0.36384, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4206 - acc: 0.8012 - val_loss: 0.3638 - val_acc: 0.8475\n",
      "Epoch 54/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.7995Epoch 00053: val_loss improved from 0.36384 to 0.36225, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.4127 - acc: 0.7991 - val_loss: 0.3623 - val_acc: 0.8441\n",
      "Epoch 55/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4177 - acc: 0.8070Epoch 00054: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.4174 - acc: 0.8094 - val_loss: 0.3675 - val_acc: 0.8508\n",
      "Epoch 56/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.7943Epoch 00055: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4293 - acc: 0.7899 - val_loss: 0.3695 - val_acc: 0.8475\n",
      "Epoch 57/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.7925Epoch 00056: val_loss improved from 0.36225 to 0.36193, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4310 - acc: 0.7946 - val_loss: 0.3619 - val_acc: 0.8237\n",
      "Epoch 58/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8012Epoch 00057: val_loss improved from 0.36193 to 0.35706, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4239 - acc: 0.7922 - val_loss: 0.3571 - val_acc: 0.8407\n",
      "Epoch 59/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8064Epoch 00058: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4214 - acc: 0.8035 - val_loss: 0.3693 - val_acc: 0.8542\n",
      "Epoch 60/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8212Epoch 00059: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4184 - acc: 0.8197 - val_loss: 0.3981 - val_acc: 0.7966\n",
      "Epoch 61/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4303 - acc: 0.7995Epoch 00060: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4288 - acc: 0.7991 - val_loss: 0.3830 - val_acc: 0.8475\n",
      "Epoch 62/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8168Epoch 00061: val_loss improved from 0.35706 to 0.34725, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3977 - acc: 0.8219 - val_loss: 0.3472 - val_acc: 0.8576\n",
      "Epoch 63/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8038Epoch 00062: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4031 - acc: 0.7968 - val_loss: 0.3666 - val_acc: 0.8610\n",
      "Epoch 64/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8038Epoch 00063: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4139 - acc: 0.8011 - val_loss: 0.3484 - val_acc: 0.8610\n",
      "Epoch 65/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.7960Epoch 00064: val_loss improved from 0.34725 to 0.34533, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4083 - acc: 0.8000 - val_loss: 0.3453 - val_acc: 0.8475\n",
      "Epoch 66/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8090Epoch 00065: val_loss improved from 0.34533 to 0.33613, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.4036 - acc: 0.8081 - val_loss: 0.3361 - val_acc: 0.8407\n",
      "Epoch 67/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.7899Epoch 00066: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4048 - acc: 0.7964 - val_loss: 0.3403 - val_acc: 0.8407\n",
      "Epoch 68/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.7934Epoch 00067: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4137 - acc: 0.7933 - val_loss: 0.3489 - val_acc: 0.8576\n",
      "Epoch 69/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.4097 - acc: 0.8134Epoch 00068: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.4106 - acc: 0.8116 - val_loss: 0.3406 - val_acc: 0.8576\n",
      "Epoch 70/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8099Epoch 00069: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3986 - acc: 0.8068 - val_loss: 0.3557 - val_acc: 0.8237\n",
      "Epoch 71/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4077 - acc: 0.8116Epoch 00070: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4056 - acc: 0.8127 - val_loss: 0.3362 - val_acc: 0.8508\n",
      "Epoch 72/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8099Epoch 00071: val_loss improved from 0.33613 to 0.33234, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3950 - acc: 0.8111 - val_loss: 0.3323 - val_acc: 0.8542\n",
      "Epoch 73/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8125Epoch 00072: val_loss improved from 0.33234 to 0.32878, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3994 - acc: 0.8114 - val_loss: 0.3288 - val_acc: 0.8508\n",
      "Epoch 74/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.7995Epoch 00073: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4007 - acc: 0.8055 - val_loss: 0.3418 - val_acc: 0.8407\n",
      "Epoch 75/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8203Epoch 00074: val_loss improved from 0.32878 to 0.32812, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3802 - acc: 0.8252 - val_loss: 0.3281 - val_acc: 0.8644\n",
      "Epoch 76/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3869 - acc: 0.8244Epoch 00075: val_loss improved from 0.32812 to 0.32292, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3779 - acc: 0.8302 - val_loss: 0.3229 - val_acc: 0.8508\n",
      "Epoch 77/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3795 - acc: 0.8070Epoch 00076: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3967 - acc: 0.8020 - val_loss: 0.3312 - val_acc: 0.8576\n",
      "Epoch 78/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8090Epoch 00077: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3980 - acc: 0.8060 - val_loss: 0.3380 - val_acc: 0.8305\n",
      "Epoch 79/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8116Epoch 00078: val_loss improved from 0.32292 to 0.31561, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3964 - acc: 0.8106 - val_loss: 0.3156 - val_acc: 0.8542\n",
      "Epoch 80/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8212Epoch 00079: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3759 - acc: 0.8239 - val_loss: 0.3172 - val_acc: 0.8644\n",
      "Epoch 81/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8125Epoch 00080: val_loss improved from 0.31561 to 0.31029, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3787 - acc: 0.8136 - val_loss: 0.3103 - val_acc: 0.8712\n",
      "Epoch 82/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8142Epoch 00081: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3775 - acc: 0.8195 - val_loss: 0.3250 - val_acc: 0.8712\n",
      "Epoch 83/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8264Epoch 00082: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3745 - acc: 0.8289 - val_loss: 0.3295 - val_acc: 0.8271\n",
      "Epoch 84/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3749 - acc: 0.8105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.239000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8151Epoch 00083: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3782 - acc: 0.8139 - val_loss: 0.3220 - val_acc: 0.8576\n",
      "Epoch 85/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.7951Epoch 00084: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.4177 - acc: 0.7950 - val_loss: 0.3186 - val_acc: 0.8475\n",
      "Epoch 86/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8194Epoch 00085: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3861 - acc: 0.8116 - val_loss: 0.3107 - val_acc: 0.8542\n",
      "Epoch 87/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8194Epoch 00086: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3888 - acc: 0.8159 - val_loss: 0.3221 - val_acc: 0.8542\n",
      "Epoch 88/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8212Epoch 00087: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3702 - acc: 0.8239 - val_loss: 0.3151 - val_acc: 0.8678\n",
      "Epoch 89/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8160Epoch 00088: val_loss improved from 0.31029 to 0.30922, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3952 - acc: 0.8147 - val_loss: 0.3092 - val_acc: 0.8542\n",
      "Epoch 90/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3747 - acc: 0.8238Epoch 00089: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3842 - acc: 0.8200 - val_loss: 0.3190 - val_acc: 0.8847\n",
      "Epoch 91/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3835 - acc: 0.8086Epoch 00090: val_loss improved from 0.30922 to 0.30891, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3778 - acc: 0.8137 - val_loss: 0.3089 - val_acc: 0.8475\n",
      "Epoch 92/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8108Epoch 00091: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3911 - acc: 0.8077 - val_loss: 0.3387 - val_acc: 0.8339\n",
      "Epoch 93/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8281Epoch 00092: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3836 - acc: 0.8241 - val_loss: 0.3163 - val_acc: 0.8576\n",
      "Epoch 94/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8203Epoch 00093: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3818 - acc: 0.8167 - val_loss: 0.3129 - val_acc: 0.8542\n",
      "Epoch 95/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8134Epoch 00094: val_loss improved from 0.30891 to 0.30164, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3916 - acc: 0.8165 - val_loss: 0.3016 - val_acc: 0.8644\n",
      "Epoch 96/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3622 - acc: 0.8309Epoch 00095: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3679 - acc: 0.8250 - val_loss: 0.3085 - val_acc: 0.8814\n",
      "Epoch 97/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8351Epoch 00096: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3662 - acc: 0.8350 - val_loss: 0.3037 - val_acc: 0.8475\n",
      "Epoch 98/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8134Epoch 00097: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3872 - acc: 0.8059 - val_loss: 0.3075 - val_acc: 0.8475\n",
      "Epoch 99/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8264Epoch 00098: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3832 - acc: 0.8204 - val_loss: 0.3065 - val_acc: 0.8542\n",
      "Epoch 100/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8229Epoch 00099: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3652 - acc: 0.8298 - val_loss: 0.3032 - val_acc: 0.8576\n",
      "Epoch 101/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8307Epoch 00100: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3639 - acc: 0.8372 - val_loss: 0.3016 - val_acc: 0.8610\n",
      "Epoch 102/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.8177Epoch 00101: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3721 - acc: 0.8185 - val_loss: 0.3126 - val_acc: 0.8644\n",
      "Epoch 103/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8212Epoch 00102: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3649 - acc: 0.8239 - val_loss: 0.3077 - val_acc: 0.8508\n",
      "Epoch 104/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8351Epoch 00103: val_loss improved from 0.30164 to 0.29950, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3694 - acc: 0.8350 - val_loss: 0.2995 - val_acc: 0.8610\n",
      "Epoch 105/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3736 - acc: 0.8203Epoch 00104: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3795 - acc: 0.8167 - val_loss: 0.3041 - val_acc: 0.8508\n",
      "Epoch 106/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8116Epoch 00105: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3722 - acc: 0.8191 - val_loss: 0.3055 - val_acc: 0.8576\n",
      "Epoch 107/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8255Epoch 00106: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3837 - acc: 0.8174 - val_loss: 0.3015 - val_acc: 0.8746\n",
      "Epoch 108/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8194Epoch 00107: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3798 - acc: 0.8223 - val_loss: 0.3343 - val_acc: 0.8644\n",
      "Epoch 109/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3635 - acc: 0.8273Epoch 00108: val_loss improved from 0.29950 to 0.29604, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3689 - acc: 0.8233 - val_loss: 0.2960 - val_acc: 0.8610\n",
      "Epoch 110/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8281Epoch 00109: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3713 - acc: 0.8284 - val_loss: 0.3025 - val_acc: 0.8542\n",
      "Epoch 111/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8238Epoch 00110: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3723 - acc: 0.8221 - val_loss: 0.3042 - val_acc: 0.8746\n",
      "Epoch 112/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8255Epoch 00111: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3844 - acc: 0.8238 - val_loss: 0.3382 - val_acc: 0.8441\n",
      "Epoch 113/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8229Epoch 00112: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3745 - acc: 0.8256 - val_loss: 0.3014 - val_acc: 0.8678\n",
      "Epoch 114/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8281Epoch 00113: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3523 - acc: 0.8284 - val_loss: 0.2984 - val_acc: 0.8610\n",
      "Epoch 115/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8368Epoch 00114: val_loss improved from 0.29604 to 0.29494, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3536 - acc: 0.8345 - val_loss: 0.2949 - val_acc: 0.8610\n",
      "Epoch 116/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8411Epoch 00115: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3512 - acc: 0.8429 - val_loss: 0.3043 - val_acc: 0.8881\n",
      "Epoch 117/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8299Epoch 00116: val_loss improved from 0.29494 to 0.28841, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3566 - acc: 0.8215 - val_loss: 0.2884 - val_acc: 0.8644\n",
      "Epoch 118/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3547 - acc: 0.8272Epoch 00117: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3581 - acc: 0.8300 - val_loss: 0.2957 - val_acc: 0.8678\n",
      "Epoch 119/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8238Epoch 00118: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3627 - acc: 0.8285 - val_loss: 0.2905 - val_acc: 0.8712\n",
      "Epoch 120/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8429Epoch 00119: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3643 - acc: 0.8360 - val_loss: 0.2915 - val_acc: 0.8542\n",
      "Epoch 121/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8273Epoch 00120: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3673 - acc: 0.8254 - val_loss: 0.2908 - val_acc: 0.8847\n",
      "Epoch 122/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3659 - acc: 0.8342Epoch 00121: val_loss improved from 0.28841 to 0.28834, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.3617 - acc: 0.8384 - val_loss: 0.2883 - val_acc: 0.8678\n",
      "Epoch 123/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8238Epoch 00122: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3632 - acc: 0.8264 - val_loss: 0.3056 - val_acc: 0.8610\n",
      "Epoch 124/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8220Epoch 00123: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3591 - acc: 0.8226 - val_loss: 0.3075 - val_acc: 0.8542\n",
      "Epoch 125/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8273Epoch 00124: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3586 - acc: 0.8318 - val_loss: 0.3129 - val_acc: 0.8644\n",
      "Epoch 126/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8264Epoch 00125: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3778 - acc: 0.8246 - val_loss: 0.2957 - val_acc: 0.8881\n",
      "Epoch 127/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8177Epoch 00126: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3758 - acc: 0.8206 - val_loss: 0.3046 - val_acc: 0.8746\n",
      "Epoch 128/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8273Epoch 00127: val_loss improved from 0.28834 to 0.28686, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3763 - acc: 0.8233 - val_loss: 0.2869 - val_acc: 0.8610\n",
      "Epoch 129/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8316Epoch 00128: val_loss improved from 0.28686 to 0.28400, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3511 - acc: 0.8338 - val_loss: 0.2840 - val_acc: 0.8814\n",
      "Epoch 130/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8333Epoch 00129: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3664 - acc: 0.8248 - val_loss: 0.2969 - val_acc: 0.8576\n",
      "Epoch 131/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8203Epoch 00130: val_loss improved from 0.28400 to 0.28144, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3675 - acc: 0.8167 - val_loss: 0.2814 - val_acc: 0.8780\n",
      "Epoch 132/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8307Epoch 00131: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3722 - acc: 0.8245 - val_loss: 0.2875 - val_acc: 0.8678\n",
      "Epoch 133/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8307Epoch 00132: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3581 - acc: 0.8287 - val_loss: 0.2875 - val_acc: 0.8644\n",
      "Epoch 134/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8273Epoch 00133: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3544 - acc: 0.8318 - val_loss: 0.3153 - val_acc: 0.8407\n",
      "Epoch 135/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8229Epoch 00134: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3644 - acc: 0.8192 - val_loss: 0.2842 - val_acc: 0.8644\n",
      "Epoch 136/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.3581 - acc: 0.8406Epoch 00135: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3773 - acc: 0.8257 - val_loss: 0.3069 - val_acc: 0.8712\n",
      "Epoch 137/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8177Epoch 00136: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3583 - acc: 0.8206 - val_loss: 0.2895 - val_acc: 0.8678\n",
      "Epoch 138/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8134Epoch 00137: val_loss improved from 0.28144 to 0.27336, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3729 - acc: 0.8186 - val_loss: 0.2734 - val_acc: 0.8847\n",
      "Epoch 139/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8316Epoch 00138: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3603 - acc: 0.8317 - val_loss: 0.2812 - val_acc: 0.8746\n",
      "Epoch 140/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8377Epoch 00139: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3393 - acc: 0.8438 - val_loss: 0.2812 - val_acc: 0.8678\n",
      "Epoch 141/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3599 - acc: 0.8401Epoch 00140: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3524 - acc: 0.8409 - val_loss: 0.2840 - val_acc: 0.8915\n",
      "Epoch 142/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8377Epoch 00141: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3518 - acc: 0.8417 - val_loss: 0.2934 - val_acc: 0.8508\n",
      "Epoch 143/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3412 - acc: 0.8379Epoch 00142: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3461 - acc: 0.8342 - val_loss: 0.2879 - val_acc: 0.8915\n",
      "Epoch 144/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8377Epoch 00143: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3539 - acc: 0.8417 - val_loss: 0.2819 - val_acc: 0.8746\n",
      "Epoch 145/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8299Epoch 00144: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3589 - acc: 0.8300 - val_loss: 0.2834 - val_acc: 0.8678\n",
      "Epoch 146/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8307Epoch 00145: val_loss improved from 0.27336 to 0.27195, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3409 - acc: 0.8351 - val_loss: 0.2720 - val_acc: 0.8712\n",
      "Epoch 147/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8420Epoch 00146: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/18 [===============================] - 1s - loss: 0.3482 - acc: 0.8437 - val_loss: 0.2767 - val_acc: 0.8644\n",
      "Epoch 148/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8290Epoch 00147: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3488 - acc: 0.8335 - val_loss: 0.2737 - val_acc: 0.8780\n",
      "Epoch 149/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8229Epoch 00148: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3499 - acc: 0.8256 - val_loss: 0.2724 - val_acc: 0.8814\n",
      "Epoch 150/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3637 - acc: 0.8272Epoch 00149: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3597 - acc: 0.8276 - val_loss: 0.3341 - val_acc: 0.8305\n",
      "Epoch 151/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3683 - acc: 0.8134Epoch 00150: val_loss improved from 0.27195 to 0.26991, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.3589 - acc: 0.8203 - val_loss: 0.2699 - val_acc: 0.8881\n",
      "Epoch 152/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8368Epoch 00151: val_loss improved from 0.26991 to 0.26512, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3497 - acc: 0.8366 - val_loss: 0.2651 - val_acc: 0.8915\n",
      "Epoch 153/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8420Epoch 00152: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3540 - acc: 0.8394 - val_loss: 0.2856 - val_acc: 0.8881\n",
      "Epoch 154/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8325Epoch 00153: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3700 - acc: 0.8304 - val_loss: 0.2738 - val_acc: 0.8746\n",
      "Epoch 155/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8377Epoch 00154: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3629 - acc: 0.8268 - val_loss: 0.2702 - val_acc: 0.8881\n",
      "Epoch 156/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3586 - acc: 0.8247Epoch 00155: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3530 - acc: 0.8272 - val_loss: 0.2692 - val_acc: 0.8814\n",
      "Epoch 157/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8394Epoch 00156: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3397 - acc: 0.8434 - val_loss: 0.2678 - val_acc: 0.8881\n",
      "Epoch 158/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8368Epoch 00157: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3407 - acc: 0.8303 - val_loss: 0.2760 - val_acc: 0.8847\n",
      "Epoch 159/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3404 - acc: 0.8447Epoch 00158: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3412 - acc: 0.8399 - val_loss: 0.2817 - val_acc: 0.8576\n",
      "Epoch 160/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8273Epoch 00159: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3604 - acc: 0.8233 - val_loss: 0.2774 - val_acc: 0.8644\n",
      "Epoch 161/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8455Epoch 00160: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3452 - acc: 0.8449 - val_loss: 0.2689 - val_acc: 0.8881\n",
      "Epoch 162/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8394Epoch 00161: val_loss improved from 0.26512 to 0.26302, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3527 - acc: 0.8370 - val_loss: 0.2630 - val_acc: 0.8915\n",
      "Epoch 163/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8290- ETA: 0s - loss: 0.3457 - acc: 0.8Epoch 00162: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3452 - acc: 0.8313 - val_loss: 0.2645 - val_acc: 0.8746\n",
      "Epoch 164/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8559Epoch 00163: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3277 - acc: 0.8633 - val_loss: 0.2657 - val_acc: 0.8746\n",
      "Epoch 165/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8420Epoch 00164: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3337 - acc: 0.8437 - val_loss: 0.2716 - val_acc: 0.8847\n",
      "Epoch 166/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3403 - acc: 0.8438Epoch 00165: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3335 - acc: 0.8462 - val_loss: 0.2726 - val_acc: 0.8881\n",
      "Epoch 167/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8498Epoch 00166: val_loss improved from 0.26302 to 0.26263, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3509 - acc: 0.8490 - val_loss: 0.2626 - val_acc: 0.8881\n",
      "Epoch 168/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8611Epoch 00167: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3215 - acc: 0.8576 - val_loss: 0.2699 - val_acc: 0.8746\n",
      "Epoch 169/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8385Epoch 00168: val_loss improved from 0.26263 to 0.26165, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3432 - acc: 0.8383 - val_loss: 0.2616 - val_acc: 0.8712\n",
      "Epoch 170/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8507Epoch 00169: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3459 - acc: 0.8498 - val_loss: 0.2760 - val_acc: 0.8678\n",
      "Epoch 171/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8385Epoch 00170: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3377 - acc: 0.8383 - val_loss: 0.2746 - val_acc: 0.8780\n",
      "Epoch 172/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3447 - acc: 0.8401Epoch 00171: val_loss improved from 0.26165 to 0.25876, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3472 - acc: 0.8383 - val_loss: 0.2588 - val_acc: 0.8881\n",
      "Epoch 173/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8559Epoch 00172: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3351 - acc: 0.8611 - val_loss: 0.2662 - val_acc: 0.8678\n",
      "Epoch 174/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8411Epoch 00173: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3384 - acc: 0.8407 - val_loss: 0.2665 - val_acc: 0.8881\n",
      "Epoch 175/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8429Epoch 00174: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3312 - acc: 0.8466 - val_loss: 0.2592 - val_acc: 0.8746\n",
      "Epoch 176/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8446Epoch 00175: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3390 - acc: 0.8462 - val_loss: 0.2702 - val_acc: 0.8915\n",
      "Epoch 177/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8455Epoch 00176: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3392 - acc: 0.8512 - val_loss: 0.2601 - val_acc: 0.8847\n",
      "Epoch 178/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8368Epoch 00177: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3286 - acc: 0.8430 - val_loss: 0.2817 - val_acc: 0.8983\n",
      "Epoch 179/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8351Epoch 00178: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/18 [===============================] - 2s - loss: 0.3379 - acc: 0.8329 - val_loss: 0.2659 - val_acc: 0.8780\n",
      "Epoch 180/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3318 - acc: 0.8465Epoch 00179: val_loss improved from 0.25876 to 0.25802, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.3364 - acc: 0.8460 - val_loss: 0.2580 - val_acc: 0.8712\n",
      "Epoch 181/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3302 - acc: 0.8575Epoch 00180: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3224 - acc: 0.8602 - val_loss: 0.2616 - val_acc: 0.8814\n",
      "Epoch 182/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8403Epoch 00181: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3443 - acc: 0.8399 - val_loss: 0.2696 - val_acc: 0.8780\n",
      "Epoch 183/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8281Epoch 00182: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3736 - acc: 0.8241 - val_loss: 0.2635 - val_acc: 0.8915\n",
      "Epoch 184/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8394Epoch 00183: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3491 - acc: 0.8306 - val_loss: 0.2588 - val_acc: 0.8847\n",
      "Epoch 185/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3436 - acc: 0.8429Epoch 00184: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3349 - acc: 0.8488 - val_loss: 0.2623 - val_acc: 0.8780\n",
      "Epoch 186/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8550Epoch 00185: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3312 - acc: 0.8497 - val_loss: 0.2583 - val_acc: 0.8881\n",
      "Epoch 187/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8429Epoch 00186: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3364 - acc: 0.8466 - val_loss: 0.2608 - val_acc: 0.8780\n",
      "Epoch 188/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8498Epoch 00187: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3415 - acc: 0.8469 - val_loss: 0.2652 - val_acc: 0.8746\n",
      "Epoch 189/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3508 - acc: 0.8456Epoch 00188: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3528 - acc: 0.8383 - val_loss: 0.2735 - val_acc: 0.8881\n",
      "Epoch 190/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8368Epoch 00189: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3467 - acc: 0.8366 - val_loss: 0.2759 - val_acc: 0.8610\n",
      "Epoch 191/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8394Epoch 00190: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3418 - acc: 0.8348 - val_loss: 0.2620 - val_acc: 0.8746\n",
      "Epoch 192/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8446Epoch 00191: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3349 - acc: 0.8483 - val_loss: 0.2677 - val_acc: 0.8610\n",
      "Epoch 193/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8559Epoch 00192: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3297 - acc: 0.8569 - val_loss: 0.2628 - val_acc: 0.8780\n",
      "Epoch 194/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.3214 - acc: 0.8458Epoch 00193: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3316 - acc: 0.8450 - val_loss: 0.2597 - val_acc: 0.8814\n",
      "Epoch 195/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3421 - acc: 0.8474Epoch 00194: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3382 - acc: 0.8495 - val_loss: 0.2639 - val_acc: 0.8847\n",
      "Epoch 196/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3357 - acc: 0.8457Epoch 00195: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3426 - acc: 0.8450 - val_loss: 0.2863 - val_acc: 0.8712\n",
      "Epoch 197/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3508 - acc: 0.8382Epoch 00196: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3492 - acc: 0.8363 - val_loss: 0.2826 - val_acc: 0.8576\n",
      "Epoch 198/1000\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 0.3465 - acc: 0.8359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.841502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.421751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8455Epoch 00197: val_loss improved from 0.25802 to 0.25291, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3241 - acc: 0.8491 - val_loss: 0.2529 - val_acc: 0.8949\n",
      "Epoch 199/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3313 - acc: 0.8401Epoch 00198: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3465 - acc: 0.8294 - val_loss: 0.2606 - val_acc: 0.8983\n",
      "Epoch 200/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.106750). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00199: val_loss improved from 0.25291 to 0.25085, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3255 - acc: 0.8512 - val_loss: 0.2509 - val_acc: 0.8983\n",
      "Epoch 201/1000\n",
      " 2/18 [==>...........................] - ETA: 8s - loss: 0.3211 - acc: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.969118). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.485560). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/18 [=======================>......] - ETA: 0s - loss: 0.3323 - acc: 0.8458Epoch 00200: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3318 - acc: 0.8493 - val_loss: 0.2859 - val_acc: 0.8508\n",
      "Epoch 202/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3515 - acc: 0.8368Epoch 00201: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3444 - acc: 0.8388 - val_loss: 0.2624 - val_acc: 0.8983\n",
      "Epoch 203/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8490Epoch 00202: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3347 - acc: 0.8524 - val_loss: 0.2555 - val_acc: 0.8881\n",
      "Epoch 204/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8533Epoch 00203: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3262 - acc: 0.8544 - val_loss: 0.2790 - val_acc: 0.8678\n",
      "Epoch 205/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.3359 - acc: 0.8395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.941148). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.471074). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8377Epoch 00204: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3424 - acc: 0.8353 - val_loss: 0.2650 - val_acc: 0.8746\n",
      "Epoch 206/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8533Epoch 00205: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3317 - acc: 0.8544 - val_loss: 0.2737 - val_acc: 0.8780\n",
      "Epoch 207/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8420Epoch 00206: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3428 - acc: 0.8416 - val_loss: 0.2604 - val_acc: 0.8915\n",
      "Epoch 208/1000\n",
      "11/18 [================>.............] - ETA: 1s - loss: 0.3556 - acc: 0.8196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.539037). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.356920). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.224710). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.8333Epoch 00207: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3392 - acc: 0.8333 - val_loss: 0.2677 - val_acc: 0.8780\n",
      "Epoch 209/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8542Epoch 00208: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3174 - acc: 0.8574 - val_loss: 0.2570 - val_acc: 0.8847\n",
      "Epoch 210/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8429Epoch 00209: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3314 - acc: 0.8466 - val_loss: 0.2771 - val_acc: 0.8678\n",
      "Epoch 211/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8516Epoch 00210: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3207 - acc: 0.8549 - val_loss: 0.2707 - val_acc: 0.8780\n",
      "Epoch 212/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8446Epoch 00211: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3285 - acc: 0.8483 - val_loss: 0.2546 - val_acc: 0.8847\n",
      "Epoch 213/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.3407 - acc: 0.8469Epoch 00212: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3456 - acc: 0.8472 - val_loss: 0.2546 - val_acc: 0.8881\n",
      "Epoch 214/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.3495 - acc: 0.8338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.987073). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.506538). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8498Epoch 00213: val_loss improved from 0.25085 to 0.24882, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3347 - acc: 0.8490 - val_loss: 0.2488 - val_acc: 0.8949\n",
      "Epoch 215/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8359Epoch 00214: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3407 - acc: 0.8379 - val_loss: 0.2564 - val_acc: 0.8983\n",
      "Epoch 216/1000\n",
      "10/18 [===============>..............] - ETA: 1s - loss: 0.3115 - acc: 0.8547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.985502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.649567). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.313632). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.157817). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8455Epoch 00215: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3387 - acc: 0.8406 - val_loss: 0.2610 - val_acc: 0.8814\n",
      "Epoch 217/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8429Epoch 00216: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3495 - acc: 0.8381 - val_loss: 0.2558 - val_acc: 0.8746\n",
      "Epoch 218/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8446Epoch 00217: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3196 - acc: 0.8483 - val_loss: 0.2568 - val_acc: 0.8881\n",
      "Epoch 219/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8368Epoch 00218: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3439 - acc: 0.8388 - val_loss: 0.2748 - val_acc: 0.8712\n",
      "Epoch 220/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8438Epoch 00219: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3310 - acc: 0.8496 - val_loss: 0.2561 - val_acc: 0.8881\n",
      "Epoch 221/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.3193 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.165999). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.330999). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.200249). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8594Epoch 00220: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3350 - acc: 0.8559 - val_loss: 0.2686 - val_acc: 0.8847\n",
      "Epoch 222/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8490Epoch 00221: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3259 - acc: 0.8482 - val_loss: 0.2526 - val_acc: 0.8814\n",
      "Epoch 223/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8542Epoch 00222: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3285 - acc: 0.8552 - val_loss: 0.2539 - val_acc: 0.8780\n",
      "Epoch 224/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.8602Epoch 00223: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3269 - acc: 0.8589 - val_loss: 0.2539 - val_acc: 0.8881\n",
      "Epoch 225/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8568Epoch 00224: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3333 - acc: 0.8513 - val_loss: 0.2571 - val_acc: 0.8915\n",
      "Epoch 226/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3186 - acc: 0.8594Epoch 00225: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3234 - acc: 0.8561 - val_loss: 0.2559 - val_acc: 0.8949\n",
      "Epoch 227/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8420Epoch 00226: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3264 - acc: 0.8479 - val_loss: 0.2969 - val_acc: 0.8712\n",
      "Epoch 228/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.8411Epoch 00227: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3213 - acc: 0.8386 - val_loss: 0.2489 - val_acc: 0.8949\n",
      "Epoch 229/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.8394Epoch 00228: val_loss improved from 0.24882 to 0.24669, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3346 - acc: 0.8476 - val_loss: 0.2467 - val_acc: 0.8915\n",
      "Epoch 230/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3033 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.130751). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.142502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3109 - acc: 0.8741Epoch 00229: val_loss improved from 0.24669 to 0.24474, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3149 - acc: 0.8678 - val_loss: 0.2447 - val_acc: 0.8881\n",
      "Epoch 231/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8533Epoch 00230: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3201 - acc: 0.8544 - val_loss: 0.2459 - val_acc: 0.9051\n",
      "Epoch 232/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8533Epoch 00231: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3322 - acc: 0.8459 - val_loss: 0.2451 - val_acc: 0.8949\n",
      "Epoch 233/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8611Epoch 00232: val_loss improved from 0.24474 to 0.24280, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3244 - acc: 0.8576 - val_loss: 0.2428 - val_acc: 0.9051\n",
      "Epoch 234/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8516Epoch 00233: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3350 - acc: 0.8485 - val_loss: 0.2622 - val_acc: 0.8814\n",
      "Epoch 235/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8333Epoch 00234: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3452 - acc: 0.8355 - val_loss: 0.2554 - val_acc: 0.8814\n",
      "Epoch 236/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.3656 - acc: 0.8316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.831388). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.416195). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8507Epoch 00235: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3338 - acc: 0.8519 - val_loss: 0.2798 - val_acc: 0.8610\n",
      "Epoch 237/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8351Epoch 00236: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3399 - acc: 0.8329 - val_loss: 0.2835 - val_acc: 0.8441\n",
      "Epoch 238/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3376 - acc: 0.8389Epoch 00237: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3407 - acc: 0.8299 - val_loss: 0.2452 - val_acc: 0.9017\n",
      "Epoch 239/1000\n",
      " 9/18 [=============>................] - ETA: 0s - loss: 0.3248 - acc: 0.855"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.796062). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.398781). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8524Epoch 00238: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3339 - acc: 0.8472 - val_loss: 0.2471 - val_acc: 0.8847\n",
      "Epoch 240/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.8611Epoch 00239: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3174 - acc: 0.8618 - val_loss: 0.2568 - val_acc: 0.8915\n",
      "Epoch 241/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8559Epoch 00240: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3113 - acc: 0.8611 - val_loss: 0.2479 - val_acc: 0.8881\n",
      "Epoch 242/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8498Epoch 00241: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3318 - acc: 0.8511 - val_loss: 0.2563 - val_acc: 0.8881\n",
      "Epoch 243/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8524Epoch 00242: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3323 - acc: 0.8578 - val_loss: 0.2442 - val_acc: 0.8847\n",
      "Epoch 244/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8533Epoch 00243: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3153 - acc: 0.8587 - val_loss: 0.2671 - val_acc: 0.8610\n",
      "Epoch 245/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.3288 - acc: 0.8625Epoch 00244: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3290 - acc: 0.8557 - val_loss: 0.2467 - val_acc: 0.8915\n",
      "Epoch 246/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8481Epoch 00245: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3370 - acc: 0.8431 - val_loss: 0.2562 - val_acc: 0.8881\n",
      "Epoch 247/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3462 - acc: 0.8252Epoch 00246: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3511 - acc: 0.8297 - val_loss: 0.2617 - val_acc: 0.8847\n",
      "Epoch 248/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8533Epoch 00247: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3199 - acc: 0.8544 - val_loss: 0.2562 - val_acc: 0.8814\n",
      "Epoch 249/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8559Epoch 00248: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3222 - acc: 0.8569 - val_loss: 0.2648 - val_acc: 0.8780\n",
      "Epoch 250/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8576Epoch 00249: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3263 - acc: 0.8585 - val_loss: 0.2451 - val_acc: 0.8983\n",
      "Epoch 251/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8646Epoch 00250: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3326 - acc: 0.8566 - val_loss: 0.2568 - val_acc: 0.8746\n",
      "Epoch 252/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8438Epoch 00251: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3201 - acc: 0.8432 - val_loss: 0.2428 - val_acc: 0.9017\n",
      "Epoch 253/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.8655Epoch 00252: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3257 - acc: 0.8617 - val_loss: 0.2474 - val_acc: 0.8881\n",
      "Epoch 254/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3154 - acc: 0.8652Epoch 00253: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3107 - acc: 0.8661 - val_loss: 0.2525 - val_acc: 0.8881\n",
      "Epoch 255/1000\n",
      "13/18 [====================>.........] - ETA: 0s - loss: 0.3219 - acc: 0.8510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.982303). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.656154). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.330006). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.165753). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8550Epoch 00254: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3230 - acc: 0.8497 - val_loss: 0.2543 - val_acc: 0.8780\n",
      "Epoch 256/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8507Epoch 00255: val_loss improved from 0.24280 to 0.24156, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3228 - acc: 0.8583 - val_loss: 0.2416 - val_acc: 0.9051\n",
      "Epoch 257/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8481Epoch 00256: val_loss improved from 0.24156 to 0.24050, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3168 - acc: 0.8473 - val_loss: 0.2405 - val_acc: 0.8983\n",
      "Epoch 258/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8429Epoch 00257: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3306 - acc: 0.8445 - val_loss: 0.2486 - val_acc: 0.8949\n",
      "Epoch 259/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8472Epoch 00258: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3209 - acc: 0.8508 - val_loss: 0.2437 - val_acc: 0.8847\n",
      "Epoch 260/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8576Epoch 00259: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3121 - acc: 0.8628 - val_loss: 0.2607 - val_acc: 0.8712\n",
      "Epoch 261/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8533Epoch 00260: val_loss improved from 0.24050 to 0.23906, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3306 - acc: 0.8502 - val_loss: 0.2391 - val_acc: 0.8949\n",
      "Epoch 262/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3037 - acc: 0.8585Epoch 00261: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3106 - acc: 0.8530 - val_loss: 0.2539 - val_acc: 0.8712\n",
      "Epoch 263/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8559Epoch 00262: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3250 - acc: 0.8548 - val_loss: 0.2431 - val_acc: 0.9051\n",
      "Epoch 264/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8472Epoch 00263: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3320 - acc: 0.8423 - val_loss: 0.2805 - val_acc: 0.8542\n",
      "Epoch 265/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8542Epoch 00264: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3125 - acc: 0.8574 - val_loss: 0.2450 - val_acc: 0.8983\n",
      "Epoch 266/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8524Epoch 00265: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3093 - acc: 0.8557 - val_loss: 0.2450 - val_acc: 0.8983\n",
      "Epoch 267/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.8655Epoch 00266: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3169 - acc: 0.8617 - val_loss: 0.2438 - val_acc: 0.8983\n",
      "Epoch 268/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3170 - acc: 0.8507Epoch 00267: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3245 - acc: 0.8456 - val_loss: 0.2407 - val_acc: 0.9017\n",
      "Epoch 269/1000\n",
      "11/18 [================>.............] - ETA: 1s - loss: 0.3079 - acc: 0.859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.355547). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8559Epoch 00268: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3106 - acc: 0.8633 - val_loss: 0.2440 - val_acc: 0.8915\n",
      "Epoch 270/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3048 - acc: 0.8686Epoch 00269: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3057 - acc: 0.8689 - val_loss: 0.2425 - val_acc: 0.9051\n",
      "Epoch 271/1000\n",
      "10/18 [===============>..............] - ETA: 1s - loss: 0.3376 - acc: 0.8531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.985998). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.509000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8490Epoch 00270: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3313 - acc: 0.8503 - val_loss: 0.2556 - val_acc: 0.9017\n",
      "Epoch 272/1000\n",
      " 4/18 [=====>........................] - ETA: 5s - loss: 0.3074 - acc: 0.8477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.658832). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.228755). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.125509). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8333Epoch 00271: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3372 - acc: 0.8376 - val_loss: 0.2460 - val_acc: 0.8915\n",
      "Epoch 273/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8646Epoch 00272: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3149 - acc: 0.8672 - val_loss: 0.2509 - val_acc: 0.8780\n",
      "Epoch 274/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.8585Epoch 00273: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3161 - acc: 0.8508 - val_loss: 0.2540 - val_acc: 0.8881\n",
      "Epoch 275/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.8637Epoch 00274: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3042 - acc: 0.8643 - val_loss: 0.2442 - val_acc: 0.8949\n",
      "Epoch 276/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8707Epoch 00275: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3193 - acc: 0.8688 - val_loss: 0.2485 - val_acc: 0.8915\n",
      "Epoch 277/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8663Epoch 00276: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3031 - acc: 0.8731 - val_loss: 0.2419 - val_acc: 0.8983\n",
      "Epoch 278/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8498Epoch 00277: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3175 - acc: 0.8532 - val_loss: 0.2488 - val_acc: 0.8847\n",
      "Epoch 279/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8576Epoch 00278: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3088 - acc: 0.8543 - val_loss: 0.2420 - val_acc: 0.8949\n",
      "Epoch 280/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3091 - acc: 0.8637Epoch 00279: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3147 - acc: 0.8622 - val_loss: 0.2505 - val_acc: 0.8712\n",
      "Epoch 281/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3190 - acc: 0.8602Epoch 00280: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3191 - acc: 0.8589 - val_loss: 0.2488 - val_acc: 0.8949\n",
      "Epoch 282/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8655Epoch 00281: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3154 - acc: 0.8681 - val_loss: 0.2443 - val_acc: 0.8915\n",
      "Epoch 283/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8446Epoch 00282: val_loss improved from 0.23906 to 0.23848, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3198 - acc: 0.8462 - val_loss: 0.2385 - val_acc: 0.9119\n",
      "Epoch 284/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.8646Epoch 00283: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3093 - acc: 0.8609 - val_loss: 0.2522 - val_acc: 0.8814\n",
      "Epoch 285/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.8576Epoch 00284: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3070 - acc: 0.8543 - val_loss: 0.2411 - val_acc: 0.9051\n",
      "Epoch 286/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8472Epoch 00285: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3162 - acc: 0.8465 - val_loss: 0.2458 - val_acc: 0.9119\n",
      "Epoch 287/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8516Epoch 00286: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3178 - acc: 0.8528 - val_loss: 0.2617 - val_acc: 0.8949\n",
      "Epoch 288/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8550Epoch 00287: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3318 - acc: 0.8475 - val_loss: 0.2492 - val_acc: 0.8915\n",
      "Epoch 289/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8663Epoch 00288: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3136 - acc: 0.8646 - val_loss: 0.2543 - val_acc: 0.9017\n",
      "Epoch 290/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3164 - acc: 0.8594Epoch 00289: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.3147 - acc: 0.8580 - val_loss: 0.2516 - val_acc: 0.8881\n",
      "Epoch 291/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3173 - acc: 0.8613Epoch 00290: val_loss improved from 0.23848 to 0.23743, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.3088 - acc: 0.8605 - val_loss: 0.2374 - val_acc: 0.8915\n",
      "Epoch 292/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8585Epoch 00291: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3115 - acc: 0.8615 - val_loss: 0.2474 - val_acc: 0.9119\n",
      "Epoch 293/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8663Epoch 00292: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3107 - acc: 0.8689 - val_loss: 0.2442 - val_acc: 0.8881\n",
      "Epoch 294/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8602Epoch 00293: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3112 - acc: 0.8610 - val_loss: 0.2488 - val_acc: 0.8983\n",
      "Epoch 295/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8420- ETA: 0s - loss: 0.3366 - acc: 0.8Epoch 00294: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3285 - acc: 0.8501 - val_loss: 0.2668 - val_acc: 0.8814\n",
      "Epoch 296/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.8707Epoch 00295: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3124 - acc: 0.8603 - val_loss: 0.2378 - val_acc: 0.8949\n",
      "Epoch 297/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8585Epoch 00296: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3111 - acc: 0.8615 - val_loss: 0.2524 - val_acc: 0.8847\n",
      "Epoch 298/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2999 - acc: 0.8750Epoch 00297: val_loss improved from 0.23743 to 0.23740, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.3125 - acc: 0.8707 - val_loss: 0.2374 - val_acc: 0.9017\n",
      "Epoch 299/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8620Epoch 00298: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3103 - acc: 0.8605 - val_loss: 0.2445 - val_acc: 0.8915\n",
      "Epoch 300/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8594Epoch 00299: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3144 - acc: 0.8602 - val_loss: 0.2527 - val_acc: 0.8847\n",
      "Epoch 301/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8550Epoch 00300: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3112 - acc: 0.8539 - val_loss: 0.2423 - val_acc: 0.8983\n",
      "Epoch 302/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8620Epoch 00301: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3147 - acc: 0.8584 - val_loss: 0.2576 - val_acc: 0.8780\n",
      "Epoch 303/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8568Epoch 00302: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3164 - acc: 0.8556 - val_loss: 0.2562 - val_acc: 0.8780\n",
      "Epoch 304/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.8707Epoch 00303: val_loss improved from 0.23740 to 0.23451, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.2989 - acc: 0.8709 - val_loss: 0.2345 - val_acc: 0.9051\n",
      "Epoch 305/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8655Epoch 00304: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3062 - acc: 0.8681 - val_loss: 0.2363 - val_acc: 0.9051\n",
      "Epoch 306/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.8655Epoch 00305: val_loss improved from 0.23451 to 0.23420, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3030 - acc: 0.8659 - val_loss: 0.2342 - val_acc: 0.9017\n",
      "Epoch 307/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.8481Epoch 00306: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3361 - acc: 0.8516 - val_loss: 0.2398 - val_acc: 0.9017\n",
      "Epoch 308/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8516Epoch 00307: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3150 - acc: 0.8570 - val_loss: 0.2622 - val_acc: 0.8644\n",
      "Epoch 309/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.8568Epoch 00308: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3108 - acc: 0.8577 - val_loss: 0.2716 - val_acc: 0.8847\n",
      "Epoch 310/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.3120 - acc: 0.8423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.127502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8420Epoch 00309: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3202 - acc: 0.8373 - val_loss: 0.2470 - val_acc: 0.8847\n",
      "Epoch 311/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.8568Epoch 00310: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2934 - acc: 0.8641 - val_loss: 0.2446 - val_acc: 0.9017\n",
      "Epoch 312/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3235 - acc: 0.8539Epoch 00311: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3216 - acc: 0.8526 - val_loss: 0.2386 - val_acc: 0.8949\n",
      "Epoch 313/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8594Epoch 00312: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3118 - acc: 0.8602 - val_loss: 0.2458 - val_acc: 0.8983\n",
      "Epoch 314/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8516Epoch 00313: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3169 - acc: 0.8485 - val_loss: 0.2463 - val_acc: 0.8881\n",
      "Epoch 315/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8507Epoch 00314: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3123 - acc: 0.8519 - val_loss: 0.2514 - val_acc: 0.8780\n",
      "Epoch 316/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.8741Epoch 00315: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3210 - acc: 0.8657 - val_loss: 0.2511 - val_acc: 0.8847\n",
      "Epoch 317/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.8741Epoch 00316: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3103 - acc: 0.8699 - val_loss: 0.2472 - val_acc: 0.8814\n",
      "Epoch 318/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.8628Epoch 00317: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2970 - acc: 0.8677 - val_loss: 0.2460 - val_acc: 0.8847\n",
      "Epoch 319/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.8611Epoch 00318: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3011 - acc: 0.8618 - val_loss: 0.2415 - val_acc: 0.9017\n",
      "Epoch 320/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8594Epoch 00319: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3116 - acc: 0.8580 - val_loss: 0.3315 - val_acc: 0.8542\n",
      "Epoch 321/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3216 - acc: 0.8511Epoch 00320: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3289 - acc: 0.8436 - val_loss: 0.2645 - val_acc: 0.8814\n",
      "Epoch 322/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8663Epoch 00321: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3053 - acc: 0.8710 - val_loss: 0.2464 - val_acc: 0.9017\n",
      "Epoch 323/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3050 - acc: 0.8713Epoch 00322: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2937 - acc: 0.8755 - val_loss: 0.2482 - val_acc: 0.9051\n",
      "Epoch 324/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.3139 - acc: 0.8613Epoch 00323: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3181 - acc: 0.8559 - val_loss: 0.2535 - val_acc: 0.8780\n",
      "Epoch 325/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8550Epoch 00324: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3164 - acc: 0.8539 - val_loss: 0.2706 - val_acc: 0.8915\n",
      "Epoch 326/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3133 - acc: 0.8559Epoch 00325: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3120 - acc: 0.8569 - val_loss: 0.2689 - val_acc: 0.8746\n",
      "Epoch 327/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.8628Epoch 00326: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2976 - acc: 0.8677 - val_loss: 0.2461 - val_acc: 0.8881\n",
      "Epoch 328/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.8698Epoch 00327: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3084 - acc: 0.8637 - val_loss: 0.2545 - val_acc: 0.8881\n",
      "Epoch 329/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8672Epoch 00328: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3223 - acc: 0.8591 - val_loss: 0.2442 - val_acc: 0.8983\n",
      "Epoch 330/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8646Epoch 00329: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3100 - acc: 0.8651 - val_loss: 0.2467 - val_acc: 0.8881\n",
      "Epoch 331/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3096 - acc: 0.8568Epoch 00330: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3105 - acc: 0.8556 - val_loss: 0.2766 - val_acc: 0.8610\n",
      "Epoch 332/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.8646Epoch 00331: val_loss improved from 0.23420 to 0.23236, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 0s - loss: 0.3125 - acc: 0.8630 - val_loss: 0.2324 - val_acc: 0.8949\n",
      "Epoch 333/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.8585Epoch 00332: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3094 - acc: 0.8572 - val_loss: 0.2516 - val_acc: 0.8983\n",
      "Epoch 334/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3249 - acc: 0.8456Epoch 00333: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3130 - acc: 0.8575 - val_loss: 0.2393 - val_acc: 0.8983\n",
      "Epoch 335/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8594Epoch 00334: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3060 - acc: 0.8538 - val_loss: 0.2335 - val_acc: 0.8949\n",
      "Epoch 336/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.8620Epoch 00335: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3076 - acc: 0.8605 - val_loss: 0.2526 - val_acc: 0.8814\n",
      "Epoch 337/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8559Epoch 00336: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3198 - acc: 0.8526 - val_loss: 0.2379 - val_acc: 0.8915\n",
      "Epoch 338/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.8698Epoch 00337: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2966 - acc: 0.8701 - val_loss: 0.2437 - val_acc: 0.8983\n",
      "Epoch 339/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8628Epoch 00338: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3065 - acc: 0.8613 - val_loss: 0.2696 - val_acc: 0.8746\n",
      "Epoch 340/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8576Epoch 00339: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3317 - acc: 0.8500 - val_loss: 0.2376 - val_acc: 0.8949\n",
      "Epoch 341/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.8724Epoch 00340: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2959 - acc: 0.8704 - val_loss: 0.2404 - val_acc: 0.8983\n",
      "Epoch 342/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8602Epoch 00341: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3160 - acc: 0.8525 - val_loss: 0.2470 - val_acc: 0.8814\n",
      "Epoch 343/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8559Epoch 00342: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3187 - acc: 0.8569 - val_loss: 0.2427 - val_acc: 0.8814\n",
      "Epoch 344/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.8689Epoch 00343: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.3025 - acc: 0.8650 - val_loss: 0.2435 - val_acc: 0.8881\n",
      "Epoch 345/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.8698Epoch 00344: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3099 - acc: 0.8658 - val_loss: 0.2665 - val_acc: 0.8949\n",
      "Epoch 346/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8594Epoch 00345: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3127 - acc: 0.8602 - val_loss: 0.2377 - val_acc: 0.8983\n",
      "Epoch 347/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.8602Epoch 00346: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3023 - acc: 0.8567 - val_loss: 0.2423 - val_acc: 0.8949\n",
      "Epoch 348/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.8707Epoch 00347: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3040 - acc: 0.8751 - val_loss: 0.2377 - val_acc: 0.8983\n",
      "Epoch 349/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.8672Epoch 00348: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3005 - acc: 0.8697 - val_loss: 0.2366 - val_acc: 0.8881\n",
      "Epoch 350/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2949 - acc: 0.8715Epoch 00349: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2857 - acc: 0.8781 - val_loss: 0.2325 - val_acc: 0.9051\n",
      "Epoch 351/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2959 - acc: 0.8740Epoch 00350: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2908 - acc: 0.8734 - val_loss: 0.2432 - val_acc: 0.8983\n",
      "Epoch 352/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.8733Epoch 00351: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2908 - acc: 0.8776 - val_loss: 0.2573 - val_acc: 0.8847\n",
      "Epoch 353/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.8741Epoch 00352: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2974 - acc: 0.8784 - val_loss: 0.2398 - val_acc: 0.8881\n",
      "Epoch 354/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8516Epoch 00353: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3111 - acc: 0.8549 - val_loss: 0.2439 - val_acc: 0.9051\n",
      "Epoch 355/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.8689Epoch 00354: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3047 - acc: 0.8671 - val_loss: 0.2367 - val_acc: 0.8915\n",
      "Epoch 356/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.8620Epoch 00355: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3067 - acc: 0.8626 - val_loss: 0.2328 - val_acc: 0.9017\n",
      "Epoch 357/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.8672Epoch 00356: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3023 - acc: 0.8697 - val_loss: 0.2349 - val_acc: 0.8915\n",
      "Epoch 358/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.8819Epoch 00357: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2797 - acc: 0.8816 - val_loss: 0.2386 - val_acc: 0.8915\n",
      "Epoch 359/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.8802Epoch 00358: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2869 - acc: 0.8842 - val_loss: 0.2446 - val_acc: 0.8847\n",
      "Epoch 360/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.8759Epoch 00359: val_loss improved from 0.23236 to 0.23181, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3069 - acc: 0.8737 - val_loss: 0.2318 - val_acc: 0.8949\n",
      "Epoch 361/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.8559Epoch 00360: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3134 - acc: 0.8590 - val_loss: 0.2429 - val_acc: 0.8881\n",
      "Epoch 362/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8628Epoch 00361: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3067 - acc: 0.8656 - val_loss: 0.2365 - val_acc: 0.8949\n",
      "Epoch 363/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.8533Epoch 00362: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3119 - acc: 0.8523 - val_loss: 0.2593 - val_acc: 0.8881\n",
      "Epoch 364/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3169 - acc: 0.8511Epoch 00363: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3077 - acc: 0.8558 - val_loss: 0.2434 - val_acc: 0.8847\n",
      "Epoch 365/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.3318 - acc: 0.8395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.869016). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.435509). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8533Epoch 00364: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3246 - acc: 0.8523 - val_loss: 0.2353 - val_acc: 0.8983\n",
      "Epoch 366/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8628Epoch 00365: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3104 - acc: 0.8635 - val_loss: 0.2647 - val_acc: 0.8644\n",
      "Epoch 367/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8533Epoch 00366: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3213 - acc: 0.8565 - val_loss: 0.2375 - val_acc: 0.9085\n",
      "Epoch 368/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8472Epoch 00367: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3251 - acc: 0.8529 - val_loss: 0.2562 - val_acc: 0.8983\n",
      "Epoch 369/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.8568Epoch 00368: val_loss improved from 0.23181 to 0.22761, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3121 - acc: 0.8577 - val_loss: 0.2276 - val_acc: 0.9119\n",
      "Epoch 370/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.8707Epoch 00369: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2977 - acc: 0.8709 - val_loss: 0.2302 - val_acc: 0.9051\n",
      "Epoch 371/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2797 - acc: 0.8732Epoch 00370: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2883 - acc: 0.8721 - val_loss: 0.2343 - val_acc: 0.8847\n",
      "Epoch 372/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.8837Epoch 00371: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3003 - acc: 0.8790 - val_loss: 0.2347 - val_acc: 0.8915\n",
      "Epoch 373/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.8681Epoch 00372: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3006 - acc: 0.8620 - val_loss: 0.2435 - val_acc: 0.8949\n",
      "Epoch 374/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8672Epoch 00373: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3120 - acc: 0.8676 - val_loss: 0.2337 - val_acc: 0.8949\n",
      "Epoch 375/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2955 - acc: 0.8759Epoch 00374: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2954 - acc: 0.8771 - val_loss: 0.2441 - val_acc: 0.9085\n",
      "Epoch 376/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.8802Epoch 00375: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2857 - acc: 0.8842 - val_loss: 0.2379 - val_acc: 0.8983\n",
      "Epoch 377/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.8698Epoch 00376: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.3040 - acc: 0.8679 - val_loss: 0.2418 - val_acc: 0.8814\n",
      "Epoch 378/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.8620Epoch 00377: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3002 - acc: 0.8626 - val_loss: 0.2394 - val_acc: 0.9017\n",
      "Epoch 379/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.8741Epoch 00378: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2871 - acc: 0.8699 - val_loss: 0.2443 - val_acc: 0.8949\n",
      "Epoch 380/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.8767Epoch 00379: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2918 - acc: 0.8788 - val_loss: 0.2599 - val_acc: 0.8780\n",
      "Epoch 381/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.8689Epoch 00380: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2958 - acc: 0.8714 - val_loss: 0.2441 - val_acc: 0.8881\n",
      "Epoch 382/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.8750Epoch 00381: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2933 - acc: 0.8729 - val_loss: 0.2552 - val_acc: 0.8915\n",
      "Epoch 383/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.3015 - acc: 0.8649Epoch 00382: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2944 - acc: 0.8668 - val_loss: 0.2440 - val_acc: 0.8915\n",
      "Epoch 384/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.8655Epoch 00383: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3113 - acc: 0.8638 - val_loss: 0.2592 - val_acc: 0.8881\n",
      "Epoch 385/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8602Epoch 00384: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3124 - acc: 0.8631 - val_loss: 0.2366 - val_acc: 0.9017\n",
      "Epoch 386/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.8733Epoch 00385: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2828 - acc: 0.8776 - val_loss: 0.2296 - val_acc: 0.8949\n",
      "Epoch 387/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2874 - acc: 0.8698Epoch 00386: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2880 - acc: 0.8679 - val_loss: 0.2276 - val_acc: 0.9017\n",
      "Epoch 388/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.8698Epoch 00387: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2901 - acc: 0.8722 - val_loss: 0.2398 - val_acc: 0.8915\n",
      "Epoch 389/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2984 - acc: 0.8789Epoch 00388: val_loss improved from 0.22761 to 0.22736, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.2902 - acc: 0.8804 - val_loss: 0.2274 - val_acc: 0.8983\n",
      "Epoch 390/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8707Epoch 00389: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2905 - acc: 0.8751 - val_loss: 0.2333 - val_acc: 0.8949\n",
      "Epoch 391/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.8767Epoch 00390: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2975 - acc: 0.8745 - val_loss: 0.2282 - val_acc: 0.9017\n",
      "Epoch 392/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.8637Epoch 00391: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2957 - acc: 0.8664 - val_loss: 0.2464 - val_acc: 0.8814\n",
      "Epoch 393/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.8663Epoch 00392: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2943 - acc: 0.8646 - val_loss: 0.2415 - val_acc: 0.9119\n",
      "Epoch 394/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.8663Epoch 00393: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3048 - acc: 0.8668 - val_loss: 0.2426 - val_acc: 0.8915\n",
      "Epoch 395/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.8698Epoch 00394: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2924 - acc: 0.8743 - val_loss: 0.2543 - val_acc: 0.8881\n",
      "Epoch 396/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.8733Epoch 00395: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3083 - acc: 0.8691 - val_loss: 0.2490 - val_acc: 0.8949\n",
      "Epoch 397/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.8776Epoch 00396: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2904 - acc: 0.8753 - val_loss: 0.2454 - val_acc: 0.8881\n",
      "Epoch 398/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.8733Epoch 00397: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/18 [===============================] - 1s - loss: 0.3014 - acc: 0.8712 - val_loss: 0.2392 - val_acc: 0.8949\n",
      "Epoch 399/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.8733Epoch 00398: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3186 - acc: 0.8691 - val_loss: 0.2483 - val_acc: 0.8915\n",
      "Epoch 400/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.8689Epoch 00399: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2866 - acc: 0.8692 - val_loss: 0.2573 - val_acc: 0.8746\n",
      "Epoch 401/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2974 - acc: 0.8730Epoch 00400: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2923 - acc: 0.8766 - val_loss: 0.2475 - val_acc: 0.8949\n",
      "Epoch 402/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8594Epoch 00401: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3098 - acc: 0.8580 - val_loss: 0.2324 - val_acc: 0.8881\n",
      "Epoch 403/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2961 - acc: 0.8604Epoch 00402: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2943 - acc: 0.8607 - val_loss: 0.2532 - val_acc: 0.8780\n",
      "Epoch 404/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.8655Epoch 00403: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3001 - acc: 0.8681 - val_loss: 0.2315 - val_acc: 0.8983\n",
      "Epoch 405/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.8750Epoch 00404: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2877 - acc: 0.8729 - val_loss: 0.2479 - val_acc: 0.8814\n",
      "Epoch 406/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2959 - acc: 0.8655Epoch 00405: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2934 - acc: 0.8659 - val_loss: 0.2471 - val_acc: 0.8983\n",
      "Epoch 407/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.8741Epoch 00406: val_loss improved from 0.22736 to 0.22560, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.3073 - acc: 0.8742 - val_loss: 0.2256 - val_acc: 0.8949\n",
      "Epoch 408/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.8628Epoch 00407: val_loss improved from 0.22560 to 0.22368, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.2989 - acc: 0.8635 - val_loss: 0.2237 - val_acc: 0.9119\n",
      "Epoch 409/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.8707Epoch 00408: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2931 - acc: 0.8730 - val_loss: 0.2527 - val_acc: 0.8881\n",
      "Epoch 410/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8750Epoch 00409: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2912 - acc: 0.8771 - val_loss: 0.2626 - val_acc: 0.8949\n",
      "Epoch 411/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3058 - acc: 0.8741Epoch 00410: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3026 - acc: 0.8742 - val_loss: 0.2278 - val_acc: 0.9119\n",
      "Epoch 412/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.8872Epoch 00411: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2792 - acc: 0.8865 - val_loss: 0.2343 - val_acc: 0.9051\n",
      "Epoch 413/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.8759Epoch 00412: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2842 - acc: 0.8737 - val_loss: 0.2303 - val_acc: 0.8915\n",
      "Epoch 414/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.8750Epoch 00413: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3081 - acc: 0.8686 - val_loss: 0.2485 - val_acc: 0.8881\n",
      "Epoch 415/1000\n",
      " 8/18 [============>.................] - ETA: 1s - loss: 0.2802 - acc: 0.8809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.918002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.459751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.8672Epoch 00414: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2925 - acc: 0.8718 - val_loss: 0.2348 - val_acc: 0.9051\n",
      "Epoch 416/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.8741Epoch 00415: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2808 - acc: 0.8742 - val_loss: 0.2279 - val_acc: 0.9017\n",
      "Epoch 417/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.8811Epoch 00416: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2937 - acc: 0.8808 - val_loss: 0.2391 - val_acc: 0.8847\n",
      "Epoch 418/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.8715Epoch 00417: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2951 - acc: 0.8696 - val_loss: 0.2319 - val_acc: 0.8983\n",
      "Epoch 419/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.8750Epoch 00418: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2961 - acc: 0.8729 - val_loss: 0.2294 - val_acc: 0.9085\n",
      "Epoch 420/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2948 - acc: 0.8689Epoch 00419: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2992 - acc: 0.8650 - val_loss: 0.2369 - val_acc: 0.8915\n",
      "Epoch 421/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.8785Epoch 00420: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2856 - acc: 0.8719 - val_loss: 0.2359 - val_acc: 0.8983\n",
      "Epoch 422/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2981 - acc: 0.8698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.978501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.556501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.134501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.8750Epoch 00421: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2929 - acc: 0.8729 - val_loss: 0.2370 - val_acc: 0.9051\n",
      "Epoch 423/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.8724Epoch 00422: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2973 - acc: 0.8619 - val_loss: 0.2474 - val_acc: 0.8915\n",
      "Epoch 424/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.8767Epoch 00423: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2828 - acc: 0.8788 - val_loss: 0.2485 - val_acc: 0.8610\n",
      "Epoch 425/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.8750Epoch 00424: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2970 - acc: 0.8771 - val_loss: 0.2343 - val_acc: 0.8983\n",
      "Epoch 426/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.8750Epoch 00425: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2905 - acc: 0.8750 - val_loss: 0.2355 - val_acc: 0.8915\n",
      "Epoch 427/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.8741Epoch 00426: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2958 - acc: 0.8721 - val_loss: 0.2401 - val_acc: 0.9017\n",
      "Epoch 428/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2890 - acc: 0.8768Epoch 00427: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2869 - acc: 0.8758 - val_loss: 0.2273 - val_acc: 0.8983\n",
      "Epoch 429/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.8785Epoch 00428: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2962 - acc: 0.8825 - val_loss: 0.2429 - val_acc: 0.8780\n",
      "Epoch 430/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.8793Epoch 00429: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2888 - acc: 0.8706 - val_loss: 0.2380 - val_acc: 0.8983\n",
      "Epoch 431/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2835 - acc: 0.8793Epoch 00430: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2874 - acc: 0.8770 - val_loss: 0.2349 - val_acc: 0.9017\n",
      "Epoch 432/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.8707Epoch 00431: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3032 - acc: 0.8709 - val_loss: 0.2416 - val_acc: 0.8983\n",
      "Epoch 433/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.8802Epoch 00432: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2748 - acc: 0.8842 - val_loss: 0.2337 - val_acc: 0.8915\n",
      "Epoch 434/1000\n",
      " 4/18 [=====>........................] - ETA: 4s - loss: 0.2664 - acc: 0.8711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.113501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.224001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.196501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/18 [=====================>........] - ETA: 0s - loss: 0.2804 - acc: 0.8705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.169001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.8663Epoch 00433: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2860 - acc: 0.8689 - val_loss: 0.2319 - val_acc: 0.8881\n",
      "Epoch 435/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.8845Epoch 00434: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2774 - acc: 0.8862 - val_loss: 0.2372 - val_acc: 0.8814\n",
      "Epoch 436/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.8802Epoch 00435: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2840 - acc: 0.8757 - val_loss: 0.2358 - val_acc: 0.9085\n",
      "Epoch 437/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.8715Epoch 00436: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2908 - acc: 0.8696 - val_loss: 0.2337 - val_acc: 0.9051\n",
      "Epoch 438/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.8750Epoch 00437: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2854 - acc: 0.8707 - val_loss: 0.2329 - val_acc: 0.8881\n",
      "Epoch 439/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.8793Epoch 00438: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2794 - acc: 0.8770 - val_loss: 0.2286 - val_acc: 0.8915\n",
      "Epoch 440/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8689Epoch 00439: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2891 - acc: 0.8692 - val_loss: 0.2363 - val_acc: 0.8814\n",
      "Epoch 441/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.8845Epoch 00440: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2841 - acc: 0.8819 - val_loss: 0.2378 - val_acc: 0.8881\n",
      "Epoch 442/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.8750Epoch 00441: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2811 - acc: 0.8793 - val_loss: 0.2413 - val_acc: 0.8814\n",
      "Epoch 443/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.8750Epoch 00442: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2891 - acc: 0.8729 - val_loss: 0.2289 - val_acc: 0.8881\n",
      "Epoch 444/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.8819Epoch 00443: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2814 - acc: 0.8795 - val_loss: 0.2352 - val_acc: 0.9017\n",
      "Epoch 445/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2871 - acc: 0.8750Epoch 00444: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2807 - acc: 0.8776 - val_loss: 0.2337 - val_acc: 0.8983\n",
      "Epoch 446/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.8681Epoch 00445: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2854 - acc: 0.8727 - val_loss: 0.2390 - val_acc: 0.8949\n",
      "Epoch 447/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.8759Epoch 00446: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2791 - acc: 0.8758 - val_loss: 0.2363 - val_acc: 0.9051\n",
      "Epoch 448/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8767Epoch 00447: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2917 - acc: 0.8745 - val_loss: 0.2465 - val_acc: 0.8712\n",
      "Epoch 449/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2981 - acc: 0.8698Epoch 00448: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2885 - acc: 0.8764 - val_loss: 0.2416 - val_acc: 0.8814\n",
      "Epoch 450/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.8759Epoch 00449: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2880 - acc: 0.8737 - val_loss: 0.2246 - val_acc: 0.8983\n",
      "Epoch 451/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2805 - acc: 0.8814Epoch 00450: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2775 - acc: 0.8832 - val_loss: 0.2499 - val_acc: 0.8915\n",
      "Epoch 452/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.8715Epoch 00451: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3018 - acc: 0.8738 - val_loss: 0.2329 - val_acc: 0.8949\n",
      "Epoch 453/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.8724Epoch 00452: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2865 - acc: 0.8768 - val_loss: 0.2558 - val_acc: 0.9017\n",
      "Epoch 454/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8681Epoch 00453: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2931 - acc: 0.8705 - val_loss: 0.2313 - val_acc: 0.8881\n",
      "Epoch 455/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.8802Epoch 00454: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2816 - acc: 0.8799 - val_loss: 0.2443 - val_acc: 0.8780\n",
      "Epoch 456/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.8611Epoch 00455: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2847 - acc: 0.8661 - val_loss: 0.2606 - val_acc: 0.8746\n",
      "Epoch 457/1000\n",
      "15/18 [=======================>......] - ETA: 0s - loss: 0.2862 - acc: 0.8708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.132501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.8724Epoch 00456: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2841 - acc: 0.8704 - val_loss: 0.2334 - val_acc: 0.8949\n",
      "Epoch 458/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.8741Epoch 00457: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2869 - acc: 0.8784 - val_loss: 0.2387 - val_acc: 0.8949\n",
      "Epoch 459/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2984 - acc: 0.8655Epoch 00458: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2963 - acc: 0.8659 - val_loss: 0.2347 - val_acc: 0.8881\n",
      "Epoch 460/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2884 - acc: 0.8750Epoch 00459: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2862 - acc: 0.8771 - val_loss: 0.2593 - val_acc: 0.8678\n",
      "Epoch 461/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3075 - acc: 0.8759Epoch 00460: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3029 - acc: 0.8779 - val_loss: 0.2435 - val_acc: 0.8847\n",
      "Epoch 462/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.3047 - acc: 0.8681"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.939509). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.687256). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.435002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.218251). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.8802Epoch 00461: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2838 - acc: 0.8799 - val_loss: 0.2383 - val_acc: 0.8949\n",
      "Epoch 463/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.8715Epoch 00462: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2874 - acc: 0.8696 - val_loss: 0.2316 - val_acc: 0.8949\n",
      "Epoch 464/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.8872Epoch 00463: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2636 - acc: 0.8908 - val_loss: 0.2444 - val_acc: 0.8915\n",
      "Epoch 465/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.8776Epoch 00464: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2695 - acc: 0.8796 - val_loss: 0.2395 - val_acc: 0.8780\n",
      "Epoch 466/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8733Epoch 00465: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2946 - acc: 0.8755 - val_loss: 0.2312 - val_acc: 0.8949\n",
      "Epoch 467/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.8802Epoch 00466: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2724 - acc: 0.8821 - val_loss: 0.2357 - val_acc: 0.8847\n",
      "Epoch 468/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.8854Epoch 00467: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2739 - acc: 0.8828 - val_loss: 0.2492 - val_acc: 0.8847\n",
      "Epoch 469/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.8898Epoch 00468: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2825 - acc: 0.8890 - val_loss: 0.2375 - val_acc: 0.8949\n",
      "Epoch 470/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.8741Epoch 00469: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2835 - acc: 0.8784 - val_loss: 0.2579 - val_acc: 0.8881\n",
      "Epoch 471/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2848 - acc: 0.8767Epoch 00470: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2836 - acc: 0.8788 - val_loss: 0.2336 - val_acc: 0.8814\n",
      "Epoch 472/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.8819Epoch 00471: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2684 - acc: 0.8837 - val_loss: 0.2304 - val_acc: 0.8847\n",
      "Epoch 473/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2742 - acc: 0.8867Epoch 00472: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2823 - acc: 0.8819 - val_loss: 0.2504 - val_acc: 0.8780\n",
      "Epoch 474/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2852 - acc: 0.8707Epoch 00473: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2756 - acc: 0.8773 - val_loss: 0.2340 - val_acc: 0.8983\n",
      "Epoch 475/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.8828Epoch 00474: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2867 - acc: 0.8824 - val_loss: 0.2338 - val_acc: 0.8847\n",
      "Epoch 476/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2682 - acc: 0.8845Epoch 00475: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2694 - acc: 0.8819 - val_loss: 0.2324 - val_acc: 0.8949\n",
      "Epoch 477/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.8811Epoch 00476: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2742 - acc: 0.8850 - val_loss: 0.2380 - val_acc: 0.8814\n",
      "Epoch 478/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.8837Epoch 00477: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2806 - acc: 0.8790 - val_loss: 0.2296 - val_acc: 0.8949\n",
      "Epoch 479/1000\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 0.2791 - acc: 0.8809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.466001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.234001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.8741Epoch 00478: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2866 - acc: 0.8763 - val_loss: 0.2356 - val_acc: 0.9017\n",
      "Epoch 480/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.8802Epoch 00479: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2726 - acc: 0.8842 - val_loss: 0.2331 - val_acc: 0.9017\n",
      "Epoch 481/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2769 - acc: 0.8828Epoch 00480: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2742 - acc: 0.8867 - val_loss: 0.2350 - val_acc: 0.8881\n",
      "Epoch 482/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.8819Epoch 00481: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2994 - acc: 0.8710 - val_loss: 0.2296 - val_acc: 0.8949\n",
      "Epoch 483/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.8759Epoch 00482: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2764 - acc: 0.8779 - val_loss: 0.2331 - val_acc: 0.8949\n",
      "Epoch 484/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.3181 - acc: 0.8542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.464000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.513001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.257501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.8733Epoch 00483: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2902 - acc: 0.8712 - val_loss: 0.2334 - val_acc: 0.8983\n",
      "Epoch 485/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.8915Epoch 00484: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2909 - acc: 0.8885 - val_loss: 0.2277 - val_acc: 0.8949\n",
      "Epoch 486/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.8759Epoch 00485: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2949 - acc: 0.8673 - val_loss: 0.2329 - val_acc: 0.9017\n",
      "Epoch 487/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.8828Epoch 00486: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2798 - acc: 0.8803 - val_loss: 0.2259 - val_acc: 0.9017\n",
      "Epoch 488/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8750Epoch 00487: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2922 - acc: 0.8771 - val_loss: 0.2538 - val_acc: 0.8678\n",
      "Epoch 489/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.8828Epoch 00488: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2783 - acc: 0.8824 - val_loss: 0.2397 - val_acc: 0.8949\n",
      "Epoch 490/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2689 - acc: 0.8819Epoch 00489: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2659 - acc: 0.8816 - val_loss: 0.2342 - val_acc: 0.8983\n",
      "Epoch 491/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.8845Epoch 00490: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2743 - acc: 0.8862 - val_loss: 0.2386 - val_acc: 0.9051\n",
      "Epoch 492/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.8750Epoch 00491: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2761 - acc: 0.8771 - val_loss: 0.2793 - val_acc: 0.8746\n",
      "Epoch 493/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8689Epoch 00492: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3148 - acc: 0.8650 - val_loss: 0.2318 - val_acc: 0.8915\n",
      "Epoch 494/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2936 - acc: 0.8682Epoch 00493: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2906 - acc: 0.8760 - val_loss: 0.2844 - val_acc: 0.8610\n",
      "Epoch 495/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.8845Epoch 00494: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2683 - acc: 0.8883 - val_loss: 0.2350 - val_acc: 0.8881\n",
      "Epoch 496/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.8845Epoch 00495: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2834 - acc: 0.8798 - val_loss: 0.2338 - val_acc: 0.8949\n",
      "Epoch 497/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.8932Epoch 00496: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2667 - acc: 0.8923 - val_loss: 0.2337 - val_acc: 0.9017\n",
      "Epoch 498/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2912 - acc: 0.8707Epoch 00497: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2815 - acc: 0.8751 - val_loss: 0.2292 - val_acc: 0.8983\n",
      "Epoch 499/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.8785Epoch 00498: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2720 - acc: 0.8804 - val_loss: 0.2327 - val_acc: 0.8915\n",
      "Epoch 500/1000\n",
      "10/18 [===============>..............] - ETA: 1s - loss: 0.3140 - acc: 0.8578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.410501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.603000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.302500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.8663Epoch 00499: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2943 - acc: 0.8604 - val_loss: 0.2298 - val_acc: 0.9017\n",
      "Epoch 501/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.8672Epoch 00500: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2907 - acc: 0.8718 - val_loss: 0.2485 - val_acc: 0.8814\n",
      "Epoch 502/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.8611Epoch 00501: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2932 - acc: 0.8597 - val_loss: 0.2947 - val_acc: 0.8780\n",
      "Epoch 503/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.8698Epoch 00502: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2831 - acc: 0.8701 - val_loss: 0.2326 - val_acc: 0.8983\n",
      "Epoch 504/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.8793Epoch 00503: val_loss improved from 0.22368 to 0.22141, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.2845 - acc: 0.8749 - val_loss: 0.2214 - val_acc: 0.9017\n",
      "Epoch 505/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8950Epoch 00504: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2534 - acc: 0.8982 - val_loss: 0.2269 - val_acc: 0.9017\n",
      "Epoch 506/1000\n",
      " 8/18 [============>.................] - ETA: 1s - loss: 0.2788 - acc: 0.8809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.653002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.328501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.8793Epoch 00505: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2808 - acc: 0.8791 - val_loss: 0.2337 - val_acc: 0.8983\n",
      "Epoch 507/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.8863Epoch 00506: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2947 - acc: 0.8815 - val_loss: 0.2406 - val_acc: 0.8881\n",
      "Epoch 508/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.8915Epoch 00507: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2681 - acc: 0.8907 - val_loss: 0.2427 - val_acc: 0.8847\n",
      "Epoch 509/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8793Epoch 00508: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2845 - acc: 0.8791 - val_loss: 0.2376 - val_acc: 0.9051\n",
      "Epoch 510/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8837Epoch 00509: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2898 - acc: 0.8747 - val_loss: 0.2371 - val_acc: 0.9017\n",
      "Epoch 511/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.8880Epoch 00510: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2614 - acc: 0.8852 - val_loss: 0.2378 - val_acc: 0.8915\n",
      "Epoch 512/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.8811Epoch 00511: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2727 - acc: 0.8829 - val_loss: 0.2309 - val_acc: 0.9085\n",
      "Epoch 513/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.8750Epoch 00512: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2760 - acc: 0.8793 - val_loss: 0.2280 - val_acc: 0.9017\n",
      "Epoch 514/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.8924Epoch 00513: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2572 - acc: 0.8979 - val_loss: 0.2367 - val_acc: 0.8814\n",
      "Epoch 515/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2639 - acc: 0.8936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.182001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00514: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2640 - acc: 0.8928 - val_loss: 0.2318 - val_acc: 0.8983\n",
      "Epoch 516/1000\n",
      " 8/18 [============>.................] - ETA: 1s - loss: 0.2631 - acc: 0.9023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.936501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.469250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2795 - acc: 0.8814Epoch 00515: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2831 - acc: 0.8791 - val_loss: 0.2404 - val_acc: 0.9119\n",
      "Epoch 517/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.8707Epoch 00516: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2767 - acc: 0.8709 - val_loss: 0.2327 - val_acc: 0.8983\n",
      "Epoch 518/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2804 - acc: 0.8778Epoch 00517: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2801 - acc: 0.8804 - val_loss: 0.2344 - val_acc: 0.8847\n",
      "Epoch 519/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.8915Epoch 00518: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2687 - acc: 0.8885 - val_loss: 0.2241 - val_acc: 0.8983\n",
      "Epoch 520/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9002Epoch 00519: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2640 - acc: 0.8925 - val_loss: 0.2356 - val_acc: 0.8915\n",
      "Epoch 521/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.8854Epoch 00520: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2791 - acc: 0.8785 - val_loss: 0.2262 - val_acc: 0.8847\n",
      "Epoch 522/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.8776Epoch 00521: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2728 - acc: 0.8775 - val_loss: 0.2384 - val_acc: 0.8949\n",
      "Epoch 523/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.8872Epoch 00522: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2633 - acc: 0.8865 - val_loss: 0.2363 - val_acc: 0.8983\n",
      "Epoch 524/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2672 - acc: 0.8787Epoch 00523: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2644 - acc: 0.8816 - val_loss: 0.2303 - val_acc: 0.9017\n",
      "Epoch 525/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.8854Epoch 00524: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2736 - acc: 0.8870 - val_loss: 0.2302 - val_acc: 0.8949\n",
      "Epoch 526/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.8958Epoch 00525: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2596 - acc: 0.8948 - val_loss: 0.2306 - val_acc: 0.9051\n",
      "Epoch 527/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.8663Epoch 00526: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2787 - acc: 0.8668 - val_loss: 0.2318 - val_acc: 0.8847\n",
      "Epoch 528/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2756 - acc: 0.8811Epoch 00527: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2663 - acc: 0.8850 - val_loss: 0.2407 - val_acc: 0.8881\n",
      "Epoch 529/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.8898Epoch 00528: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2556 - acc: 0.8890 - val_loss: 0.2296 - val_acc: 0.8949\n",
      "Epoch 530/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.8863Epoch 00529: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2823 - acc: 0.8772 - val_loss: 0.2387 - val_acc: 0.8949\n",
      "Epoch 531/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.8889Epoch 00530: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2718 - acc: 0.8861 - val_loss: 0.2313 - val_acc: 0.8847\n",
      "Epoch 532/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.8819Epoch 00531: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2729 - acc: 0.8795 - val_loss: 0.2345 - val_acc: 0.8949\n",
      "Epoch 533/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.8724Epoch 00532: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2804 - acc: 0.8747 - val_loss: 0.2525 - val_acc: 0.8915\n",
      "Epoch 534/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.8620Epoch 00533: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.3023 - acc: 0.8605 - val_loss: 0.2320 - val_acc: 0.9051\n",
      "Epoch 535/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2961 - acc: 0.8796Epoch 00534: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2913 - acc: 0.8847 - val_loss: 0.2292 - val_acc: 0.8949\n",
      "Epoch 536/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.8837Epoch 00535: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2734 - acc: 0.8790 - val_loss: 0.2701 - val_acc: 0.8915\n",
      "Epoch 537/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.8785Epoch 00536: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2707 - acc: 0.8847 - val_loss: 0.2469 - val_acc: 0.8881\n",
      "Epoch 538/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2650 - acc: 0.8880Epoch 00537: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2651 - acc: 0.8852 - val_loss: 0.2390 - val_acc: 0.8983\n",
      "Epoch 539/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.8976Epoch 00538: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2639 - acc: 0.8964 - val_loss: 0.2376 - val_acc: 0.8949\n",
      "Epoch 540/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.8872Epoch 00539: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2716 - acc: 0.8908 - val_loss: 0.2387 - val_acc: 0.8949\n",
      "Epoch 541/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.8906Epoch 00540: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2769 - acc: 0.8941 - val_loss: 0.2370 - val_acc: 0.8746\n",
      "Epoch 542/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.8889Epoch 00541: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2608 - acc: 0.8903 - val_loss: 0.2301 - val_acc: 0.8949\n",
      "Epoch 543/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.8924Epoch 00542: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2729 - acc: 0.8915 - val_loss: 0.2329 - val_acc: 0.8949\n",
      "Epoch 544/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.8880Epoch 00543: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2670 - acc: 0.8895 - val_loss: 0.2336 - val_acc: 0.8915\n",
      "Epoch 545/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.8889Epoch 00544: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2573 - acc: 0.8882 - val_loss: 0.2464 - val_acc: 0.8746\n",
      "Epoch 546/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.8993Epoch 00545: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2590 - acc: 0.9002 - val_loss: 0.2278 - val_acc: 0.8983\n",
      "Epoch 547/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.8785Epoch 00546: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2802 - acc: 0.8825 - val_loss: 0.2770 - val_acc: 0.8780\n",
      "Epoch 548/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.8733Epoch 00547: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2865 - acc: 0.8755 - val_loss: 0.2675 - val_acc: 0.8881\n",
      "Epoch 549/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.8759Epoch 00548: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2699 - acc: 0.8758 - val_loss: 0.2362 - val_acc: 0.8915\n",
      "Epoch 550/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.8950Epoch 00549: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2722 - acc: 0.8939 - val_loss: 0.2303 - val_acc: 0.9051\n",
      "Epoch 551/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.8767Epoch 00550: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2820 - acc: 0.8745 - val_loss: 0.2310 - val_acc: 0.8915\n",
      "Epoch 552/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2716 - acc: 0.8805Epoch 00551: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2670 - acc: 0.8824 - val_loss: 0.2383 - val_acc: 0.8915\n",
      "Epoch 553/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.8828Epoch 00552: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2657 - acc: 0.8845 - val_loss: 0.2546 - val_acc: 0.8814\n",
      "Epoch 554/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8932Epoch 00553: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2606 - acc: 0.8902 - val_loss: 0.2416 - val_acc: 0.8983\n",
      "Epoch 555/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8819Epoch 00554: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2874 - acc: 0.8837 - val_loss: 0.2451 - val_acc: 0.8915\n",
      "Epoch 556/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.8967Epoch 00555: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2621 - acc: 0.8998 - val_loss: 0.2384 - val_acc: 0.8983\n",
      "Epoch 557/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.8854Epoch 00556: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2682 - acc: 0.8849 - val_loss: 0.2694 - val_acc: 0.8847\n",
      "Epoch 558/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2774 - acc: 0.8897Epoch 00557: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2716 - acc: 0.8920 - val_loss: 0.2309 - val_acc: 0.8949\n",
      "Epoch 559/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2770 - acc: 0.8799Epoch 00558: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2706 - acc: 0.8783 - val_loss: 0.2552 - val_acc: 0.8915\n",
      "Epoch 560/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.8906Epoch 00559: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2581 - acc: 0.8941 - val_loss: 0.2332 - val_acc: 0.8881\n",
      "Epoch 561/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.8967Epoch 00560: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2762 - acc: 0.8892 - val_loss: 0.2269 - val_acc: 0.8983\n",
      "Epoch 562/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.8880Epoch 00561: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2719 - acc: 0.8852 - val_loss: 0.2325 - val_acc: 0.8949\n",
      "Epoch 563/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.8715Epoch 00562: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2738 - acc: 0.8717 - val_loss: 0.2808 - val_acc: 0.8780\n",
      "Epoch 564/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.8854Epoch 00563: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2798 - acc: 0.8849 - val_loss: 0.2397 - val_acc: 0.8881\n",
      "Epoch 565/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2731 - acc: 0.8863Epoch 00564: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2715 - acc: 0.8836 - val_loss: 0.2311 - val_acc: 0.9017\n",
      "Epoch 566/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.8828Epoch 00565: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2603 - acc: 0.8867 - val_loss: 0.2278 - val_acc: 0.9017\n",
      "Epoch 567/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2834 - acc: 0.8767Epoch 00566: val_loss improved from 0.22141 to 0.22065, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 1s - loss: 0.2787 - acc: 0.8788 - val_loss: 0.2207 - val_acc: 0.9017\n",
      "Epoch 568/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2744 - acc: 0.8824Epoch 00567: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2695 - acc: 0.8832 - val_loss: 0.2284 - val_acc: 0.9051\n",
      "Epoch 569/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.8872Epoch 00568: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2845 - acc: 0.8844 - val_loss: 0.2255 - val_acc: 0.9017\n",
      "Epoch 570/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.8880Epoch 00569: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2790 - acc: 0.8810 - val_loss: 0.2324 - val_acc: 0.8847\n",
      "Epoch 571/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2670 - acc: 0.8880Epoch 00570: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2674 - acc: 0.8874 - val_loss: 0.2277 - val_acc: 0.8983\n",
      "Epoch 572/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.8828Epoch 00571: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2533 - acc: 0.8845 - val_loss: 0.2279 - val_acc: 0.8983\n",
      "Epoch 573/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2752 - acc: 0.8785Epoch 00572: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2749 - acc: 0.8762 - val_loss: 0.2315 - val_acc: 0.8983\n",
      "Epoch 574/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2686 - acc: 0.8814Epoch 00573: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2652 - acc: 0.8858 - val_loss: 0.2272 - val_acc: 0.8949\n",
      "Epoch 575/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.8837Epoch 00574: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2875 - acc: 0.8811 - val_loss: 0.2444 - val_acc: 0.8915\n",
      "Epoch 576/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.8924Epoch 00575: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2748 - acc: 0.8915 - val_loss: 0.2340 - val_acc: 0.8983\n",
      "Epoch 577/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2523 - acc: 0.8915Epoch 00576: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2625 - acc: 0.8915 - val_loss: 0.2346 - val_acc: 0.8983\n",
      "Epoch 578/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.2756 - acc: 0.8892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.982501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.491750). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.8837Epoch 00577: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2718 - acc: 0.8811 - val_loss: 0.2307 - val_acc: 0.8915\n",
      "Epoch 579/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2779 - acc: 0.8779Epoch 00578: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2750 - acc: 0.8762 - val_loss: 0.2335 - val_acc: 0.8881\n",
      "Epoch 580/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.8672Epoch 00579: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2983 - acc: 0.8676 - val_loss: 0.2729 - val_acc: 0.8814\n",
      "Epoch 581/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2996 - acc: 0.8542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.984502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.656002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.327501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.164751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8689Epoch 00580: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2840 - acc: 0.8692 - val_loss: 0.2509 - val_acc: 0.8983\n",
      "Epoch 582/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.8845Epoch 00581: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2578 - acc: 0.8841 - val_loss: 0.2302 - val_acc: 0.8983\n",
      "Epoch 583/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2589 - acc: 0.8906Epoch 00582: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2638 - acc: 0.8882 - val_loss: 0.2479 - val_acc: 0.8949\n",
      "Epoch 584/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.8924Epoch 00583: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2676 - acc: 0.8915 - val_loss: 0.2309 - val_acc: 0.8915\n",
      "Epoch 585/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.8967Epoch 00584: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2613 - acc: 0.8956 - val_loss: 0.2334 - val_acc: 0.8983\n",
      "Epoch 586/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2654 - acc: 0.8975Epoch 00585: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2608 - acc: 0.8944 - val_loss: 0.2281 - val_acc: 0.8983\n",
      "Epoch 587/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.8889Epoch 00586: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2654 - acc: 0.8924 - val_loss: 0.2222 - val_acc: 0.9119\n",
      "Epoch 588/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.8889Epoch 00587: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2734 - acc: 0.8903 - val_loss: 0.2369 - val_acc: 0.8847\n",
      "Epoch 589/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.8785Epoch 00588: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2812 - acc: 0.8783 - val_loss: 0.2373 - val_acc: 0.8881\n",
      "Epoch 590/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2710 - acc: 0.8916Epoch 00589: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2778 - acc: 0.8885 - val_loss: 0.2376 - val_acc: 0.8915\n",
      "Epoch 591/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2562 - acc: 0.8915Epoch 00590: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2586 - acc: 0.8920 - val_loss: 0.2468 - val_acc: 0.8915\n",
      "Epoch 592/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.8906Epoch 00591: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2524 - acc: 0.8920 - val_loss: 0.2353 - val_acc: 0.9119\n",
      "Epoch 593/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.8663Epoch 00592: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2914 - acc: 0.8731 - val_loss: 0.2448 - val_acc: 0.8983\n",
      "Epoch 594/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.8854Epoch 00593: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2725 - acc: 0.8828 - val_loss: 0.2339 - val_acc: 0.8949\n",
      "Epoch 595/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.8932Epoch 00594: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2559 - acc: 0.8902 - val_loss: 0.2330 - val_acc: 0.8915\n",
      "Epoch 596/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2571 - acc: 0.8887Epoch 00595: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2505 - acc: 0.8936 - val_loss: 0.2338 - val_acc: 0.9017\n",
      "Epoch 597/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.8880Epoch 00596: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2663 - acc: 0.8895 - val_loss: 0.2237 - val_acc: 0.9119\n",
      "Epoch 598/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.8889Epoch 00597: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2633 - acc: 0.8861 - val_loss: 0.2294 - val_acc: 0.9017\n",
      "Epoch 599/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.8932Epoch 00598: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2591 - acc: 0.8966 - val_loss: 0.2327 - val_acc: 0.9017\n",
      "Epoch 600/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8698Epoch 00599: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2730 - acc: 0.8722 - val_loss: 0.2362 - val_acc: 0.8949\n",
      "Epoch 601/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.8767Epoch 00600: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2735 - acc: 0.8745 - val_loss: 0.2644 - val_acc: 0.8780\n",
      "Epoch 602/1000\n",
      "12/18 [==================>...........] - ETA: 1s - loss: 0.2859 - acc: 0.8711"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.124501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.8776Epoch 00601: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2684 - acc: 0.8796 - val_loss: 0.2424 - val_acc: 0.8915\n",
      "Epoch 603/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.8915Epoch 00602: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2584 - acc: 0.8928 - val_loss: 0.2289 - val_acc: 0.8949\n",
      "Epoch 604/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9002Epoch 00603: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2651 - acc: 0.8989 - val_loss: 0.2730 - val_acc: 0.8780\n",
      "Epoch 605/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9002Epoch 00604: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2715 - acc: 0.8946 - val_loss: 0.2367 - val_acc: 0.8949\n",
      "Epoch 606/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.8967Epoch 00605: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2437 - acc: 0.8998 - val_loss: 0.2264 - val_acc: 0.8983\n",
      "Epoch 607/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8915Epoch 00606: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2556 - acc: 0.8907 - val_loss: 0.2349 - val_acc: 0.8983\n",
      "Epoch 608/1000\n",
      "10/18 [===============>..............] - ETA: 1s - loss: 0.2436 - acc: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.523501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.385504). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.227002). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.8906Epoch 00607: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2611 - acc: 0.8898 - val_loss: 0.2290 - val_acc: 0.8881\n",
      "Epoch 609/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.8854Epoch 00608: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2629 - acc: 0.8891 - val_loss: 0.2433 - val_acc: 0.8983\n",
      "Epoch 610/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.8889Epoch 00609: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2494 - acc: 0.8924 - val_loss: 0.2313 - val_acc: 0.9017\n",
      "Epoch 611/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.8932Epoch 00610: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2615 - acc: 0.8944 - val_loss: 0.2297 - val_acc: 0.8983\n",
      "Epoch 612/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.8889Epoch 00611: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2594 - acc: 0.8882 - val_loss: 0.2378 - val_acc: 0.8949\n",
      "Epoch 613/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.8889Epoch 00612: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2622 - acc: 0.8839 - val_loss: 0.2814 - val_acc: 0.8847\n",
      "Epoch 614/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.8811Epoch 00613: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2650 - acc: 0.8808 - val_loss: 0.2417 - val_acc: 0.8847\n",
      "Epoch 615/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8932Epoch 00614: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2547 - acc: 0.8966 - val_loss: 0.2701 - val_acc: 0.8814\n",
      "Epoch 616/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.8863Epoch 00615: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2594 - acc: 0.8836 - val_loss: 0.2305 - val_acc: 0.9017\n",
      "Epoch 617/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.8993Epoch 00616: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2592 - acc: 0.9002 - val_loss: 0.2267 - val_acc: 0.8949\n",
      "Epoch 618/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.8880Epoch 00617: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2604 - acc: 0.8831 - val_loss: 0.2220 - val_acc: 0.8983\n",
      "Epoch 619/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.8898Epoch 00618: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2691 - acc: 0.8890 - val_loss: 0.2266 - val_acc: 0.8983\n",
      "Epoch 620/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.8915Epoch 00619: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2545 - acc: 0.8928 - val_loss: 0.2562 - val_acc: 0.8847\n",
      "Epoch 621/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.8854Epoch 00620: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2800 - acc: 0.8785 - val_loss: 0.2311 - val_acc: 0.8949\n",
      "Epoch 622/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.8915Epoch 00621: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2476 - acc: 0.8970 - val_loss: 0.2418 - val_acc: 0.8915\n",
      "Epoch 623/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.8898Epoch 00622: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2616 - acc: 0.8869 - val_loss: 0.2387 - val_acc: 0.8983\n",
      "Epoch 624/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2557 - acc: 0.8971Epoch 00623: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2559 - acc: 0.8961 - val_loss: 0.2352 - val_acc: 0.9051\n",
      "Epoch 625/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2463 - acc: 0.8896Epoch 00624: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2515 - acc: 0.8903 - val_loss: 0.2549 - val_acc: 0.8881\n",
      "Epoch 626/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.8976Epoch 00625: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2509 - acc: 0.8964 - val_loss: 0.2722 - val_acc: 0.8949\n",
      "Epoch 627/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.8906Epoch 00626: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2578 - acc: 0.8856 - val_loss: 0.2425 - val_acc: 0.8983\n",
      "Epoch 628/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.8906Epoch 00627: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2531 - acc: 0.8898 - val_loss: 0.2337 - val_acc: 0.8983\n",
      "Epoch 629/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.8845Epoch 00628: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2589 - acc: 0.8883 - val_loss: 0.2338 - val_acc: 0.8847\n",
      "Epoch 630/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.8932Epoch 00629: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2602 - acc: 0.8923 - val_loss: 0.2543 - val_acc: 0.8847\n",
      "Epoch 631/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.8854Epoch 00630: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2554 - acc: 0.8891 - val_loss: 0.2412 - val_acc: 0.8983\n",
      "Epoch 632/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.8941Epoch 00631: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2454 - acc: 0.8974 - val_loss: 0.2896 - val_acc: 0.8915\n",
      "Epoch 633/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.8793Epoch 00632: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2732 - acc: 0.8834 - val_loss: 0.2555 - val_acc: 0.8847\n",
      "Epoch 634/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8976Epoch 00633: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2484 - acc: 0.8964 - val_loss: 0.2386 - val_acc: 0.8949\n",
      "Epoch 635/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.2503 - acc: 0.8991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.154021). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.8993Epoch 00634: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2434 - acc: 0.9023 - val_loss: 0.2375 - val_acc: 0.8949\n",
      "Epoch 636/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.8993Epoch 00635: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2482 - acc: 0.9023 - val_loss: 0.2341 - val_acc: 0.8983\n",
      "Epoch 637/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.8906Epoch 00636: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2607 - acc: 0.8898 - val_loss: 0.2345 - val_acc: 0.9051\n",
      "Epoch 638/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.8993Epoch 00637: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2533 - acc: 0.8981 - val_loss: 0.2290 - val_acc: 0.9017\n",
      "Epoch 639/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.8906Epoch 00638: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2640 - acc: 0.8898 - val_loss: 0.2420 - val_acc: 0.8915\n",
      "Epoch 640/1000\n",
      " 9/18 [=============>................] - ETA: 0s - loss: 0.2294 - acc: 0.9097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.149999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9002Epoch 00639: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2479 - acc: 0.8968 - val_loss: 0.2460 - val_acc: 0.8847\n",
      "Epoch 641/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.8984Epoch 00640: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2457 - acc: 0.9015 - val_loss: 0.2480 - val_acc: 0.8847\n",
      "Epoch 642/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.8976Epoch 00641: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2468 - acc: 0.8922 - val_loss: 0.2549 - val_acc: 0.8847\n",
      "Epoch 643/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.8854Epoch 00642: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2608 - acc: 0.8891 - val_loss: 0.2383 - val_acc: 0.8983\n",
      "Epoch 644/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9071Epoch 00643: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2533 - acc: 0.9076 - val_loss: 0.2318 - val_acc: 0.9119\n",
      "Epoch 645/1000\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 0.1857 - acc: 0.9316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.167502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8958Epoch 00644: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2537 - acc: 0.8948 - val_loss: 0.2356 - val_acc: 0.9017\n",
      "Epoch 646/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9028Epoch 00645: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2368 - acc: 0.9056 - val_loss: 0.2324 - val_acc: 0.9017\n",
      "Epoch 647/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.8967Epoch 00646: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2420 - acc: 0.8998 - val_loss: 0.2247 - val_acc: 0.9085\n",
      "Epoch 648/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.8819Epoch 00647: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2618 - acc: 0.8880 - val_loss: 0.2330 - val_acc: 0.9017\n",
      "Epoch 649/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.8854Epoch 00648: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2587 - acc: 0.8828 - val_loss: 0.2415 - val_acc: 0.8949\n",
      "Epoch 650/1000\n",
      "12/18 [==================>...........] - ETA: 0s - loss: 0.2651 - acc: 0.8841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.134001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.198001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.8898Epoch 00649: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2533 - acc: 0.8890 - val_loss: 0.2352 - val_acc: 0.8915\n",
      "Epoch 651/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.8924Epoch 00650: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2536 - acc: 0.8915 - val_loss: 0.2311 - val_acc: 0.8983\n",
      "Epoch 652/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9010Epoch 00651: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2554 - acc: 0.8997 - val_loss: 0.2563 - val_acc: 0.8847\n",
      "Epoch 653/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.8906Epoch 00652: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2513 - acc: 0.8898 - val_loss: 0.2408 - val_acc: 0.9017\n",
      "Epoch 654/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.8984Epoch 00653: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2390 - acc: 0.8972 - val_loss: 0.2496 - val_acc: 0.8915\n",
      "Epoch 655/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2702 - acc: 0.8889Epoch 00654: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2669 - acc: 0.8861 - val_loss: 0.2333 - val_acc: 0.9051\n",
      "Epoch 656/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.8802Epoch 00655: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2598 - acc: 0.8821 - val_loss: 0.2307 - val_acc: 0.9017\n",
      "Epoch 657/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.8993Epoch 00656: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2523 - acc: 0.9002 - val_loss: 0.2556 - val_acc: 0.9085\n",
      "Epoch 658/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.8889Epoch 00657: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2574 - acc: 0.8903 - val_loss: 0.2414 - val_acc: 0.8949\n",
      "Epoch 659/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.8880Epoch 00658: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2524 - acc: 0.8916 - val_loss: 0.2353 - val_acc: 0.8949\n",
      "Epoch 660/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.8906Epoch 00659: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2536 - acc: 0.8920 - val_loss: 0.2428 - val_acc: 0.8949\n",
      "Epoch 661/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9045Epoch 00660: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2523 - acc: 0.8988 - val_loss: 0.2258 - val_acc: 0.9051\n",
      "Epoch 662/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9019Epoch 00661: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2409 - acc: 0.9048 - val_loss: 0.2287 - val_acc: 0.8915\n",
      "Epoch 663/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.8993Epoch 00662: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2492 - acc: 0.8981 - val_loss: 0.2449 - val_acc: 0.8983\n",
      "Epoch 664/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9002Epoch 00663: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2383 - acc: 0.9031 - val_loss: 0.2476 - val_acc: 0.8881\n",
      "Epoch 665/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.8898Epoch 00664: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2553 - acc: 0.8911 - val_loss: 0.2437 - val_acc: 0.9017\n",
      "Epoch 666/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2578 - acc: 0.8941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.976001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.611501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.247001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.124251). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9010Epoch 00665: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2495 - acc: 0.8997 - val_loss: 0.2341 - val_acc: 0.8949\n",
      "Epoch 667/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9045Epoch 00666: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2329 - acc: 0.9051 - val_loss: 0.2464 - val_acc: 0.8780\n",
      "Epoch 668/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.8932Epoch 00667: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2656 - acc: 0.8944 - val_loss: 0.2523 - val_acc: 0.8949\n",
      "Epoch 669/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.8976Epoch 00668: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2518 - acc: 0.8985 - val_loss: 0.2250 - val_acc: 0.9085\n",
      "Epoch 670/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.8984Epoch 00669: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2555 - acc: 0.9015 - val_loss: 0.2381 - val_acc: 0.9017\n",
      "Epoch 671/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2549 - acc: 0.9017Epoch 00670: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2528 - acc: 0.9056 - val_loss: 0.2458 - val_acc: 0.8949\n",
      "Epoch 672/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9002Epoch 00671: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2454 - acc: 0.8989 - val_loss: 0.2369 - val_acc: 0.8915\n",
      "Epoch 673/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9019Epoch 00672: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2389 - acc: 0.9048 - val_loss: 0.2263 - val_acc: 0.9051\n",
      "Epoch 674/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.9002Epoch 00673: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2440 - acc: 0.9031 - val_loss: 0.2363 - val_acc: 0.8949\n",
      "Epoch 675/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9002Epoch 00674: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2579 - acc: 0.8989 - val_loss: 0.2440 - val_acc: 0.8983\n",
      "Epoch 676/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.8854Epoch 00675: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2567 - acc: 0.8849 - val_loss: 0.2642 - val_acc: 0.8780\n",
      "Epoch 677/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.8889Epoch 00676: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2609 - acc: 0.8882 - val_loss: 0.2475 - val_acc: 0.9017\n",
      "Epoch 678/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.8976Epoch 00677: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2661 - acc: 0.8943 - val_loss: 0.2390 - val_acc: 0.8949\n",
      "Epoch 679/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.8941Epoch 00678: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2633 - acc: 0.8931 - val_loss: 0.2443 - val_acc: 0.9017\n",
      "Epoch 680/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9002Epoch 00679: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2591 - acc: 0.8968 - val_loss: 0.2340 - val_acc: 0.8983\n",
      "Epoch 681/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.8845Epoch 00680: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2614 - acc: 0.8819 - val_loss: 0.2388 - val_acc: 0.9085\n",
      "Epoch 682/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.8958Epoch 00681: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2504 - acc: 0.8926 - val_loss: 0.2333 - val_acc: 0.8983\n",
      "Epoch 683/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.8950Epoch 00682: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2432 - acc: 0.8918 - val_loss: 0.2409 - val_acc: 0.8983\n",
      "Epoch 684/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.8950Epoch 00683: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2726 - acc: 0.8961 - val_loss: 0.2248 - val_acc: 0.9085\n",
      "Epoch 685/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.8793Epoch 00684: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2536 - acc: 0.8834 - val_loss: 0.2682 - val_acc: 0.8847\n",
      "Epoch 686/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.8889Epoch 00685: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2583 - acc: 0.8903 - val_loss: 0.2634 - val_acc: 0.8983\n",
      "Epoch 687/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9002Epoch 00686: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2399 - acc: 0.8989 - val_loss: 0.2304 - val_acc: 0.8983\n",
      "Epoch 688/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.8941Epoch 00687: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2531 - acc: 0.8995 - val_loss: 0.2329 - val_acc: 0.9119\n",
      "Epoch 689/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.8984Epoch 00688: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2494 - acc: 0.8972 - val_loss: 0.2348 - val_acc: 0.8983\n",
      "Epoch 690/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2485 - acc: 0.8915Epoch 00689: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2418 - acc: 0.8944 - val_loss: 0.2408 - val_acc: 0.8949\n",
      "Epoch 691/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.8958Epoch 00690: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2450 - acc: 0.8969 - val_loss: 0.2281 - val_acc: 0.9085\n",
      "Epoch 692/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.8984Epoch 00691: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2420 - acc: 0.9036 - val_loss: 0.2308 - val_acc: 0.9017\n",
      "Epoch 693/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.8872Epoch 00692: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2665 - acc: 0.8844 - val_loss: 0.2422 - val_acc: 0.9017\n",
      "Epoch 694/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.8950Epoch 00693: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2387 - acc: 0.8982 - val_loss: 0.2333 - val_acc: 0.8983\n",
      "Epoch 695/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.8967Epoch 00694: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2522 - acc: 0.8956 - val_loss: 0.2468 - val_acc: 0.8915\n",
      "Epoch 696/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2646 - acc: 0.8833Epoch 00695: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2656 - acc: 0.8854 - val_loss: 0.2511 - val_acc: 0.9017\n",
      "Epoch 697/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9028Epoch 00696: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2364 - acc: 0.9014 - val_loss: 0.2452 - val_acc: 0.9017\n",
      "Epoch 698/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2482 - acc: 0.8932Epoch 00697: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2433 - acc: 0.8966 - val_loss: 0.2288 - val_acc: 0.9085\n",
      "Epoch 699/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9019Epoch 00698: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2487 - acc: 0.8963 - val_loss: 0.2440 - val_acc: 0.8915\n",
      "Epoch 700/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9002Epoch 00699: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2540 - acc: 0.8968 - val_loss: 0.2423 - val_acc: 0.8983\n",
      "Epoch 701/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.8863Epoch 00700: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2736 - acc: 0.8857 - val_loss: 0.2309 - val_acc: 0.9085\n",
      "Epoch 702/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2882 - acc: 0.8750Epoch 00701: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2920 - acc: 0.8760 - val_loss: 0.2525 - val_acc: 0.8915\n",
      "Epoch 703/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9089Epoch 00702: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2558 - acc: 0.9050 - val_loss: 0.2395 - val_acc: 0.8983\n",
      "Epoch 704/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.8967Epoch 00703: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2433 - acc: 0.8998 - val_loss: 0.2322 - val_acc: 0.9085\n",
      "Epoch 705/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8958Epoch 00704: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2484 - acc: 0.8969 - val_loss: 0.2513 - val_acc: 0.8847\n",
      "Epoch 706/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9123Epoch 00705: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2428 - acc: 0.9083 - val_loss: 0.2566 - val_acc: 0.8949\n",
      "Epoch 707/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8950Epoch 00706: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2500 - acc: 0.8918 - val_loss: 0.2335 - val_acc: 0.8949\n",
      "Epoch 708/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.8854Epoch 00707: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2637 - acc: 0.8891 - val_loss: 0.2279 - val_acc: 0.9085\n",
      "Epoch 709/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.8958Epoch 00708: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2273 - acc: 0.9011 - val_loss: 0.2515 - val_acc: 0.8915\n",
      "Epoch 710/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9036Epoch 00709: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2297 - acc: 0.9043 - val_loss: 0.2262 - val_acc: 0.8983\n",
      "Epoch 711/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2493 - acc: 0.8976Epoch 00710: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2523 - acc: 0.8922 - val_loss: 0.2529 - val_acc: 0.8814\n",
      "Epoch 712/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9062Epoch 00711: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2375 - acc: 0.9110 - val_loss: 0.2414 - val_acc: 0.8949\n",
      "Epoch 713/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.8932Epoch 00712: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2538 - acc: 0.8966 - val_loss: 0.2210 - val_acc: 0.9119\n",
      "Epoch 714/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2581 - acc: 0.8872Epoch 00713: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2570 - acc: 0.8887 - val_loss: 0.2236 - val_acc: 0.9085\n",
      "Epoch 715/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.8993Epoch 00714: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2393 - acc: 0.8981 - val_loss: 0.2322 - val_acc: 0.9051\n",
      "Epoch 716/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.8898Epoch 00715: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2511 - acc: 0.8954 - val_loss: 0.2345 - val_acc: 0.8983\n",
      "Epoch 717/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9036Epoch 00716: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2438 - acc: 0.9022 - val_loss: 0.2401 - val_acc: 0.8915\n",
      "Epoch 718/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.8967Epoch 00717: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2499 - acc: 0.9020 - val_loss: 0.2351 - val_acc: 0.9051\n",
      "Epoch 719/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9002Epoch 00718: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2569 - acc: 0.9010 - val_loss: 0.2360 - val_acc: 0.8983\n",
      "Epoch 720/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9028Epoch 00719: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2385 - acc: 0.9035 - val_loss: 0.2343 - val_acc: 0.8983\n",
      "Epoch 721/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.8967Epoch 00720: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2504 - acc: 0.8956 - val_loss: 0.2405 - val_acc: 0.8983\n",
      "Epoch 722/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9019Epoch 00721: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2420 - acc: 0.9048 - val_loss: 0.2303 - val_acc: 0.8983\n",
      "Epoch 723/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9010Epoch 00722: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2336 - acc: 0.9040 - val_loss: 0.2307 - val_acc: 0.9051\n",
      "Epoch 724/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.8889Epoch 00723: val_loss improved from 0.22065 to 0.21899, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "19/18 [===============================] - 2s - loss: 0.2461 - acc: 0.8903 - val_loss: 0.2190 - val_acc: 0.9085\n",
      "Epoch 725/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9071Epoch 00724: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2319 - acc: 0.9097 - val_loss: 0.2392 - val_acc: 0.8915\n",
      "Epoch 726/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9028Epoch 00725: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2450 - acc: 0.8971 - val_loss: 0.2449 - val_acc: 0.8949\n",
      "Epoch 727/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9062Epoch 00726: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2443 - acc: 0.9004 - val_loss: 0.2375 - val_acc: 0.8949\n",
      "Epoch 728/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9019Epoch 00727: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2395 - acc: 0.9005 - val_loss: 0.2412 - val_acc: 0.9119\n",
      "Epoch 729/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9115Epoch 00728: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2298 - acc: 0.9075 - val_loss: 0.2531 - val_acc: 0.8881\n",
      "Epoch 730/1000\n",
      " 4/18 [=====>........................] - ETA: 4s - loss: 0.2632 - acc: 0.8906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.120502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.239503). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.205002). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/18 [=====================>........] - ETA: 0s - loss: 0.2404 - acc: 0.8962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.170502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.8906Epoch 00729: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2594 - acc: 0.8877 - val_loss: 0.2501 - val_acc: 0.8881\n",
      "Epoch 731/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9045Epoch 00730: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2275 - acc: 0.9094 - val_loss: 0.2278 - val_acc: 0.8983\n",
      "Epoch 732/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.8941Epoch 00731: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2395 - acc: 0.8931 - val_loss: 0.2296 - val_acc: 0.8881\n",
      "Epoch 733/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9036Epoch 00732: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2354 - acc: 0.9086 - val_loss: 0.2404 - val_acc: 0.8949\n",
      "Epoch 734/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.8915Epoch 00733: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2406 - acc: 0.8928 - val_loss: 0.2378 - val_acc: 0.8983\n",
      "Epoch 735/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9036Epoch 00734: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2390 - acc: 0.9064 - val_loss: 0.2292 - val_acc: 0.9085\n",
      "Epoch 736/1000\n",
      "12/18 [==================>...........] - ETA: 0s - loss: 0.2483 - acc: 0.8880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.131750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.200504). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101252). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.8889Epoch 00735: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2409 - acc: 0.8924 - val_loss: 0.2480 - val_acc: 0.8915\n",
      "Epoch 737/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.8750Epoch 00736: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2665 - acc: 0.8729 - val_loss: 0.2303 - val_acc: 0.9051\n",
      "Epoch 738/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.8854Epoch 00737: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2494 - acc: 0.8891 - val_loss: 0.2348 - val_acc: 0.8983\n",
      "Epoch 739/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.8993Epoch 00738: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2519 - acc: 0.9002 - val_loss: 0.2451 - val_acc: 0.9017\n",
      "Epoch 740/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.8932Epoch 00739: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2489 - acc: 0.8923 - val_loss: 0.2357 - val_acc: 0.9085\n",
      "Epoch 741/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.8924Epoch 00740: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2514 - acc: 0.8936 - val_loss: 0.2318 - val_acc: 0.8949\n",
      "Epoch 742/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9045Epoch 00741: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2271 - acc: 0.9051 - val_loss: 0.2645 - val_acc: 0.8949\n",
      "Epoch 743/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.8906Epoch 00742: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2551 - acc: 0.8920 - val_loss: 0.2343 - val_acc: 0.9017\n",
      "Epoch 744/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9002Epoch 00743: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2427 - acc: 0.9031 - val_loss: 0.2441 - val_acc: 0.8983\n",
      "Epoch 745/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.8958Epoch 00744: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2373 - acc: 0.9011 - val_loss: 0.2520 - val_acc: 0.8915\n",
      "Epoch 746/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9028Epoch 00745: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2393 - acc: 0.9014 - val_loss: 0.2344 - val_acc: 0.8983\n",
      "Epoch 747/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.8915Epoch 00746: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2496 - acc: 0.8885 - val_loss: 0.2438 - val_acc: 0.8983\n",
      "Epoch 748/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2376 - acc: 0.9102Epoch 00747: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2301 - acc: 0.9163 - val_loss: 0.2484 - val_acc: 0.8915\n",
      "Epoch 749/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.8967Epoch 00748: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2537 - acc: 0.8977 - val_loss: 0.2486 - val_acc: 0.8847\n",
      "Epoch 750/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9149Epoch 00749: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2254 - acc: 0.9171 - val_loss: 0.2483 - val_acc: 0.8915\n",
      "Epoch 751/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.8932Epoch 00750: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2444 - acc: 0.8923 - val_loss: 0.2339 - val_acc: 0.9051\n",
      "Epoch 752/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.8845Epoch 00751: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2441 - acc: 0.8904 - val_loss: 0.2291 - val_acc: 0.9119\n",
      "Epoch 753/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9036Epoch 00752: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2212 - acc: 0.9086 - val_loss: 0.2523 - val_acc: 0.9017\n",
      "Epoch 754/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2274 - acc: 0.9062Epoch 00753: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2292 - acc: 0.9073 - val_loss: 0.2400 - val_acc: 0.9051\n",
      "Epoch 755/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9019Epoch 00754: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2262 - acc: 0.9027 - val_loss: 0.2369 - val_acc: 0.8983\n",
      "Epoch 756/1000\n",
      " 2/18 [==>...........................] - ETA: 1s - loss: 0.2889 - acc: 0.8594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.182500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/18 [===============>..............] - ETA: 1s - loss: 0.2408 - acc: 0.8953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.580750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.296501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.239501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.8984Epoch 00755: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2374 - acc: 0.8994 - val_loss: 0.2625 - val_acc: 0.8881\n",
      "Epoch 757/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9010Epoch 00756: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2402 - acc: 0.9018 - val_loss: 0.2852 - val_acc: 0.8915\n",
      "Epoch 758/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.8950Epoch 00757: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2463 - acc: 0.8982 - val_loss: 0.2657 - val_acc: 0.8915\n",
      "Epoch 759/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9002Epoch 00758: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2380 - acc: 0.9010 - val_loss: 0.2581 - val_acc: 0.8983\n",
      "Epoch 760/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9045Epoch 00759: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2418 - acc: 0.9009 - val_loss: 0.2722 - val_acc: 0.8814\n",
      "Epoch 761/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.8967Epoch 00760: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2427 - acc: 0.8977 - val_loss: 0.2670 - val_acc: 0.8881\n",
      "Epoch 762/1000\n",
      " 8/18 [============>.................] - ETA: 0s - loss: 0.2509 - acc: 0.8984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.344503). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.173252). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9045Epoch 00761: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2323 - acc: 0.9051 - val_loss: 0.2460 - val_acc: 0.8881\n",
      "Epoch 763/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9115Epoch 00762: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2225 - acc: 0.9117 - val_loss: 0.2434 - val_acc: 0.9017\n",
      "Epoch 764/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9036Epoch 00763: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2420 - acc: 0.9001 - val_loss: 0.2430 - val_acc: 0.8949\n",
      "Epoch 765/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9010Epoch 00764: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2304 - acc: 0.9040 - val_loss: 0.2440 - val_acc: 0.8915\n",
      "Epoch 766/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9106Epoch 00765: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2286 - acc: 0.9109 - val_loss: 0.2246 - val_acc: 0.9186\n",
      "Epoch 767/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.8984Epoch 00766: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2500 - acc: 0.8972 - val_loss: 0.2273 - val_acc: 0.9017\n",
      "Epoch 768/1000\n",
      "11/18 [================>.............] - ETA: 1s - loss: 0.2203 - acc: 0.899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.233752). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.8967Epoch 00767: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2372 - acc: 0.8998 - val_loss: 0.2455 - val_acc: 0.8915\n",
      "Epoch 769/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9019Epoch 00768: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2305 - acc: 0.9048 - val_loss: 0.2244 - val_acc: 0.9085\n",
      "Epoch 770/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.8984Epoch 00769: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2390 - acc: 0.8994 - val_loss: 0.2276 - val_acc: 0.8983\n",
      "Epoch 771/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.8915Epoch 00770: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2479 - acc: 0.8907 - val_loss: 0.2486 - val_acc: 0.8915\n",
      "Epoch 772/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9010Epoch 00771: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2369 - acc: 0.9040 - val_loss: 0.2295 - val_acc: 0.9085\n",
      "Epoch 773/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9106Epoch 00772: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2380 - acc: 0.9109 - val_loss: 0.2333 - val_acc: 0.8881\n",
      "Epoch 774/1000\n",
      "12/18 [==================>...........] - ETA: 0s - loss: 0.2571 - acc: 0.8984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.982502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.694751). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.407001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.205001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.8984Epoch 00773: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2545 - acc: 0.8930 - val_loss: 0.2428 - val_acc: 0.8949\n",
      "Epoch 775/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9002Epoch 00774: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2429 - acc: 0.8968 - val_loss: 0.2407 - val_acc: 0.9051\n",
      "Epoch 776/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.8950Epoch 00775: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2422 - acc: 0.8982 - val_loss: 0.2252 - val_acc: 0.9051\n",
      "Epoch 777/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.8950Epoch 00776: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2383 - acc: 0.8961 - val_loss: 0.2263 - val_acc: 0.9085\n",
      "Epoch 778/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9019Epoch 00777: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2323 - acc: 0.8984 - val_loss: 0.2343 - val_acc: 0.9051\n",
      "Epoch 779/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2409 - acc: 0.8965Epoch 00778: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2357 - acc: 0.9044 - val_loss: 0.2424 - val_acc: 0.8949\n",
      "Epoch 780/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.8958Epoch 00779: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2415 - acc: 0.8948 - val_loss: 0.2295 - val_acc: 0.9017\n",
      "Epoch 781/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9002Epoch 00780: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2298 - acc: 0.8989 - val_loss: 0.2275 - val_acc: 0.9051\n",
      "Epoch 782/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2258 - acc: 0.9053Epoch 00781: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2303 - acc: 0.9009 - val_loss: 0.2388 - val_acc: 0.9051\n",
      "Epoch 783/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.8976Epoch 00782: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2437 - acc: 0.8964 - val_loss: 0.2633 - val_acc: 0.8881\n",
      "Epoch 784/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.8976Epoch 00783: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2390 - acc: 0.8922 - val_loss: 0.2247 - val_acc: 0.9186\n",
      "Epoch 785/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2159 - acc: 0.9082Epoch 00784: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2337 - acc: 0.8988 - val_loss: 0.2449 - val_acc: 0.8915\n",
      "Epoch 786/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9054Epoch 00785: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2357 - acc: 0.9038 - val_loss: 0.2563 - val_acc: 0.8949\n",
      "Epoch 787/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9080Epoch 00786: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2412 - acc: 0.9063 - val_loss: 0.2600 - val_acc: 0.8915\n",
      "Epoch 788/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9028Epoch 00787: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2341 - acc: 0.9056 - val_loss: 0.2251 - val_acc: 0.9153\n",
      "Epoch 789/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9036Epoch 00788: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2449 - acc: 0.9043 - val_loss: 0.2223 - val_acc: 0.9085\n",
      "Epoch 790/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9106Epoch 00789: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2294 - acc: 0.9130 - val_loss: 0.3051 - val_acc: 0.8712\n",
      "Epoch 791/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.8993Epoch 00790: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2483 - acc: 0.8959 - val_loss: 0.2352 - val_acc: 0.9085\n",
      "Epoch 792/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2587 - acc: 0.8916Epoch 00791: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2497 - acc: 0.8941 - val_loss: 0.2348 - val_acc: 0.8983\n",
      "Epoch 793/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9002Epoch 00792: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2299 - acc: 0.9010 - val_loss: 0.2301 - val_acc: 0.9017\n",
      "Epoch 794/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.8958Epoch 00793: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2305 - acc: 0.8990 - val_loss: 0.2421 - val_acc: 0.8949\n",
      "Epoch 795/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2296 - acc: 0.9062Epoch 00794: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2214 - acc: 0.9096 - val_loss: 0.2479 - val_acc: 0.9017\n",
      "Epoch 796/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.8854Epoch 00795: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2655 - acc: 0.8849 - val_loss: 0.3005 - val_acc: 0.8780\n",
      "Epoch 797/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.8889Epoch 00796: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2622 - acc: 0.8882 - val_loss: 0.2241 - val_acc: 0.9119\n",
      "Epoch 798/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.8950Epoch 00797: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2327 - acc: 0.9003 - val_loss: 0.2261 - val_acc: 0.9085\n",
      "Epoch 799/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9019Epoch 00798: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2289 - acc: 0.9027 - val_loss: 0.2387 - val_acc: 0.8983\n",
      "Epoch 800/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.8993Epoch 00799: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2266 - acc: 0.9023 - val_loss: 0.2274 - val_acc: 0.9119\n",
      "Epoch 801/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9115Epoch 00800: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2258 - acc: 0.9117 - val_loss: 0.2351 - val_acc: 0.9017\n",
      "Epoch 802/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9115Epoch 00801: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2365 - acc: 0.9096 - val_loss: 0.2251 - val_acc: 0.9119\n",
      "Epoch 803/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2561 - acc: 0.8906Epoch 00802: val_loss did not improve\n",
      "19/18 [===============================] - 3s - loss: 0.2556 - acc: 0.8890 - val_loss: 0.3057 - val_acc: 0.8780\n",
      "Epoch 804/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.8941Epoch 00803: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2368 - acc: 0.8952 - val_loss: 0.2505 - val_acc: 0.8847\n",
      "Epoch 805/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2378 - acc: 0.9035Epoch 00804: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2332 - acc: 0.9035 - val_loss: 0.2335 - val_acc: 0.8983\n",
      "Epoch 806/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9045Epoch 00805: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2241 - acc: 0.9051 - val_loss: 0.2541 - val_acc: 0.8847\n",
      "Epoch 807/1000\n",
      "12/18 [==================>...........] - ETA: 0s - loss: 0.2371 - acc: 0.8893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.8967Epoch 00806: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2221 - acc: 0.8998 - val_loss: 0.2393 - val_acc: 0.9085\n",
      "Epoch 808/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9045Epoch 00807: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2315 - acc: 0.9030 - val_loss: 0.2410 - val_acc: 0.8983\n",
      "Epoch 809/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2386 - acc: 0.9090Epoch 00808: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2538 - acc: 0.9014 - val_loss: 0.2415 - val_acc: 0.8949\n",
      "Epoch 810/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9080Epoch 00809: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2349 - acc: 0.8999 - val_loss: 0.2501 - val_acc: 0.8949\n",
      "Epoch 811/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9115Epoch 00810: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2300 - acc: 0.9117 - val_loss: 0.2650 - val_acc: 0.8814\n",
      "Epoch 812/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2355 - acc: 0.8989Epoch 00811: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2481 - acc: 0.8935 - val_loss: 0.2609 - val_acc: 0.8949\n",
      "Epoch 813/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9045Epoch 00812: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2283 - acc: 0.9073 - val_loss: 0.2473 - val_acc: 0.8949\n",
      "Epoch 814/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2257 - acc: 0.9090Epoch 00813: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2248 - acc: 0.9080 - val_loss: 0.2342 - val_acc: 0.9119\n",
      "Epoch 815/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9036Epoch 00814: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2360 - acc: 0.9064 - val_loss: 0.2289 - val_acc: 0.9051\n",
      "Epoch 816/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9080Epoch 00815: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2226 - acc: 0.9084 - val_loss: 0.2398 - val_acc: 0.8915\n",
      "Epoch 817/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9002Epoch 00816: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2328 - acc: 0.8968 - val_loss: 0.2398 - val_acc: 0.9051\n",
      "Epoch 818/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9089Epoch 00817: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2286 - acc: 0.9093 - val_loss: 0.2342 - val_acc: 0.8983\n",
      "Epoch 819/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2209 - acc: 0.9127Epoch 00818: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2427 - acc: 0.9045 - val_loss: 0.2346 - val_acc: 0.9085\n",
      "Epoch 820/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.8967Epoch 00819: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2293 - acc: 0.8977 - val_loss: 0.2447 - val_acc: 0.8949\n",
      "Epoch 821/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9149Epoch 00820: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2158 - acc: 0.9171 - val_loss: 0.2409 - val_acc: 0.8847\n",
      "Epoch 822/1000\n",
      " 3/18 [===>..........................] - ETA: 1s - loss: 0.1858 - acc: 0.9271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.128250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/18 [================>.............] - ETA: 1s - loss: 0.2369 - acc: 0.8935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.255501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.223500). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.191500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9062Epoch 00821: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2254 - acc: 0.9068 - val_loss: 0.2488 - val_acc: 0.9017\n",
      "Epoch 823/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2210 - acc: 0.9118Epoch 00822: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2244 - acc: 0.9104 - val_loss: 0.2502 - val_acc: 0.8949\n",
      "Epoch 824/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2250 - acc: 0.9082Epoch 00823: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2235 - acc: 0.9109 - val_loss: 0.2469 - val_acc: 0.8949\n",
      "Epoch 825/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9045Epoch 00824: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2334 - acc: 0.9009 - val_loss: 0.2296 - val_acc: 0.9153\n",
      "Epoch 826/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9028Epoch 00825: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2354 - acc: 0.9014 - val_loss: 0.2299 - val_acc: 0.9119\n",
      "Epoch 827/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9089Epoch 00826: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2281 - acc: 0.9050 - val_loss: 0.2471 - val_acc: 0.9085\n",
      "Epoch 828/1000\n",
      " 2/18 [==>...........................] - ETA: 1s - loss: 0.2546 - acc: 0.8906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.178500). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/18 [===============>..............] - ETA: 1s - loss: 0.2235 - acc: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.579250). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.292001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.235250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9080Epoch 00827: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2231 - acc: 0.9084 - val_loss: 0.2332 - val_acc: 0.9119\n",
      "Epoch 829/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9080Epoch 00828: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2376 - acc: 0.9127 - val_loss: 0.2540 - val_acc: 0.8983\n",
      "Epoch 830/1000\n",
      " 5/18 [=======>......................] - ETA: 2s - loss: 0.2515 - acc: 0.8781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.671502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.336251). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2406 - acc: 0.9035Epoch 00829: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2635 - acc: 0.8989 - val_loss: 0.2466 - val_acc: 0.8881\n",
      "Epoch 831/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.2386 - acc: 0.9048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.983000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.492250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9071Epoch 00830: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2292 - acc: 0.9076 - val_loss: 0.2460 - val_acc: 0.9051\n",
      "Epoch 832/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9010Epoch 00831: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2310 - acc: 0.9040 - val_loss: 0.2619 - val_acc: 0.8983\n",
      "Epoch 833/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9141Epoch 00832: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2135 - acc: 0.9163 - val_loss: 0.2560 - val_acc: 0.8915\n",
      "Epoch 834/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9080Epoch 00833: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2161 - acc: 0.9127 - val_loss: 0.2666 - val_acc: 0.8915\n",
      "Epoch 835/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2348 - acc: 0.9043Epoch 00834: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2291 - acc: 0.9081 - val_loss: 0.2519 - val_acc: 0.8915\n",
      "Epoch 836/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9123Epoch 00835: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2152 - acc: 0.9125 - val_loss: 0.2559 - val_acc: 0.8915\n",
      "Epoch 837/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9071Epoch 00836: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2371 - acc: 0.9055 - val_loss: 0.2833 - val_acc: 0.8847\n",
      "Epoch 838/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9097Epoch 00837: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2290 - acc: 0.9016 - val_loss: 0.2457 - val_acc: 0.8949\n",
      "Epoch 839/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9097Epoch 00838: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2186 - acc: 0.9122 - val_loss: 0.2514 - val_acc: 0.8949\n",
      "Epoch 840/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.8993Epoch 00839: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2472 - acc: 0.8981 - val_loss: 0.2418 - val_acc: 0.8915\n",
      "Epoch 841/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2301 - acc: 0.9099Epoch 00840: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2398 - acc: 0.9060 - val_loss: 0.2607 - val_acc: 0.8847\n",
      "Epoch 842/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9036Epoch 00841: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2407 - acc: 0.9043 - val_loss: 0.2433 - val_acc: 0.8915\n",
      "Epoch 843/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9141Epoch 00842: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2295 - acc: 0.9142 - val_loss: 0.2511 - val_acc: 0.8847\n",
      "Epoch 844/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9019Epoch 00843: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2285 - acc: 0.9027 - val_loss: 0.2464 - val_acc: 0.9017\n",
      "Epoch 845/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.8950Epoch 00844: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2321 - acc: 0.9003 - val_loss: 0.2269 - val_acc: 0.9017\n",
      "Epoch 846/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9010Epoch 00845: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2306 - acc: 0.9061 - val_loss: 0.2827 - val_acc: 0.8814\n",
      "Epoch 847/1000\n",
      "14/18 [=====================>........] - ETA: 0s - loss: 0.2322 - acc: 0.9118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.328001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9115Epoch 00846: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2272 - acc: 0.9096 - val_loss: 0.2298 - val_acc: 0.8915\n",
      "Epoch 848/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2437 - acc: 0.8984Epoch 00847: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2505 - acc: 0.8994 - val_loss: 0.2552 - val_acc: 0.8949\n",
      "Epoch 849/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9123Epoch 00848: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2240 - acc: 0.9104 - val_loss: 0.2454 - val_acc: 0.8949\n",
      "Epoch 850/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9071Epoch 00849: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2167 - acc: 0.9097 - val_loss: 0.2606 - val_acc: 0.8881\n",
      "Epoch 851/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2498 - acc: 0.8941Epoch 00850: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2490 - acc: 0.8952 - val_loss: 0.2376 - val_acc: 0.9051\n",
      "Epoch 852/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.8958Epoch 00851: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2415 - acc: 0.8926 - val_loss: 0.2300 - val_acc: 0.9153\n",
      "Epoch 853/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9080Epoch 00852: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2171 - acc: 0.9084 - val_loss: 0.2321 - val_acc: 0.9085\n",
      "Epoch 854/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9010Epoch 00853: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2330 - acc: 0.8955 - val_loss: 0.2459 - val_acc: 0.8983\n",
      "Epoch 855/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9115Epoch 00854: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2221 - acc: 0.9096 - val_loss: 0.2401 - val_acc: 0.8949\n",
      "Epoch 856/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.8941Epoch 00855: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2426 - acc: 0.8910 - val_loss: 0.2291 - val_acc: 0.9085\n",
      "Epoch 857/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9132Epoch 00856: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2336 - acc: 0.9112 - val_loss: 0.2384 - val_acc: 0.9051\n",
      "Epoch 858/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.8984Epoch 00857: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2447 - acc: 0.8951 - val_loss: 0.2401 - val_acc: 0.8915\n",
      "Epoch 859/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9010Epoch 00858: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2389 - acc: 0.9040 - val_loss: 0.2485 - val_acc: 0.8915\n",
      "Epoch 860/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9089Epoch 00859: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2323 - acc: 0.9050 - val_loss: 0.2283 - val_acc: 0.9017\n",
      "Epoch 861/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.8898Epoch 00860: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2333 - acc: 0.8933 - val_loss: 0.2482 - val_acc: 0.8847\n",
      "Epoch 862/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9149Epoch 00861: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2229 - acc: 0.9108 - val_loss: 0.2925 - val_acc: 0.8881\n",
      "Epoch 863/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.8976Epoch 00862: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2406 - acc: 0.8985 - val_loss: 0.2600 - val_acc: 0.8881\n",
      "Epoch 864/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.8958Epoch 00863: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2397 - acc: 0.8990 - val_loss: 0.2429 - val_acc: 0.9017\n",
      "Epoch 865/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9045Epoch 00864: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2398 - acc: 0.9009 - val_loss: 0.2398 - val_acc: 0.9051\n",
      "Epoch 866/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9149Epoch 00865: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2154 - acc: 0.9171 - val_loss: 0.2313 - val_acc: 0.9051\n",
      "Epoch 867/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9210Epoch 00866: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2029 - acc: 0.9187 - val_loss: 0.2565 - val_acc: 0.9017\n",
      "Epoch 868/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2479 - acc: 0.8924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.420501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.622002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.312001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9080Epoch 00867: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2233 - acc: 0.9106 - val_loss: 0.2506 - val_acc: 0.8983\n",
      "Epoch 869/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9062Epoch 00868: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2121 - acc: 0.9089 - val_loss: 0.2214 - val_acc: 0.9153\n",
      "Epoch 870/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9097Epoch 00869: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2144 - acc: 0.9122 - val_loss: 0.2560 - val_acc: 0.8949\n",
      "Epoch 871/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9115Epoch 00870: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2128 - acc: 0.9138 - val_loss: 0.2349 - val_acc: 0.8983\n",
      "Epoch 872/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9149Epoch 00871: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2223 - acc: 0.9108 - val_loss: 0.2459 - val_acc: 0.9085\n",
      "Epoch 873/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9123Epoch 00872: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2179 - acc: 0.9104 - val_loss: 0.2498 - val_acc: 0.8949\n",
      "Epoch 874/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9036Epoch 00873: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2270 - acc: 0.9043 - val_loss: 0.2475 - val_acc: 0.8983\n",
      "Epoch 875/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9115Epoch 00874: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2157 - acc: 0.9096 - val_loss: 0.2565 - val_acc: 0.8881\n",
      "Epoch 876/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9089Epoch 00875: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2217 - acc: 0.9093 - val_loss: 0.2479 - val_acc: 0.9017\n",
      "Epoch 877/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9097Epoch 00876: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2277 - acc: 0.9080 - val_loss: 0.2420 - val_acc: 0.8983\n",
      "Epoch 878/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2144 - acc: 0.9062Epoch 00877: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2225 - acc: 0.8999 - val_loss: 0.2282 - val_acc: 0.9017\n",
      "Epoch 879/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9010Epoch 00878: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2357 - acc: 0.8997 - val_loss: 0.2228 - val_acc: 0.9153\n",
      "Epoch 880/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2177 - acc: 0.902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.580751). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.185502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2125 - acc: 0.9097Epoch 00879: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2226 - acc: 0.9037 - val_loss: 0.2358 - val_acc: 0.8949\n",
      "Epoch 881/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.8941Epoch 00880: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2427 - acc: 0.8910 - val_loss: 0.2455 - val_acc: 0.8983\n",
      "Epoch 882/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9071Epoch 00881: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2289 - acc: 0.9119 - val_loss: 0.2486 - val_acc: 0.8949\n",
      "Epoch 883/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2217 - acc: 0.9118Epoch 00882: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2200 - acc: 0.9163 - val_loss: 0.2242 - val_acc: 0.9085\n",
      "Epoch 884/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9115Epoch 00883: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2079 - acc: 0.9138 - val_loss: 0.2330 - val_acc: 0.9186\n",
      "Epoch 885/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.8967Epoch 00884: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2389 - acc: 0.8998 - val_loss: 0.2307 - val_acc: 0.9085\n",
      "Epoch 886/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9106Epoch 00885: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2305 - acc: 0.9109 - val_loss: 0.2518 - val_acc: 0.9085\n",
      "Epoch 887/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9132Epoch 00886: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2184 - acc: 0.9134 - val_loss: 0.2576 - val_acc: 0.8949\n",
      "Epoch 888/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9045Epoch 00887: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2229 - acc: 0.9073 - val_loss: 0.2471 - val_acc: 0.8983\n",
      "Epoch 889/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9184Epoch 00888: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2177 - acc: 0.9204 - val_loss: 0.2447 - val_acc: 0.9085\n",
      "Epoch 890/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9071Epoch 00889: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2104 - acc: 0.9119 - val_loss: 0.2488 - val_acc: 0.9119\n",
      "Epoch 891/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9193Epoch 00890: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2143 - acc: 0.9213 - val_loss: 0.2613 - val_acc: 0.8814\n",
      "Epoch 892/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9097Epoch 00891: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2174 - acc: 0.9122 - val_loss: 0.2608 - val_acc: 0.9051\n",
      "Epoch 893/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9115Epoch 00892: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2092 - acc: 0.9138 - val_loss: 0.2644 - val_acc: 0.8915\n",
      "Epoch 894/1000\n",
      " 9/18 [=============>................] - ETA: 1s - loss: 0.2202 - acc: 0.9167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.322500). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.644001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.323001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9141Epoch 00893: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2183 - acc: 0.9163 - val_loss: 0.2495 - val_acc: 0.9017\n",
      "Epoch 895/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9054Epoch 00894: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2160 - acc: 0.9081 - val_loss: 0.2534 - val_acc: 0.8915\n",
      "Epoch 896/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2152 - acc: 0.9167Epoch 00895: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2105 - acc: 0.9188 - val_loss: 0.2593 - val_acc: 0.8881\n",
      "Epoch 897/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9184Epoch 00896: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2198 - acc: 0.9183 - val_loss: 0.2596 - val_acc: 0.9017\n",
      "Epoch 898/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9036Epoch 00897: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2218 - acc: 0.9064 - val_loss: 0.2504 - val_acc: 0.8949\n",
      "Epoch 899/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2083 - acc: 0.9180Epoch 00898: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2200 - acc: 0.9095 - val_loss: 0.2504 - val_acc: 0.8915\n",
      "Epoch 900/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9097Epoch 00899: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.1960 - acc: 0.9143 - val_loss: 0.2579 - val_acc: 0.8915\n",
      "Epoch 901/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9010Epoch 00900: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2345 - acc: 0.8976 - val_loss: 0.2572 - val_acc: 0.8949\n",
      "Epoch 902/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.8967Epoch 00901: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2385 - acc: 0.8956 - val_loss: 0.2501 - val_acc: 0.8915\n",
      "Epoch 903/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9123Epoch 00902: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2236 - acc: 0.9125 - val_loss: 0.2447 - val_acc: 0.8881\n",
      "Epoch 904/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9227Epoch 00903: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2045 - acc: 0.9203 - val_loss: 0.2669 - val_acc: 0.8949\n",
      "Epoch 905/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2305 - acc: 0.9072Epoch 00904: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2256 - acc: 0.9106 - val_loss: 0.2443 - val_acc: 0.9017\n",
      "Epoch 906/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9010Epoch 00905: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2281 - acc: 0.9040 - val_loss: 0.2688 - val_acc: 0.8949\n",
      "Epoch 907/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9089Epoch 00906: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2256 - acc: 0.9029 - val_loss: 0.2399 - val_acc: 0.9085\n",
      "Epoch 908/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9080Epoch 00907: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2064 - acc: 0.9127 - val_loss: 0.2488 - val_acc: 0.8949\n",
      "Epoch 909/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9123Epoch 00908: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2281 - acc: 0.9083 - val_loss: 0.2529 - val_acc: 0.8881\n",
      "Epoch 910/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9097Epoch 00909: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2162 - acc: 0.9122 - val_loss: 0.2514 - val_acc: 0.9085\n",
      "Epoch 911/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2172 - acc: 0.9044Epoch 00910: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2165 - acc: 0.9064 - val_loss: 0.2668 - val_acc: 0.8881\n",
      "Epoch 912/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2222 - acc: 0.9054Epoch 00911: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2184 - acc: 0.9081 - val_loss: 0.2449 - val_acc: 0.8983\n",
      "Epoch 913/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.8950Epoch 00912: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2229 - acc: 0.9003 - val_loss: 0.2332 - val_acc: 0.8983\n",
      "Epoch 914/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9193Epoch 00913: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2162 - acc: 0.9170 - val_loss: 0.2409 - val_acc: 0.9051\n",
      "Epoch 915/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9071Epoch 00914: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2236 - acc: 0.9119 - val_loss: 0.2330 - val_acc: 0.9119\n",
      "Epoch 916/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2151 - acc: 0.9131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.108751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00915: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2311 - acc: 0.9068 - val_loss: 0.2495 - val_acc: 0.8983\n",
      "Epoch 917/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.8976Epoch 00916: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2459 - acc: 0.8985 - val_loss: 0.2319 - val_acc: 0.9119\n",
      "Epoch 918/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9028Epoch 00917: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2317 - acc: 0.8992 - val_loss: 0.2492 - val_acc: 0.8949\n",
      "Epoch 919/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9123Epoch 00918: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2099 - acc: 0.9125 - val_loss: 0.2354 - val_acc: 0.9017\n",
      "Epoch 920/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9106Epoch 00919: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2020 - acc: 0.9152 - val_loss: 0.2354 - val_acc: 0.9119\n",
      "Epoch 921/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9062Epoch 00920: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2289 - acc: 0.9068 - val_loss: 0.2396 - val_acc: 0.9051\n",
      "Epoch 922/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9089Epoch 00921: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2115 - acc: 0.9093 - val_loss: 0.2397 - val_acc: 0.9017\n",
      "Epoch 923/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9062Epoch 00922: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2178 - acc: 0.9089 - val_loss: 0.2393 - val_acc: 0.9085\n",
      "Epoch 924/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9158Epoch 00923: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2089 - acc: 0.9158 - val_loss: 0.2430 - val_acc: 0.9051\n",
      "Epoch 925/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9115Epoch 00924: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2319 - acc: 0.9117 - val_loss: 0.2581 - val_acc: 0.8915\n",
      "Epoch 926/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9115Epoch 00925: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2284 - acc: 0.9075 - val_loss: 0.2571 - val_acc: 0.8983\n",
      "Epoch 927/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9158Epoch 00926: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2095 - acc: 0.9137 - val_loss: 0.2244 - val_acc: 0.9254\n",
      "Epoch 928/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2206 - acc: 0.9127Epoch 00927: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2181 - acc: 0.9134 - val_loss: 0.2442 - val_acc: 0.9051\n",
      "Epoch 929/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9097Epoch 00928: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2008 - acc: 0.9080 - val_loss: 0.2448 - val_acc: 0.8949\n",
      "Epoch 930/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9115Epoch 00929: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2062 - acc: 0.9138 - val_loss: 0.2570 - val_acc: 0.8881\n",
      "Epoch 931/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9071Epoch 00930: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2231 - acc: 0.9097 - val_loss: 0.2344 - val_acc: 0.9186\n",
      "Epoch 932/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9158Epoch 00931: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2092 - acc: 0.9158 - val_loss: 0.2365 - val_acc: 0.8983\n",
      "Epoch 933/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.8993Epoch 00932: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2325 - acc: 0.9002 - val_loss: 0.2534 - val_acc: 0.9085\n",
      "Epoch 934/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2227 - acc: 0.9072Epoch 00933: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2142 - acc: 0.9106 - val_loss: 0.2373 - val_acc: 0.9051\n",
      "Epoch 935/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9089Epoch 00934: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2138 - acc: 0.9071 - val_loss: 0.2334 - val_acc: 0.9186\n",
      "Epoch 936/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9010Epoch 00935: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2195 - acc: 0.9040 - val_loss: 0.2414 - val_acc: 0.8847\n",
      "Epoch 937/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9184Epoch 00936: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2025 - acc: 0.9226 - val_loss: 0.2597 - val_acc: 0.8949\n",
      "Epoch 938/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9010Epoch 00937: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2266 - acc: 0.9040 - val_loss: 0.2477 - val_acc: 0.9017\n",
      "Epoch 939/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9045Epoch 00938: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2260 - acc: 0.8988 - val_loss: 0.2376 - val_acc: 0.9153\n",
      "Epoch 940/1000\n",
      "11/18 [================>.............] - ETA: 1s - loss: 0.2030 - acc: 0.9176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.104001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9028Epoch 00939: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2310 - acc: 0.9035 - val_loss: 0.2329 - val_acc: 0.9051\n",
      "Epoch 941/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9080Epoch 00940: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2054 - acc: 0.9127 - val_loss: 0.2413 - val_acc: 0.9119\n",
      "Epoch 942/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9045Epoch 00941: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2253 - acc: 0.9009 - val_loss: 0.2612 - val_acc: 0.8949\n",
      "Epoch 943/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.8984Epoch 00942: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2376 - acc: 0.8994 - val_loss: 0.2525 - val_acc: 0.9017\n",
      "Epoch 944/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9071Epoch 00943: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2286 - acc: 0.9097 - val_loss: 0.2445 - val_acc: 0.8949\n",
      "Epoch 945/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9193Epoch 00944: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2167 - acc: 0.9170 - val_loss: 0.2443 - val_acc: 0.9017\n",
      "Epoch 946/1000\n",
      "14/18 [=====================>........] - ETA: 0s - loss: 0.2038 - acc: 0.9185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.217250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9132Epoch 00945: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2099 - acc: 0.9155 - val_loss: 0.2247 - val_acc: 0.9085\n",
      "Epoch 947/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9175Epoch 00946: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2083 - acc: 0.9175 - val_loss: 0.2226 - val_acc: 0.9220\n",
      "Epoch 948/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9262Epoch 00947: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1917 - acc: 0.9279 - val_loss: 0.2384 - val_acc: 0.9051\n",
      "Epoch 949/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9097Epoch 00948: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2019 - acc: 0.9143 - val_loss: 0.2657 - val_acc: 0.8881\n",
      "Epoch 950/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9097Epoch 00949: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2326 - acc: 0.9037 - val_loss: 0.2365 - val_acc: 0.8983\n",
      "Epoch 951/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9167Epoch 00950: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2075 - acc: 0.9145 - val_loss: 0.2265 - val_acc: 0.9186\n",
      "Epoch 952/1000\n",
      "12/18 [==================>...........] - ETA: 0s - loss: 0.1981 - acc: 0.9284"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.968000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.695750). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.423501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.212751). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9175Epoch 00951: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2045 - acc: 0.9175 - val_loss: 0.2308 - val_acc: 0.9051\n",
      "Epoch 953/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9062Epoch 00952: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2086 - acc: 0.9089 - val_loss: 0.2687 - val_acc: 0.8847\n",
      "Epoch 954/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9210Epoch 00953: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2184 - acc: 0.9144 - val_loss: 0.2321 - val_acc: 0.9051\n",
      "Epoch 955/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9106Epoch 00954: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2211 - acc: 0.9045 - val_loss: 0.2447 - val_acc: 0.9017\n",
      "Epoch 956/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9036Epoch 00955: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2336 - acc: 0.8979 - val_loss: 0.2292 - val_acc: 0.8949\n",
      "Epoch 957/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9062Epoch 00956: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2138 - acc: 0.9068 - val_loss: 0.2456 - val_acc: 0.8949\n",
      "Epoch 958/1000\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 0.2314 - acc: 0.9109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.447002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.225501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9062Epoch 00957: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2223 - acc: 0.9110 - val_loss: 0.2779 - val_acc: 0.8814\n",
      "Epoch 959/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9071Epoch 00958: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2232 - acc: 0.9076 - val_loss: 0.2392 - val_acc: 0.9153\n",
      "Epoch 960/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9062Epoch 00959: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2113 - acc: 0.9068 - val_loss: 0.2334 - val_acc: 0.9288\n",
      "Epoch 961/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9193Epoch 00960: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2100 - acc: 0.9191 - val_loss: 0.2319 - val_acc: 0.9119\n",
      "Epoch 962/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9149Epoch 00961: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2343 - acc: 0.9108 - val_loss: 0.2713 - val_acc: 0.8847\n",
      "Epoch 963/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2187 - acc: 0.9141Epoch 00962: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2212 - acc: 0.9053 - val_loss: 0.2471 - val_acc: 0.9017\n",
      "Epoch 964/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9306Epoch 00963: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1893 - acc: 0.9320 - val_loss: 0.2330 - val_acc: 0.9051\n",
      "Epoch 965/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9158Epoch 00964: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2014 - acc: 0.9201 - val_loss: 0.2338 - val_acc: 0.9051\n",
      "Epoch 966/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9167Epoch 00965: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2087 - acc: 0.9167 - val_loss: 0.2620 - val_acc: 0.8949\n",
      "Epoch 967/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9184Epoch 00966: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2032 - acc: 0.9141 - val_loss: 0.2756 - val_acc: 0.8915\n",
      "Epoch 968/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9158Epoch 00967: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2205 - acc: 0.9116 - val_loss: 0.2682 - val_acc: 0.8915\n",
      "Epoch 969/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9175Epoch 00968: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2100 - acc: 0.9154 - val_loss: 0.2661 - val_acc: 0.8847\n",
      "Epoch 970/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9141Epoch 00969: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2063 - acc: 0.9184 - val_loss: 0.2662 - val_acc: 0.8949\n",
      "Epoch 971/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9184Epoch 00970: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2026 - acc: 0.9204 - val_loss: 0.2373 - val_acc: 0.9119\n",
      "Epoch 972/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9158Epoch 00971: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2065 - acc: 0.9180 - val_loss: 0.2448 - val_acc: 0.8881\n",
      "Epoch 973/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9123Epoch 00972: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2155 - acc: 0.9168 - val_loss: 0.2621 - val_acc: 0.9017\n",
      "Epoch 974/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2111 - acc: 0.9170Epoch 00973: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2134 - acc: 0.9147 - val_loss: 0.2325 - val_acc: 0.9085\n",
      "Epoch 975/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9115Epoch 00974: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2104 - acc: 0.9096 - val_loss: 0.2391 - val_acc: 0.9119\n",
      "Epoch 976/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9149Epoch 00975: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2134 - acc: 0.9193 - val_loss: 0.2669 - val_acc: 0.8983\n",
      "Epoch 977/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9175Epoch 00976: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2139 - acc: 0.9175 - val_loss: 0.2508 - val_acc: 0.8949\n",
      "Epoch 978/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2184 - acc: 0.9189Epoch 00977: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2188 - acc: 0.9124 - val_loss: 0.2427 - val_acc: 0.8949\n",
      "Epoch 979/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9149- ETA: 0s - loss: 0.2180 - acEpoch 00978: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2035 - acc: 0.9193 - val_loss: 0.2747 - val_acc: 0.8915\n",
      "Epoch 980/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9115Epoch 00979: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2121 - acc: 0.9096 - val_loss: 0.2527 - val_acc: 0.9051\n",
      "Epoch 981/1000\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.2193 - acc: 0.9131Epoch 00980: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2072 - acc: 0.9168 - val_loss: 0.2669 - val_acc: 0.8983\n",
      "Epoch 982/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9201Epoch 00981: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2092 - acc: 0.9157 - val_loss: 0.2671 - val_acc: 0.8915\n",
      "Epoch 983/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.8984Epoch 00982: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2184 - acc: 0.9015 - val_loss: 0.2453 - val_acc: 0.9051\n",
      "Epoch 984/1000\n",
      "11/18 [================>.............] - ETA: 0s - loss: 0.2010 - acc: 0.9119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.609002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.305250). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9184Epoch 00983: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2020 - acc: 0.9204 - val_loss: 0.2388 - val_acc: 0.9051\n",
      "Epoch 985/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9219Epoch 00984: val_loss did not improve\n",
      "19/18 [===============================] - 0s - loss: 0.2062 - acc: 0.9216 - val_loss: 0.2312 - val_acc: 0.9017\n",
      "Epoch 986/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9141Epoch 00985: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2152 - acc: 0.9184 - val_loss: 0.2537 - val_acc: 0.9051\n",
      "Epoch 987/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9210Epoch 00986: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2036 - acc: 0.9208 - val_loss: 0.2353 - val_acc: 0.9051\n",
      "Epoch 988/1000\n",
      " 8/18 [============>.................] - ETA: 1s - loss: 0.1826 - acc: 0.927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.687001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.344501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9201Epoch 00987: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1978 - acc: 0.9200 - val_loss: 0.2542 - val_acc: 0.9017\n",
      "Epoch 989/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9201Epoch 00988: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2016 - acc: 0.9200 - val_loss: 0.2375 - val_acc: 0.9119\n",
      "Epoch 990/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.1969 - acc: 0.9200Epoch 00989: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2155 - acc: 0.9162 - val_loss: 0.2546 - val_acc: 0.8983\n",
      "Epoch 991/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9149Epoch 00990: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2009 - acc: 0.9171 - val_loss: 0.2312 - val_acc: 0.9051\n",
      "Epoch 992/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9184Epoch 00991: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2081 - acc: 0.9183 - val_loss: 0.2817 - val_acc: 0.8847\n",
      "Epoch 993/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9036Epoch 00992: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2238 - acc: 0.9043 - val_loss: 0.2386 - val_acc: 0.9119\n",
      "Epoch 994/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9175Epoch 00993: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1980 - acc: 0.9196 - val_loss: 0.2381 - val_acc: 0.9017\n",
      "Epoch 995/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2138 - acc: 0.9127Epoch 00994: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2063 - acc: 0.9176 - val_loss: 0.2487 - val_acc: 0.9051\n",
      "Epoch 996/1000\n",
      "17/18 [==========================>...] - ETA: 0s - loss: 0.2156 - acc: 0.9145Epoch 00995: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2073 - acc: 0.9158 - val_loss: 0.2472 - val_acc: 0.9119\n",
      "Epoch 997/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9097Epoch 00996: val_loss did not improve\n",
      "19/18 [===============================] - 2s - loss: 0.2163 - acc: 0.9143 - val_loss: 0.2460 - val_acc: 0.9085\n",
      "Epoch 998/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9219Epoch 00997: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1998 - acc: 0.9216 - val_loss: 0.2645 - val_acc: 0.8915\n",
      "Epoch 999/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9227Epoch 00998: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.2015 - acc: 0.9182 - val_loss: 0.2469 - val_acc: 0.9220\n",
      "Epoch 1000/1000\n",
      "18/18 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9262Epoch 00999: val_loss did not improve\n",
      "19/18 [===============================] - 1s - loss: 0.1929 - acc: 0.9215 - val_loss: 0.2449 - val_acc: 0.9153\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "#train_tensors = X_train_new\n",
    "#train_targets = Y_train_new\n",
    "\n",
    "optimizer = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "optimizer_small = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "optimizer2 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=optimizer_small, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "#res = model.fit(train_tensors, train_targets, \n",
    "#          validation_data = (X_valid, y_valid),\n",
    "#          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "res = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid, y_valid), verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDoAAAQ8CAYAAABzdP6kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbbaffbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graphs = [res]\n",
    "\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, hist in enumerate(graphs):\n",
    "    ax1 = fig.add_subplot(110 + i + 1)\n",
    "    #plt.setp([ax1], xticks=[], yticks=[])\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_ylabel('loss')\n",
    "    #ax1.set_xlabel('epoch')\n",
    "    ax1.plot(hist.history['loss'])\n",
    "    ax1.plot(hist.history['val_loss'])\n",
    "    ax1.xaxis.set_ticks(np.arange(0, epochs, epochs // 10))\n",
    "    ax1.yaxis.set_ticks(np.arange(0, 1, 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "prediction_model = load_model('saved_models/weights.best.from_scratch.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test.json'\n",
    "test_data = read_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test, y_test, ids = prepare_dataset_ignore_angle(test_data, global_min, global_max)\n",
    "X_test, y_test, ids = prepare_dataset_with_angles(test_data, ptocessing_lambda, ptocessing_angle_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = prediction_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 1)\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'is_iceberg']\n",
    "\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for prediction, item_id in zip(res, ids):\n",
    "        csv_writer.writerow([item_id, np.asscalar(prediction)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aind-cv]",
   "language": "python",
   "name": "conda-env-aind-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
