{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Capstone: Iceberg Classifier\n",
    "\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "Icebergs presents threats to the ships navigation and various offshore activities. Especially, it as actual problem for the area offshore to Newfoundland and Labrador known as Iceberg Alley. The primary iceberg detection method for now is aerial reconnaissance using vessel-based monitoring data. Also, data received though satellites are widely being integrated now onto the monitoring systems greatly reduce monitoring cost. Additionally, Synthetic Aperture Radar (SAR) satellites can still monitor in various weather conditions such as clouds and fog.\n",
    "However, manual visual classification of SAR images to identify iceberg is very time-consuming process. So, C‑CORE company (https://www.c-core.ca/) has developed a computer vision system that analyzes SAR data to automatically detect and classify icebergs and vessels. Now it challenges ML community to build effective classification algorithm for their detection system [1]\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of the project is to build an algorithm which can reliably classify data to identify either it is iceberg or ship, based on given Synthetic Aperture Radar data. Also, the results are clearly measurable using prediction accuracy and it is important to have classifier with higher accuracy (ideally 100%).\n",
    "Additionally, analysis and classification SAR data is interesting problem. Even if it seems like standard image classification task it has some important differences which makes it challengeable to use pre-trained neural networks with transfer learning for the image classification such as VGG [2] or Inception [3]:\n",
    "• SAR data is not a three-channels regular image\n",
    "• Radar detected shapes are different than visually detected shapes.\n",
    "• Data set has additional incidence angle parameter of which the image was taken. So, it is additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "CORE provided dataset of satellite SAR images containing either a ship or an iceberg including 1604 training samples and 8424 test samples (5000 from them are autogenerated) Data was collected from SAR which bounces a signal off an object and records the echo, then that data is translated into an image. Two channels of image are provided: HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). See [4] for more details. Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format:\n",
    "\n",
    "Data was packed to the two JSON files (train.json, test.json). Each file consists of list of satellite images in following format: \n",
    "* Id of the image \n",
    "* band1 – flatten image data of the HH channel (5625 elements, 75x75 image), each element is float value measured in dB. \n",
    "* band2 – flatten image data of the HV channel (5625 elements, 75x75 image) , each element is float value measured in dB. \n",
    "* inc_angle - the incidence angle of which the image was taken \n",
    "* is_iceberg – classification label of the image. 1 is for iceberg, 0 for ship.\n",
    "\n",
    "Training dataset the only dataset which has labels assigned – so it will be used for training and validation. Test dataset does not have labels and will be used for the model evaluation.\n",
    "For the model features we are going to use band1, band2 and inc_angle data as features and is_iceberg field as labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function to load data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to find global dataset characteristics (signal strength min/max, angle min/max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_signal_minmax(data):\n",
    "    mins = [min(min(item['band_1']), min(item['band_2'])) for item in data ]\n",
    "    maxes = [max(max(item['band_1']), max(item['band_2'])) for item in data ]\n",
    "\n",
    "    global_min = min(mins)\n",
    "    global_max = max(maxes)\n",
    "    \n",
    "    return global_min, global_max\n",
    "\n",
    "def find_angle_minmax(data):\n",
    "    global_min  = min( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'  ])\n",
    "    global_max = max( [item['inc_angle'] for item in data if item['inc_angle'] != 'na'])\n",
    "  \n",
    "    return global_min, global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions to extract and display 75*75 image from raw SAR JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_image(data_item, key, image_size = 75):\n",
    "    image = np.array(data_item[key])\n",
    "    image = image.reshape(image_size, image_size)\n",
    "    return image\n",
    "    \n",
    "def extract_images(data_item, image_size = 75):\n",
    "    hh_image = extract_image(data_item, 'band_1', image_size)\n",
    "    hv_image = extract_image(data_item, 'band_2', image_size)\n",
    "    return hh_image, hv_image\n",
    "     \n",
    "def display_image(image, cmap='gray'):\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image pre-processing and normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_min_max_scale_sar_image(image, global_min, global_max):\n",
    "    image = (image - global_min) / (global_max - global_min)\n",
    "    return image\n",
    "\n",
    "def local_min_max_scale_sar_image(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    \n",
    "    image = (image - img_min) / (img_max - img_min)\n",
    "    return image\n",
    "\n",
    "def local_standard_scale_sar_image(image):\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    \n",
    "    image = (image - np.ones_like(image) * img_mean) / img_std;\n",
    "    \n",
    "    return image\n",
    "\n",
    "def flatten_image(image):\n",
    "    image = image.flatten()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 2-channel Images for CNN from HH and HV SAR channels ignoring angle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_ignore_angles(data, process_func):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        hh_image, hv_image = extract_images(item)\n",
    "        hh_image = process_func(hh_image)\n",
    "        hv_image = process_func(hv_image)\n",
    "        \n",
    "        image = np.dstack((hh_image, hv_image))\n",
    "        X.append(image)\n",
    "        if 'is_iceberg' in item.keys():\n",
    "            labels.append(item['is_iceberg'])\n",
    "        else:\n",
    "            labels.append(0)\n",
    "            \n",
    "        ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 3-channel Images for CNN from HH and HV SAR channels + angle data as separate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image)\n",
    "            hv_image = process_func(hv_image)\n",
    "            angle_layer = np.ones_like(hh_image) * angle_processing(angle)\n",
    "            image = np.dstack((hh_image, hv_image, angle_layer))\n",
    "            X.append(image)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create flat features for simple classifiers algoirithms like Logistic Regression, Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_flat_dataset_with_angles(data, process_func, angle_processing):\n",
    "    X =[]\n",
    "    labels = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in data:\n",
    "        angle = item['inc_angle']\n",
    "        if angle != 'na':\n",
    "            hh_image, hv_image = extract_images(item)\n",
    "            hh_image = process_func(hh_image).flatten()\n",
    "            hv_image = process_func(hv_image).flatten()\n",
    "            angle_layer = angle_processing(angle)\n",
    "            x_item = np.concatenate((hh_image, hv_image, [angle_layer]))\n",
    "            X.append(x_item)\n",
    "            if 'is_iceberg' in item.keys():\n",
    "                labels.append(item['is_iceberg'])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "            ids.append(item['id'])\n",
    "    return np.array(X), np.array(labels), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_image_part(data, margin):\n",
    "    return data[:, margin : -margin, margin : - margin, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1604\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/train.json'\n",
    "train_data = read_data(train_file)\n",
    "print('Training dataset size: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum incidence angle = 24.7546, Maximum incidence angle = 45.9375\n",
      "Minimum signal strength (dB)= -45.655499, Maximum signal strength (dB) = 34.574917\n"
     ]
    }
   ],
   "source": [
    "image_size = 75\n",
    "\n",
    "angle_min, angle_max = find_angle_minmax(train_data)\n",
    "global_min, global_max = find_signal_minmax(train_data)\n",
    "\n",
    "print(\"Minimum incidence angle = {}, Maximum incidence angle = {}\".format(angle_min, angle_max))\n",
    "print(\"Minimum signal strength (dB)= {}, Maximum signal strength (dB) = {}\".format(global_min, global_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and run image workflow to  extract training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    #image = cv2.bilateralFilter(image.astype(np.float32), 5, 80, 80)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "#X_train_initial, y_train_initial, _ = prepare_dataset_ignore_angles(train_data, ptocessing_lambda)\n",
    "X_train_initial, y_train_initial, _ = prepare_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "#X_train, y_train, _ = prepare_dataset_with_angle(train_data, global_min, global_max, angle_min, angle_max)\n",
    "print('Training dataset size: {}'.format(len(X_train_initial)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_channels(data, index):\n",
    "    hh_channel = data[index, :, :, 0]\n",
    "    hv_channel = data[index, :, :, 1]\n",
    "    return hh_channel, hv_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnWvMXld15//LdkJIbHIDEudifMnVEGJKxHCJEANklOkg\n+g2B1KFTVeJLZ0Q1HXVKP4w0HyrxqWo/jCohSqejMm0ZWjQVqogyDRWtqBiSSaJcnMTBcWI7iRNC\nICFAEtt7PrzPOv49r/ffz/P68jjve9ZfirJ93nP2Ze19nvNfa6+9VrTWVCgUxod1Z7sDhULh7KBe\n/kJhpKiXv1AYKerlLxRGinr5C4WRol7+QmGkqJe/UBgpTunlj4jbI+LRiHg8In73dHWqUCicecTJ\nOvlExHpJj0m6TdIBSd+X9JnW2sOnr3uFQuFMYcMpPPs+SY+31vZKUkT8paRfkWRf/nPPPbedd955\nx12PiKF89OjR7vVzzz33uOdef/31oXzkyJGh7H7Q1q07RnQ2bDg2dNc+y726169f323/8OHDQ/lN\nb3pTt515+ph1sj7+nZinbl7n/ayT9+T4e31aft31izLiOChb9+xK4MbD62yf4+SYssy/n3POOd02\n2Y67h3X31tPyPuY987wTRPb31Vdf1eHDh/s3LcOpvPxXStqPfx+Q9C9O9MB5552nW265pXs98fLL\nLw9lCnTr1q2SpgXx7LPPDuWf/OQnQ5k/ChTW+eefP5Tf9ra3de/5xS9+MZR//vOfH3cP6960aVO3\n3y+88MJQ3rFjx1DmDw4XIsEfuazzRz/60XCNsqIsKCu3yF599dWhzB8llvnsz372s+P+/sorr3T7\nSlnwxXnLW94ylF988cVuPax/1g8B+8cfFo6f9fH6j3/846HMuf3pT3963D2vvfbacG3z5s1D2a0n\n3kO89NJL3TY5/+xj3sO1wjXJsfU+OA8/PD/xPpWXfy5ExOckfU6a7nihUDi7OJWX/6Ckq/HvqybX\nptBa+5KkL0nSpk2bWv5y8wvGX0Fe59eEv9oJ/vLx15F1XHTRRUM5v2SSp52OmuYvMX+R+avOL8UV\nV1zR7RfBPjqVoYc3v/nNQ/nCCy8cyu6rynsoC46ZbIbP8qud2Lhx41B2ag/rIyNyoCwuuOCC4/pC\nGbJ/lPnFF188lN2Xkl9qxxryWY7TfaVZN9cFmcTzzz8/lDlO1kN5sZ4E+8K/k2FlHStRoU5F2fq+\npGsjYltEnCvp05L+9hTqKxQKC8RJf/lba4cj4t9LukPSeklfaa09dNp6VigUzihOSedvrf2dpL+b\n9/6IGIxEpGakjKRDBw8e0yKSMpGKk/aQ9jmjiFM1SKXcrkHSTRq5nPHJWdvZL6oMBOun4bDXb4J0\nmVTbqUOkmjQEEjk+1s1xOgPWoUOHuvWxfdJTZ6zNtUCqT1BW+/cfsz2//e1v7/bXqVecryuvvLLb\nVoKy5fyT6nNsl1xySfc6Zd6z5lNWVFfY154B1+0G9FAefoXCSFEvf6EwUpzxrT4H0hrSR9I+Uqmk\neJdeeulwjRZugs/ROk7rMCkjaaWzpnKvvVeHGw+pHvtCFYDtsM58lrSQfyftI9XmzgjbpOWZfeQ9\nzrLd+zspMMdP6zTniHPHOSJV5fxnmX93TkaUC3d1eN2pOux7zovbGWCZ9VGNY33zOF+xj7n+epZ8\nyftw5NzO2i0i6stfKIwU9fIXCiPFQmn/66+/rmeeeUaSdPnllw/X6cxDmkT6RHfcBOkqaSwpGCm9\n8+0nSCtJ8RJ0fKHTDPv63HPPDWXSR1JNp46QDifFc5ZpUkDWRws/1RXeQzmTJnN82S9a+52vups3\nqjds3zlocV56uxC8l+1Tzg6U3Sya7s5kcE24nRTnZEQ5z3LGcTsDXOenivryFwojRb38hcJIcdas\n/aS3zue5Rw1JtZ21lXX0jgJL01TKOfn06C77TXWFoMMRqR7HSXXEqQl0EEm4o6uk646aupNirIcn\nJbNOUtcDBw4MZTrTEGyfVJ9jptrHuaB8cx6dTB577LGhzHm+6qqrhjLHRhWIc9RzXGK/HaV3lnd3\n8pJrgW1yfLnO+HeqhVQBV2LZ76G+/IXCSLHQL/+GDRuGX27+CvKLwC87v8JpKOTf+YvIPWz+kvKL\n6E6e8X6eguMXKd0+9+3b1x8cwH6xHXfdIb+4lBXHQObhjELzjNkxr3QvdifW+Jz7kvOLzP1/ui73\nXHpZvzO+MVaC+1L2gsdI0mWXXTaUe/4K7BP9JihPZzTuGYqXo8dwCLJOzhXXApmMYyEnQn35C4WR\nol7+QmGkWCjtX79+/WBocS6ddFMlfUu68/TTTw/XSHVIk1x8NlJAlmkspEGH9PWHP/yhJOmGG24Y\nrpGOsd+kcc4dlCAFpFqTfXGuuFSXHO2jUcj5HBDsY6pMrJsUPFUxadqARpkTPL3HeaRqxnBYWQ+f\noxpBWRCkxmzHuT1T5umO7E59UuYuViPn3xkziVxbkvTWt75V0vTad/EBOZ/zBE1ZjvryFwojRb38\nhcJIsVDaHxHDvquzWpPi9iyepF3Oksvn3D0E6ZNzn8z+Ov8AgtdZN9ULWmppBe9RPKpIpMAsc5yO\najpZPPHEE0OZ85L7+GyH4D4/9/8Jjo179IxzyPodrU64MOKUJ6k+dyGuueaabh8556kaUP2j3wBV\nFO5YOH8KguPnsz31knC7Ub2dkUXF8CsUCqsY9fIXCiPFQmn/0aNHB1rnXBN71m7pmNWcllwXeMJZ\n1Vk3LdiuL6yn55zkYttxt8EFcOCzzkGnl73F0Tp33akUtJpfd911Q7nnMs2xudwLpLqUM9ukG3Pv\n9OLy9nMHwcVNdBlzqN6QJrugJT2rPZ2ASLtdcBL2261hrh32heVcL5zzXlAbqa+irsTZZ+aXPyK+\nEhHPRcSDuHZJRNwZEXsm/7/4RHUUCoU3Huah/f9d0u3Lrv2upL9vrV0r6e8n/y4UCqsIM2l/a+07\nEbF12eVfkfSRSfnPJP2DpP88T4NJZ+jM4XLY9QJE0GJOxwY6mbAOF3DBhaB2zhJ5P/vaC619Irjc\neqT9dETpJcokje/1T/IOTC62HVUTji/7yP6xvne84x3d+qjSUP5sh3NEmsz6s11a9d0ZAneeg2WX\nWLSnPrpcghwb5ZnOOVJ/90Cann+385JtufiEVAE4V9mvRVj7L2utpXvXs5IuO9HNhULhjYdTtva3\npZ/zvlVLS4k6I+LuiLibLouFQuHs4mSt/YciYnNr7ZmI2CzpOXcjE3VefPHFLekZKQvLLrz1448/\nLmma9vDvzpmFdbtU4E7VIMXOZ1kf6yA1JQUl7XNBQ1xI8V7/3M6EO8NA6zTb51kE3k9qnuNzdN1Z\n/p38CScLOsL0dkrc8WaqANu2bRvKpNpPPvnkUHZy7NFxpxZx/KzPOYq5LDwuEWmCMqHaQfTeq1k4\n2S//30r6tUn51yT975Osp1AonCXMs9X3F5L+WdL1EXEgIn5D0hcl3RYReyR9fPLvQqGwijCPtf8z\n5k8fO5kGe1SOVMX5bidlot+2c1qh5Z+WZNJ09oNUyjnfZL/ozOGOffacNqRpCjpPtJtsk6oDx8Az\nEc567aLKMBQ676eakOOgnNk/joFWeNJeUmOOk+oVKTCda7Lve/fuHa5xN8BRYEe7Ka+nnnqq28ec\nR8qHc8iIUU7tocw5d712pOm5S99+7h4QfI7yzHIl6iwUCjNRL3+hMFIs1Le/tTZQX1rnae12jjO9\npI20npLSueSMvN9Fm2FfSNOS4pIWu4SMHAPpGym7Sw7aOxpKRxXKjXTZJeFkX971rncNZY6DIbC3\nb98+lHuRckhjWSZIe3lcmPSZsuPxXso8+0j/fM6zS2BJOTvVgH75PX94ypBzwvVBFcX53PdUR2k6\nRDp3J7Zs2SJpWr3hPLCvbMdF+zkR6stfKIwU9fIXCiPFQmn/kSNHBqpIOnbw4MGhTMszaWVSPNI+\nlxPeBUp0wS8JFyM96RvpFak+I7CQxrGd3hHhEyHpPqPUUG6kyLRO836Oh31hPVdfffVQ7qkgpLr8\nO9URB86X2x3pRa9h+6TULjiqO+rr5tlZ6nOOSPXZP64zF7STfaEaRXWE0YF6jlUuwSfBPmZ5JVl8\n6stfKIwU9fIXCiPFwhN1JlVm0EZaLV3yy6Qz7nAQqRPpOJ2CSDVp7SWtdBQ022cdpJ2k8VRjSMc5\ntl6KqOV19qz9pPT79+/vPkdQ1XGypbx6VJI0lk5THDOvM6UZn2XAT6eacXxZpwuO6aL3cDzc1aHM\nXeShVBPyLIk0ffyczkyk4xwnZT5ProTeUW/W4YLa9najVoL68hcKI0W9/IXCSLFQ2r9u3bqBBtH5\nxOV5J8VK6kfnh95ugOSDZtIKzHtIK0mNaSlOmup81QkXkNLlZCdlI2V+97vfLclTWoLtMMIO6e2e\nPXuGMtUu3k9rcaoblDnpOlWke++9dyh/+MMfHsru2LOz9mc2ZLZFVY9zTpXKOWeRPpOmO2t+qgCk\n1xw/VQ2qY+4YNdthvygXF2EoQScnriF3DmZe1Je/UBgpFm7wy68if1n5heUvGH8p85edX1sXupug\nMYf38wvLLx9/kXshwN1pNP4K8ytEFsCvJvvCe/g1SRdQGidpZOMXi19Byo3ypKuxC/pAeWU93JN2\nhkX6Njz00EPdfvUSUkrTYyYLytOZ/KqSsfCL7PbcnZ8F54shxbMvW7duHa6RbXD9kSVSLmyTDJPr\nxcVzzLZYn9vn7zHCOtVXKBRmol7+QmGkWDjt74EUiLSF15NuuVz2pGCkV6SafPbGG28cyoztRpDu\n5ckztkNXT1KzeQJruDDO7Hsaq0jdSXvplkvwfqpXpKxsk0Yp9qtnOKUMuf/Me9mOM/g5mffou3Mp\npk8A5cx2GNjFhf3mfOX64zj5dz7HuXBxIClbysWpTzlm+nBQ1aMKSKSqsRLDX335C4WRol7+QmGk\nWHiizqRNpGykcs5qn5TJhZEmvXRZekgBCfoZMMgCr/f2f13Szt4prROB/SXtz10DF7TChdHmdefz\nQJmTgrKc7bJPLFPtcm7U7nQaKTgpdi/hKaksqTP3v1kHVTOqbty14PhJ03N9uixS9BtwocMJtwvg\n1ISUOWXLcXI9s45UTU6rtT8iro6Ib0fEwxHxUER8fnK9knUWCqsY89D+w5J+u7W2U9L7Jf1mROxU\nJessFFY15gnd/YykZybllyNit6QrdRLJOltrA8V99NFHh+vXXnvtUGZcNFpZkz67U3WklKRJdLhx\nVnOqGnQ+6QWOcG7BbMcFvyA1ZT2koz1a707AkfaRjpP6sS/zOBZxzCkvUmfOCWVOt1yqOryHuyOk\nzy4LUj5LlYrjJKhGEG53hqA6mG26RJ3uVCnHQ5X2mWeeGcougWhvTFzDDr214tS8HlZk8Jtk632P\npO+pknUWCqsac7/8EbFR0l9L+q3W2kv824mSdTJRZ89dtlAonB3MZe2PiHO09OJ/tbX2N5PLcyXr\nZKLOiy66qKWvN8NI84Qf6SMtu+ncQLruYvgdOHBgKJMakV7Ok+eezyYFplXXnfwi2CapHOtx1vl0\nomEdLhsN6bqzJLN9d/Ksl9iUcuYOCPvKACak+uwX++4chDgvs7I6O+cb0nQXIp3386NEWSRcgBeu\nFRcfkmu457S2/NlcZ+68C5/je3NGaH8srZ4/kbS7tfYH+FMl6ywUVjHm+fJ/SNK/lfRARNw3ufZ7\nWkrO+bVJ4s4nJX3qzHSxUCicCcxj7f8nSc5zYEXJOo8cOTLQTdKuHTt2DGXSxN6xU+dk4VQAOu3Q\nR5rPkiqRbrGttELzWi9RojRNI10eelrweT+dO7JflJWLG+goKK3dbpwOWT/rc+cA2BfK38VndGG8\nCR4H7sHRdfbFBVwheo5YXE/shwsaw7ll++78AXdQeD3n3/2dOxNuzc2Lcu8tFEaKevkLhZFi4b79\nSQNJbwkXdjlpHekqjzfyOusmHaS1m/eTVpOO90JD8zkmx6Tlmf1y1JDhoAlSvCyzblLtefzJ3dkC\nOpa4HY7cWXHW814iT8lbnN2xX/aFNLmXtNUlZO05hEnTzjTOR59qSs/az347lY7orVtpenfIOUvl\nmJxK2Usqyr5UJJ9CoTAT9fIXCiPFWTvS66ghaU0vtzodP+jw4/yv6fPtjleSKtGaSpqYVJ5nD5h7\nntZhqgO0/LrjxaR9tI5n++64LsfgnGZIQXmPs8hTRum4w7MXVGk4flqnSYc5n/P4q/Oe7BepM1Ud\nF5mI4HicIxTrSdk5VYty4zzzuqPe7CPVDs5XtsW+csxUi/hcOl+5CEE91Je/UBgp6uUvFEaKhdL+\n1tpAyV1EEnfsNGmYe87FbXdUl2UXcLPnXEE/b1rsHTVkDHueW6AV2lmb02pNf3qXh97tPLhklqTp\nVJMYfz+dW1wEJOY7cHU7izjpvZvzpMZuTng+gXPRO4q9HL3MOJK669PlZODxb+4CUI0jNZ+Hkmc9\nTi126kJZ+wuFwtyol79QGCkWSvvPOeecgcKSDtKC6QI0pmWfTjiMW+/8/EkjSdlJj5yzCulr9sWl\nyHJJOK+//vqhzDGTGvJ+OrEkfSXtZDusj7LgPS51GB2hbrrppqHMo8lZjwuqSmca1kd5UoYcp9u1\nYPu5VpyTi0vLxXXj1CTKhXORY3JHrlmHUx05fjdmrmOqgDlWrgOqrqy75wS1EtSXv1AYKc5axh6X\nycT9yudXzsV7435+LxSzNP0r7AxRBA06vV9Zl2yUIAvphcWWZn8RKAd+SQmOnyyIsuCX0jEsGveS\nbTGRJY1MDJrCr6AzMrpsQJRFL5MQWYVzReb89E5GLr/HZezJ9l1yTMqNc+Xcrunn0UvIKU1/zdNw\nzfeDsmVMwF5Y8trnLxQKM1Evf6EwUiyU9kfEQGu4L0+4/dfeni+puwuyQDpOwwpBKsf7SbHSoOYC\nTLBu5xpKekfXZLcXnPSVBlF3YnFW1h/Jxy1kf3vJT/nc448/PpS5t04KSnrL8dOIxfFTveL9Sb2d\nQdbJme1zXTA5KNUbrqOcC64DF2zFJVvNpK7L4Xwhen4OLnQ4+811k/KsRJ2FQmEm6uUvFEaKhdL+\ndevWDRSSe5e0SNNlk9b0pMOkPazDWZVJ2biH7HYV2BfWmZZdUmDSMVqSeZ2UlfTO7b/3TqcdOnRo\nKNP9llSfsiA15TjZL1J20mfKKF2Gn3rqqeHali1bhjKt3Wzf7WG7fXGqOr3EnrS8O/dWlpkNiqDa\n4bI65fhdElauD/bV7XawzDXCOadrdk81da6+rDvXggv20cM8obvPi4j/GxH3TxJ1/tfJ9UrUWSis\nYsxD+1+V9NHW2s2Sdkm6PSLer0rUWSisaswTurtJSh53zuS/ppNI1CkdoyUuFhqDRfSCYpDqutNu\npJ10BHIBP0iNncNN9pd0lX9nfWyHVl32l/SOVJsUc//+/ZKknTt3qgfW5zLGULakrKS17nRizgUT\nid51111DmZZlUlBau9kXyovj7CWc5D1UezhOBlah6sDdA7pjU2WcRY8pW6oLXLccg1Mj3W4H13HP\nfZj3cg05d+Vcc263rIe57oyI9ZOEHc9JurO1Vok6C4VVjrle/tbakdbaLklXSXpfRLxr2d/nStQ5\nK/daoVBYHFZk7W+t/Tgivi3pdp1Eos6NGze2pPKk94TLc55l0itaoZ0l3TnF0BGGtNLR9KSVLhQ2\n++qCiZDeO+cfjumd73znVNvSNK1zNJK02wV3oM85sXXr1qGc7T7yyCPDNTo57d27dyjzVKGTES3i\nqdJI03NK2fFMQe9eUvCef/7y6+wX1SG2mXPR87dffq+zwnMtukw6Lox3rgv2lWuL88Y5zz6e7kSd\nb4uIiyblN0u6TdIjqkSdhcKqxjxf/s2S/iwi1mvpx+JrrbVvRsQ/qxJ1FgqrFrESmnCq2LhxY8vA\nEc5ZhhZk0p2k6aT0pD2kTqRajvY6dYB0kKpB0j3WRz93WpjdMVIXXtodx02HGpeliLSY8fRITaka\nkJp/5CMfGcqcC1LWPLJLVcSdfXjwwQeHMlUdHvvlOGl5J8Vl/Skvqj3Oqs45dH75LrZgb+eHa5Iy\nZF94dJdzyzl0sf3ckfYe2D/2pZex6b777tPLL788VyC/cu8tFEaKevkLhZFi4Ud6kx7R4ePZZ58d\nynQoITXsRcpxR1epLuzYsWMoO19wl2Gm58RCKzBVB5d4lP12/X366aeHMrPj5HXSZVJHF0bc0WQX\nc5B955hzjjhXpLdUgXbv3t1th2WqCazTJRxNVY7tUEWg5ZvqijvnQflz58Wd+ejVR/WS/aaq447V\nuhiCRO4CsG46M1EWVKmyj3Wkt1AozES9/IXCSLHwI729SD48xkvraM/nnlSXdJW0jyC9dwEUSQ2d\nE0laxJ0PP9shTSRlo4WXVnu2Q4ejHB9VB1JHWu/ZPut+73vfqx5ID0kxWc7+uiCkvP7BD35wKJNG\n33PPPceNh3VL03PRy1vPvlIVnMdRyB2p5txxzOkgRas6n2NfqbpQ1SSomlDtcJmnEpQhx0z5b9++\nfShnJJ/T7ttfKBTWHurlLxRGirOWqJPBFF0AR1p2k844qzot9i5po4M70svrqZq4I7+kl6RsDqR9\n7DvrSWrsAny6vPWMUsM+cieBbTLyDdWqVEfoh044FYT9ImWlI5Dr+8GDB4dyUmbnT88y+0LLO+Fy\nKxApf46ZjlruKLhTQanGuaxKrCfXvAsI6yIJ5f0Vt79QKMxEvfyFwkixcGt/WjzpfEOaRCrX8613\njiKk0VdeeeVQdvSS/uyk2qR1pFhZD+sjXOoqUnBn7eb4e2maeI3jpKzckWLSblrNb7zxxqFMatrL\nEcD2WYezsLOPHDPVIUfNKbtUTbgOHnvssaFMuZHucj4ZDYr3cBzXXXddt84ELf8E63AOPJS/c0Ti\nbk+uF8qNf6cK0lNpT+uR3kKhsDZRL3+hMFIsPEtvL9su6SCtnKTdSZn4HMukUXQacrnqSced5bWX\nRsk5ZzzxxBPdvhBUU1z2XNK2tPySCpMuu6yz3CX5zne+M5T37dvX7RedZUj7r7nmmuP6R3rL65wL\nd1aA9JqqBnHvvfced41tUl3kUWOOmZZ6pzJRNehZ07kz4PJAsD435zy3wXXGtUDHodzhcMd8ueZ6\n6vJpjdtfKBTWJhb65X/99dcHwxh/nemOyl88Gvfyq81facaqI3tw8dlYJjsg23B76vks66YxiUyi\nl+lnOfjlp0GJ4+8ZF3mNJ/xciHLG5KPh7IEHHhjK/Nqzzvxqc2xkIZw3foUoFwYZ4deWXyie6rz+\n+uuHcs6RC5Hey+i0fAzuhB+ZEu9PGfFejpntz+NGzPVKhsU1RzaXfg70tyB7oszZx2RbLnhND/Xl\nLxRGinr5C4WRYuHBPJIquUSVpC2kqb1MJjS+MGgEr7ugCb088JLPBd8LluCMky62ICm9yzbD/maZ\nfaVa4oJgkMbT54Fg/D+XzLR3kpFjo1s2KS3HRsrO8XMcN9xww1CmfFPto0GMzzGGIvvlTl7SR4Aq\nQ081Y9h2yoSGQO7hc/7ZR46ZBk/2keso16IzIFIFowrq1sKJMPeXf5K1596I+Obk35Wos1BYxVgJ\n7f+8pN34dyXqLBRWMeai/RFxlaR/I+n3Jf3HyeUVJ+qMiCmqkiDtoqWUVCZpGp93lnTnouvudyfC\nSLESLkaa2wsnZaSllveQ9pGC5/gzUIPU34df3i+XDcnRYUcZmaknwaAdPJlJOsw5dME32D7VB85X\njuOOO+4YrlEFoOpCFdG5xhKMFck681kXYITgmN34Sd+5a+Di/2VblBvBeaYM5zmxuBzzfvn/UNLv\nSKIHQSXqLBRWMeZJ1/UJSc+11u5x98ybqNP9ChcKhcVjHtr/IUmfjIhflnSepLdExJ/rJBJ1btq0\nqSVtJ61yATJ6p5ZIXUmXSK9c3nRH+/msOwWYdMw5xNA6zDpI6R29dqfNsu8u8Sif4y4IaSp3L5zb\nK51yetmOSMXpIkzZcpxsk/dTXqSpt95661DuhaneuXPncI0hwql2UBYuoAV3PkjBOb7sey8JpjTt\nTEVHHK5FjsG5kfdcx3nd7QZwrtiv7PdpdfJprX2htXZVa22rpE9Luqu19quqRJ2FwqrGqTj5fFHS\nbRGxR9LHJ/8uFAqrBCty8mmt/YOWrPpqrb0g6WMreT4iurSEVIZ0a9u2bUM5zwSQ6pDGuvDSpJek\nevQnp1MGQy1TNcm2SKlpYWZfWCZNZH2kdYxb16P9jEnoTnWxTBWAlJGnALkLQZn2/OJ5jfdSRWC/\nSYF5eo9jpoWf/aL8c/yZsFTyKiLbcdlzKBfuFLDNnF93DoHXOX6n9nFtOws+5ZtqMVVEF8CD48+1\nWKG7C4XCTNTLXyiMFAv17T969OhAyUjfSM1IqxgIISmoC/NNekU6ThWAVI9+4XSWoUMN20r1gZSe\n1IyUluoFaRrpJYN/kPbR+SQtxZQVKS3p5d13360ePvvZzw5lF/CEtJJ937Nnj6TpkNM8Zk1rPy3f\nzCTD/rrj1ayHFvycF6oaVIEY+MNZ+OkURjly14ZzmuoQ1VOqjqzP0XjuyFC9pMzdjlSufxcrkeuM\namSqIJWos1AozES9/IXCSLFQ2n/kyJGBqpDKkOrS8t6jMC6GH63qtILS4YI03jn8kD5SrUiruQvL\n7JKAErQ2s25aaHsZaXpOKNI0pbz55puHMndJmI2HbZKOcy6ojqR8qQpRdeF1yoUqmAvFzjGzHtL6\nnCOqRYzbd//996sHtsOxOfWBSNrvMua4nSTOOZ/lGuazVLWIXFOs26kARMqoYvgVCoWZqJe/UBgp\nFh66O0F6Mk+o66TMpFeORhK9yDjStKXcRQHqZcdx/SY1o7WXVJ+0j/2l5Zf1ZB95jXSQ/Sa95xjo\nF08LN2k/d1Xoi57WccqZyTbZl127dg1lzhvHzN0RRu8hepmHqBZQvWNf+RzH785ZuOO1eYyYtJxU\nm7LgPVQvqaa4XQ2qcj0VhCot6+P67yUKrUSdhUJhJurlLxRGioXS/tbaQMlIzXpx05cjKTtpHKmT\ny6RDZx69TkM0AAAUy0lEQVRSduaqd9bmXrQb0jFaskndOTaXOJG0k/SZskinEPrhu6wzPBZLMGgl\nqT6f5ThdnPsE4/CT0nJeXJBVypxUm7sWRN7Dsw90AuJuC+fQOY2RJlPOvfMCPP5LKk1nJqeuUu2j\nqulUQ9afcqRaSFWHsuW7kjss5eRTKBRmol7+QmGkWCjtX7du3UCPSXtohaWlmBS7R2foTOKOfZKa\nkWqSMvEettmznJKusx1SM4LXeeyT6oCzTidNd44drIOyIGW8/PLLhzLVB8qCjjtEqiOUiQvF5hyY\nKEMGBOVOBek7afp3v/vd454jHecxXufDT9WEa4jj55hyB8EFYaWKxHnh/ZxDtzvEuaPal/LiOQCO\nh+plb8eorP2FQmEm6uUvFEaKhdL+9evXd7OJkgJRBSBlzuO4pEjMQEsKRKpFStnLeitNU1bW39tN\nYB20UrvsqS7PO8dGyzPViqSYTlYcG9vPqEfStBXcUXb2i5b1PAtB+bhzCy6SEuGy3f7gBz8YylRT\n8vgwKbqjw3S4oWrg0oVRFtwF6cmI16iWuIzBvJ/qJe+nUxL7lX2hAxMdiIhe6rby7S8UCjOx8GAe\n+UtMAw2NMm7vNL9yNLgQ/PXmrx9/bV3d/FK6AA3ZLr82BL+wLmMOv/YuKAQZTH5ByAz4FSYLIcNg\n8A2On4Yrnt7jl4df5wS/9mQ7bg55epJMzhkCafzlqb2UF2XCLzznk3XQEMg23Uk+yjHH7+ITMiAM\n4ZKp8lnew7lg3/N+3kvZ8jkyJp6MnRfzpuvaJ+llSUckHW6t3RIRl0j6K0lbJe2T9KnW2ouujkKh\n8MbCSmj/v2yt7Wqt3TL5dyXqLBRWMU6F9p9Uos6kmKSapEbM8NKj2KRujlLRmEIa62Kr8VQbKXgv\nRh3pMvvCvXLn0kuaTEMQaV1vzLyXBieXkJIGMsqC6g2f7WVGYl9cmzSmOTrugmL0Ti9K/RDcfI5l\nqjQ0kDlfDOfezDlPdYSqFqk255BryBmcKSMXC7DXX7ZPWXGuZhmHZ2HeL3+T9H8i4p6I+NzkWiXq\nLBRWMeb98t/aWjsYEW+XdGdETOVubq21iLCJOiV9TvKeaoVCYfGY6+VvrR2c/P+5iPiGpPfpJBJ1\nnn/++S3pDKkeqSSpDKlP0iru85JeOrfLZX0ZynS1Ja0mHSN9zr44qk0KSosw6SBVA9IzlnvhrWlt\nZ59cpiPW4dxLSVm5L06321QZqC6wPucWzHs4tt5OwvK+9+Iikrpz/tlX7rO7oC0uYxLXTrbVi98o\nTcuQ6gDnwiVH5Q4H9+6pMuX6467O3r17hzL9ILgWcmyn9VRfRFwQEZuyLOlfSXpQlaizUFjVmOfL\nf5mkb0x+wTdI+p+ttW9FxPclfS0ifkPSk5I+dea6WSgUTjdmvvyttb2Sbu5cX3Gizg0bNgwOMM7V\nlA4ydBZJmko6RHpPeudi+5HeuXDYRI+OkQJyZ8IFkCBNpdrBcbgQ4ElNXQYiR/FIBzl+qjSUHR2U\nemqCc9phoBTnxsp5YZmUmfSd9+QaIe0n7XUOXy6ZKK9TLrTCp1w4h5wrzg/XkMve1ItDuRy9+aeK\nRJVi9+7d3eeyv5Wos1AozES9/IXCSLFw3/6kRO6klKOySQ35dxd7jhSU97NNlkmlXajvHh2kPzWt\nug4uyAOdT0hxk1ayry4DDeleT3WQpumto6BEqgOUp7OC8zot2WzTBdbgSULKohc3kSDVJgV35zwI\nR+vzWa4J1s2+8DnKn7tUVJncbgfHkXJhm1xn8+yezIv68hcKI0W9/IXCSLHwjD09Wk9rs8vhnpSZ\nVJMgHWa5F5ZZmnbEcZZ30u18lveSOtKS73YSSNMcTaVq0FMlXHw6lw2G1x1Ndkkjs37OWU8VkvyR\nVlryeaTZqWOk1SkXF7qaTka8x4XF5j3sI8eU4PqgQxjVS6c6cT0TnC+OmWpPrm/ey35z/bN9l3j2\nRKgvf6EwUtTLXyiMFAul/TzSS5DquuSXeZ3OIaRrtKo6P3tScFqkac12+eSTPjqKzjbpqORCSrvM\nP6Ryeb2XkHF5HY6as0wq6c4FsP7eXJFGUxbsN9UbniFwlveetV06Nte0pFOGHDNl3ottt/x+Zv7p\njdMdeWb73G1xx6JdX7gjQNUo73fxJunkxPWZY6jQ3YVCYSbq5S8URoqFW/sTpKMuaGbPucRRR/rQ\nu2gmLoGmSxTai6DDSD49K7E0TXuparijrqSSPZq+Z8+ebp927do1lN3R1d6OhTRNk1nmPUm3aWGm\n3FjOMNvStHWcIH0nraZ1nOUcE1U6JyvK2fm3c565U0C1o+dQ5EKrc5eEZed849YL2891QfV2nt2W\npPtnIpJPoVBYY6iXv1AYKRbu29/LP04q5yzv6SPO/Oikg863njTIHbWk2sF7aDnNvjhKx90DUkf2\nl1SOlI330+Fjx44dkqZpNGXlgp26gJek9+5sAy3P+SxVCsqT1m4G0HRqFPvIuWUfWU/KxZ0JcE5T\nHBuDs7q56+UtcFmXOIfcmeAccseKAVQpf7bJNZftOkcxrluOJ/tYGXsKhcJM1MtfKIwUC7f292iJ\nS+PUo1uO1rjc66SX9JEm1eROAekby7287c6fmn1kX3qUdvk9pNJJWWlt53POmcftNrjY8gQpfsrU\nOblQ1XLRc1xKMfaL4ydlTou8c9riWmGZbVLmjqb3HKvcMWoXsYnrjH3hs7yHMu2tV6qiXLcuV0Lu\ntrgIWT3Ul79QGCkW+uVft27d8MvuMpYQPfdaF7qahhh+4fjL64xP/FLwV5u/7HkPv0L8IrvU1e4U\nHveZnSEsx0rDJkFjEvvistS4LEGUHZFfTceYKFvWR1lwP5+GS361HDvJeshSnEGY64kshH13Id17\nBk3n8uzCtTtW5U7+uWxTWXbMhMZUvgu5RhhmfRbm+vJHxEUR8fWIeCQidkfEByLikoi4MyL2TP7f\n99QpFApvSMxL+/9I0rdaazdoKZLvblWizkJhVWMm7Y+ICyV9WNK/k6TW2muSXouIFSfqlI7RGdIX\n0h7uM/N6UlnSLqcuOBpNKuUMI6SvvbDXLutOL+S0NE016QvgjJxEtulOPTrXYaoAfJZ0mLLgXJDK\nJq10AVScu2wvCMvyvpPKOrfXHBPlQwrOeWZfqA5xPRHsIw2R2ZbzQ3CJQl0AGZZ5Dw13nLueezGv\n0VeALtV5/XS7926T9LykP42IeyPiy5PMPZWos1BYxZjn5d8g6Zck/XFr7T2SXtEyit+WfiZtos6I\nuDsi7l7JNkShUDizmMfaf0DSgdba9yb//rqWXv4VJ+rcuHFjS6pEykRaw1NdpF5Jk9zergv57X5w\n6BpLXH311UOZNLXXJ7ef60Jx05/AhcPuWaRJEdm+u4f776TspLfsF6k078m2SCVpPSd48pB1U56s\n2/WdtDb7Ras2QQreC/ktTe8IUTXhuuDayTo5D+40KMfjYhhSdmzHJVPtgaoT1Ztepp/TSvtba89K\n2h8R108ufUzSw6pEnYXCqsa8+/z/QdJXI+JcSXsl/bqWfjgqUWehsEox18vfWrtP0i2dP60oUad0\njOKQ9jrLb8/a7vLQO3dRUlCXpYV00J22y367rD/ulCLvITWl5Z3l3mkv1n3o0KHueJz1mrHqSAk3\nb97c7RdVmRwHZe5OlVF1cAlRCaeCcP55PeEyE9GNl05GXAtO/r34ey7ZKC3sznWXa5sg7Xex/fJZ\n3utkxfnPsdWpvkKhMBP18hcKI8VCfftbawMtodWS9GlWYA136o5wlJr30wpNikVaRTqW/eY1176r\ng/SSIa2J7du3D+WkzOwfre0uAw+pH+VJyk4rOHc1ejsYbMcl53Shu50zF9tk/dwRyWfpHOXCjxPb\ntm0byi6en9tV6t1P1Y3nLDgerrPemRRpOrAL54Ljv+KKK45rk05LvM61ku248fZQX/5CYaSol79Q\nGCkWHsMvj4/SguqOt/Z88UnRSPuoItDy7Oils4o6y3PeT0rLMZBuuSPCzi+cfeGzSaV5L3O1u3iG\ntGrTIkx50imHYyJ9TypLSjuPNZkBN9gXwsVc7MUc5LyR9rIvVI2omvR2jKRp9Ylz3jtPwLKLCenm\nmXA7DBxTrmnWQTXCZSY6I04+hUJhbaJe/kJhpDhrobuvueaa4bqLjkIkrWLUGUdBnW99z2lH8uGd\nqT4kHXZUs5dpRpqOXkPKxvtdhBlayhN08qGjjjtn4OL80YJO9CLScJysm5Z5R7vdsVfez3FQ7cj6\nOecuqpBzinHZeDgvVFOyv+6INuk6Lfak2y68OefIHS/PMp2WCI6fazVlW7S/UCjMRL38hcJIsVDa\nv2HDhsGy7/y/SRNJ9x588EFJ0k033TRVX8L55PeoszRNux1I/dJqTssrfch7Iaelvq+8NG3t5jh7\n97igme4orDtGzGfZX0eZe4E96RzFNklHXfYaytOpRj0q73ZDXAQktklK3TuiLfUTfrrsTgTVQoJt\nUhYu8hKdeHpJa1mHO89yMqgvf6EwUtTLXyiMFAul/evXrx8o8Ury00vHnFjo2ELa5fzJScFJtXg/\naS8pWC/zjjuK7M4qkAK6wKLuOGhScLcD4pJTcjzzHMd1dDjpJncGqK6wPsrCRVUiTXeJMElxUy77\n9+8frjEgJ+efMueYnZpGNaUXNJbPsR2XsclFlepFCVp+f+88C+91R7d7c15HeguFwkzUy18ojBQL\npf0RMdAZd6TVOeUk9SFFJTWjZZoUkDTI+ZPTKYgUr2dlJR3jjoWzCNMvnbTz8ssv746jpzJQpaDz\nh7M2k/Y6VWcelSn7RVrudlUIl8zS0XHOC9tKau7UErbDe7jD4VQqypEUPOtxu1FcZ24ng5hnp8Il\nX+1dY31ciymrov2FQmEm6uUvFEaKedJ1XS/pr3Bpu6T/Iul/TK5vlbRP0qdaay8uf544evToQINJ\nzWjBJAUl9cqjrC5Q5Tw+zbzHHdl0DipJvVx2X9J47kgQLtsr+9WLUe+svT26Kvljzy7CkTt2y2CV\nCY6fVmp35sEFMHWUuRe0k7sNzrHH+fyzTRdwk+3n+mMdnB/2243ZHel1OxJUJXqqnIv9TzgV7ESY\nJ27/o621Xa21XZLeK+lnkr6hStRZKKxqrPTn4mOSftBae/JkEnUeOXJk+OK7cN38Ze/tHTvjB38R\nGdiCv4jOKMdnXRjt/OLyy8z+8V5+hWhkYt2sh2Pq/cpTJmQ7LiFn7+spTYfxpiyc22mOmTLkV9jF\noXN+CWQtNPJR/jSK5ph6e//S9PgpW4Jz4Vy6Ka+UtWMpnAsXn5Fy4Zec97BfPZddjtP5kPC5LDtm\n1MNKdf5PS/qLSbkSdRYKqxhzv/yTbD2flPS/lv9t3kSd7otUKBQWj5XQ/n8t6f+11jKaxEkl6kxa\nQwpGIx8pHqmXS/KYcHv7LDtfANJ3gtQs7yEFdBSMe8g0eJG+OUNYLyMNx86/01ei11dpmhq72HYE\n5Z/zQrWABkTW4Vyd2XcXW/H+++8fyjwpmM/yOdcXR68d3Phzjbh1wxOIbIdrgSoIP3jOWNtTgXgv\n1SinoqZbujMI9rAS2v8ZHaP8UiXqLBRWNeZ6+SPiAkm3SfobXP6ipNsiYo+kj0/+XSgUVgnmTdT5\niqRLl117QSeRqDPhrJKkO85NNUFLuqNJpHfOvZft8FQf60w6TFrsXJFZxzxJGwmOs2cjobpE6sxd\nAlqKnSWfaoezPPfUIcrQzSH7Tes9A4Hw2S1btnTbz50Fjo195fjZppsXt8NEWfSCtrhgK7yHZdcX\nqilupybBdUufGDeeCt1dKBTmRr38hcJIsdBTfdIxuueceRxl7VFgR6/4XC+rieTDMdOaSySdYr+d\nkwtpmst2w3IvzzrbJL1lmeoFA1K4xJ60wrMe7rZQXjkOOk2xDsqC10mT53F1dW7COS8cA+t2u0Rc\nCwzmQnC+OL4EaTnVFcLtKvBZ7nZQRlTfeidFuYb4frDM53puybNQX/5CYaSol79QGCkWHswj6R7p\nmMsw0zupRbrm/PAJ0iCX8NGdFOSzpNWznmPdLj4dLbwcR4/WuX7TscWF6KY8nYcl1YSeHz/r5rzR\nsky6zjHwOp2fqN5QFr1gJmyf4HOk2lQHemG5peldi15yVKoXlAllxfE7uk3VyO1ksY+5FqgKcc7Z\nL6pouUbOlJNPoVBYQ6iXv1AYKYLU4Yw3FvG8pFck9aNdrC28VTXOtYTVMs53tNaO11E7WOjLL0kR\ncXdr7ZaFNnoWUONcW1iL4yzaXyiMFPXyFwojxdl4+b90Fto8G6hxri2suXEuXOcvFApvDBTtLxRG\nioW+/BFxe0Q8GhGPR8SaCfUdEVdHxLcj4uGIeCgiPj+5fklE3BkReyb/v3hWXW90RMT6iLg3Ir45\n+feaG6MkRcRFEfH1iHgkInZHxAfW2lgX9vJHxHpJ/01LsQB3SvpMROxcVPtnGIcl/XZrbaek90v6\nzcnY1mJug89L2o1/r8UxStIfSfpWa+0GSTdracxra6yttYX8J+kDku7Av78g6QuLan+R/2kpnuFt\nkh6VtHlybbOkR892305xXFdpadF/VNI3J9fW1Bgn47hQ0hOa2MRwfU2NdZG0/0pJ+/HvA5NrawoR\nsVXSeyR9T2svt8EfSvodSYypttbGKEnbJD0v6U8nKs6XJ3Es19RYy+B3GhERGyX9taTfaq29xL+1\npc/Fqt1aiYhPSHqutXaPu2e1jxHYIOmXJP1xa+09WnJJn6L4a2Gsi3z5D0piSJSrJtfWBCLiHC29\n+F9trWWU40OTnAY6UW6DVYIPSfpkROyT9JeSPhoRf661NcbEAUkHWmvfm/z761r6MVhTY13ky/99\nSddGxLZJ9p9Payn2/6pHLB3s/hNJu1trf4A/rZncBq21L7TWrmqtbdXS3N3VWvtVraExJlprz0ra\nP8lQLS1FqX5Ya2ysiz7V98ta0hvXS/pKa+33F9b4GURE3CrpHyU9oGP68O9pSe//mqQtkp7UUhrz\nH3UrWUWIiI9I+k+ttU9ExKVam2PcJenLks6VtFfSr2vpY7lmxloefoXCSFEGv0JhpKiXv1AYKerl\nLxRGinr5C4WRol7+QmGkqJe/UBgp6uUvFEaKevkLhZHi/wPNwKHdMFzaxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0be3332e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx18ojBT18RcKI0V9/IXCSPH/AULrupEzoGneAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f08c425da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a500ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 24)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 25)\n",
    "display_image(hh_channel)\n",
    "\n",
    "hh_channel, hv_channel = get_image_channels(X_train_initial, 26)\n",
    "display_image(hh_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a5076630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a50765f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a525bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCaKevkLhYmiXv5CYaKol79QmCj+P9iPDg0s93lOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a525be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb5qV0g9/06yU/w98vSRBiq4YQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f0a4f92550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1250\n",
    "\n",
    "hh_image, hv_image =  get_image_channels(X_train_initial, index)\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "print(y_train_initial[index])\n",
    "\n",
    "import cv2\n",
    "\n",
    "hh_image = cv2.bilateralFilter(hh_image.astype(np.float32), 5, 80, 80)\n",
    "hv_image = cv2.bilateralFilter(hv_image.astype(np.float32), 5, 80, 80)\n",
    "\n",
    "display_image(hh_image)\n",
    "display_image(hv_image)\n",
    "\n",
    "\n",
    "image = np.dstack((hh_image, hh_image, np.zeros_like(hv_image)))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1471\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def process(image):\n",
    "    image = local_min_max_scale_sar_image(image)\n",
    "    return image\n",
    "    \n",
    "ptocessing_lambda = lambda image: process(image)\n",
    "ptocessing_angle_lambda = lambda angle: 1.0 - (angle_max - angle) / (angle_max - angle_min)\n",
    "\n",
    "X_flat_initial, y_flat_initial, _ = prepare_flat_dataset_with_angles(train_data, ptocessing_lambda, ptocessing_angle_lambda)\n",
    "\n",
    "print('Training dataset size: {}'.format(len(X_flat_initial)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1176\n",
      "Final validation dataset size: 295\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_flat_initial, y_flat_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 1176 samples.\n",
      "{'train_time': 2.3636345863342285, 'log_loss_train': 9.9920072216264148e-16, 'pred_time': 0.011500120162963867, 'acc_train': 1.0, 'log_loss_test': 6.9078393044748019, 'acc_test': 0.80000000000000004}\n",
      "AdaBoostClassifier trained on 1176 samples.\n",
      "{'train_time': 56.3687481880188, 'log_loss_train': 1.3815643824202646, 'pred_time': 0.19403505325317383, 'acc_train': 0.95999999999999996, 'log_loss_test': 8.3128281638591055, 'acc_test': 0.7593220338983051}\n",
      "SVC trained on 1176 samples.\n",
      "{'train_time': 12.975327014923096, 'log_loss_train': 8.9802097982656104, 'pred_time': 6.013159990310669, 'acc_train': 0.73999999999999999, 'log_loss_test': 8.898258258600988, 'acc_test': 0.74237288135593216}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree  import DecisionTreeClassifier\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "clf1= LogisticRegression(random_state = 3)\n",
    "clf2 = AdaBoostClassifier(random_state = 3)\n",
    "clf3 = SVC(random_state = 3)\n",
    "clf4 = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    start = time() # Get start time\n",
    "  \n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    X_train_pred = X_train[:300]\n",
    "    y_train_pred = y_train[:300]\n",
    "        \n",
    "    # TODO: Get the predictions on the test set,\n",
    "    #       then get predictions on the first 300 training samples\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train_pred)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples\n",
    "    results['acc_train'] = accuracy_score(y_train_pred, predictions_train)\n",
    "    results['log_loss_train'] = log_loss(y_train_pred, predictions_train)\n",
    "        \n",
    "    # TODO: Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    results['log_loss_test'] = log_loss(y_test, predictions_test)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, len(X_train)))\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "print(train_predict(clf1, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf2, X_train, y_train, X_valid, y_valid))\n",
    "print(train_predict(clf3, X_train, y_train, X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Alexey\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8e87bc4f6966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# TODO: Fit the grid search object to the training data and find the optimal parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mgrid_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Get the estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \"\"\"\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m                 for train, test in cv)\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1675\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aind-cv\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 3)\n",
    "\n",
    "clf_default = AdaBoostClassifier(random_state = 3, base_estimator = dtc) \n",
    "clf = AdaBoostClassifier(random_state = 3, base_estimator = dtc)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "parameters = { 'n_estimators': [5, 10, 20, 50, 100], \\\n",
    "              # 'base_estimator__criterion' : ['gini', 'entropy'], \\\n",
    "               'base_estimator__max_depth': [2, 3, 5, 10, None] \\\n",
    "             #  'base_estimator__min_samples_split': [2, 5, 10, 50], \\\n",
    "             #  'base_estimator__class_weight': [{ 1 : 0.75}, { 1 : 1}] \n",
    "             } \n",
    "\n",
    "scorer = make_scorer(log_loss, greater_is_better=False)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring = scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_default' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-78de5f13ba73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdef_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_default\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_default' is not defined"
     ]
    }
   ],
   "source": [
    "def_model = clf_default.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1f6552706f3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Make predictions using the unoptimized and model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdef_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_fit' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = def_model.predict(X_valid)\n",
    "best_predictions = best_clf.predict(X_valid)\n",
    "\n",
    "print (\"Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\n",
    "print (\"log_loss on testing data: {:.4f}\".format(log_loss(y_valid, predictions)))\n",
    "\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\n",
    "print (\"Final log_loss on the testing data: {:.4f}\".format(log_loss(y_valid, best_predictions)))\n",
    "                                                                     \n",
    "print (grid_fit.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split initial dataset to training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 935\n",
      "Final validation dataset size: 462\n",
      "Final tet dataset size: 74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_train_initial, y_train_initial, test_size=0.05, random_state=142)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_1, y_train_1, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "print('Final tet dataset size: {}'.format(len(X_test_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=180, width_shift_range = 0.05, height_shift_range = 0.05, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#X_train_new = np.copy(X_train)\n",
    "#Y_train_new = np.copy(y_train)\n",
    "\n",
    "#for i in range(40):\n",
    "#    X_batch, y_batch =  next(datagen.flow(X_train, y_train, batch_size=2000))\n",
    "#    if i == 0:\n",
    "#        X_train_new = X_batch\n",
    "#        Y_train_new = y_batch\n",
    "#    else: \n",
    "#        X_train_new = np.concatenate((X_train_new, X_batch), axis=0)\n",
    "#        Y_train_new = np.concatenate((Y_train_new, y_batch), axis=0)\n",
    "    \n",
    "    #plt.imshow(X_batch[24, :, :, 0])\n",
    "    #plt.show()\n",
    "                                    \n",
    "    \n",
    "#print(X_batch.shape)\n",
    "\n",
    "\n",
    "#print (X_batch.shape)\n",
    "##X_train = cut_image_part(X_train_new, 10)\n",
    "#print (X_train_new.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_initial, y_train_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final validation dataset size: {}'.format(len(X_valid)))\n",
    "\n",
    "features_train = []\n",
    "features_valid = []\n",
    "\n",
    "for x in X_train:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_train.append(flat)\n",
    "\n",
    "for x in X_valid:\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    features_valid.append(flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(random_state = 3)\n",
    "lrc = SVC(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(features_train, y_train)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(features_valid)\n",
    "predictions_train = lrc.predict(features_train)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_valid, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_valid, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(lrc.__class__.__name__, len(features_train)))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "entities_count = 20\n",
    "\n",
    "def getModel10(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model\n",
    "\n",
    "def getModel11(num_layers):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "  \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)  ))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "    \n",
    "def getModelBest(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "  \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def getModel20(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(2, 2), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(2, 2), strides=(1, 1), padding='same', use_bias=True))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def getTransferLearningModel():\n",
    "    activation = 'elu'\n",
    "    base_model = InceptionV3(include_top=False, weights='imagenet')\n",
    "    output = base_model.output\n",
    "    \n",
    "    layer = GlobalMaxPooling2D()(output)\n",
    "    layer = Dense(512, activation='relu', name='fc2')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(512, activation='relu', name='fc3')(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    predictions = Dense(1, activation='sigmoid')(layer)\n",
    "   \n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel30(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel32(num_layers):\n",
    "    activation = 'elu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    " \n",
    "\n",
    "    \n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel31(num_layers):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                    input_shape=(75, 75, num_layers), use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                    kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=True,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=activation,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def getModel40(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False,\n",
    "                     kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None), kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def getModel50(num_layers):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "                     input_shape=(75, 75, num_layers), use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                            kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', use_bias=False,\n",
    "                           kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\",\n",
    "                   kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 64)        1728      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 64)        36864     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 38, 38, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 19, 19, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 19, 19, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              26216448  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 31,951,297\n",
      "Trainable params: 31,951,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel11(3)\n",
    "#model = getTransferLearningModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 192\n",
    "epochs = 150\n",
    "\n",
    "def init_model(model, file_name):\n",
    "    checkpointer = ModelCheckpoint(filepath=file_name, verbose=1, save_best_only=True)\n",
    "    \n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "    #optimizer_small = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "    #optimizer2 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model, checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training dataset size: 1323\n",
      "Final test dataset size: 148\n",
      "Fold: 0\n",
      "Epoch 1/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6983 - acc: 0.5163Epoch 00000: val_loss improved from inf to 0.69296, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 7s - loss: 0.6980 - acc: 0.5108 - val_loss: 0.6930 - val_acc: 0.5180\n",
      "Epoch 2/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5301Epoch 00001: val_loss improved from 0.69296 to 0.69240, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6929 - acc: 0.5282 - val_loss: 0.6924 - val_acc: 0.5180\n",
      "Epoch 3/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6905 - acc: 0.5111Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6906 - acc: 0.5135 - val_loss: 0.6929 - val_acc: 0.5180\n",
      "Epoch 4/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6781 - acc: 0.5282Epoch 00003: val_loss improved from 0.69240 to 0.66815, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6769 - acc: 0.5284 - val_loss: 0.6681 - val_acc: 0.5811\n",
      "Epoch 5/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7030 - acc: 0.6033Epoch 00004: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7018 - acc: 0.5921 - val_loss: 0.6836 - val_acc: 0.6667\n",
      "Epoch 6/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6888 - acc: 0.5871Epoch 00005: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6882 - acc: 0.5825 - val_loss: 0.6832 - val_acc: 0.5180\n",
      "Epoch 7/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6762 - acc: 0.5978Epoch 00006: val_loss improved from 0.66815 to 0.65216, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6740 - acc: 0.6060 - val_loss: 0.6522 - val_acc: 0.7027\n",
      "Epoch 8/160\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.6107 - acc: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.406005). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.203502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.6358 - acc: 0.6873Epoch 00007: val_loss improved from 0.65216 to 0.61054, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6333 - acc: 0.6921 - val_loss: 0.6105 - val_acc: 0.7117\n",
      "Epoch 9/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5866 - acc: 0.6881Epoch 00008: val_loss improved from 0.61054 to 0.60586, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5868 - acc: 0.6868 - val_loss: 0.6059 - val_acc: 0.6351\n",
      "Epoch 10/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.7167Epoch 00009: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5891 - acc: 0.7068 - val_loss: 0.7635 - val_acc: 0.5225\n",
      "Epoch 11/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.5728Epoch 00010: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6644 - acc: 0.5906 - val_loss: 0.6149 - val_acc: 0.6892\n",
      "Epoch 12/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.6988Epoch 00011: val_loss improved from 0.60586 to 0.56645, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5935 - acc: 0.7050 - val_loss: 0.5664 - val_acc: 0.7027\n",
      "Epoch 13/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.7415Epoch 00012: val_loss improved from 0.56645 to 0.55739, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5565 - acc: 0.7336 - val_loss: 0.5574 - val_acc: 0.7117\n",
      "Epoch 14/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5427 - acc: 0.7431Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5484 - acc: 0.7364 - val_loss: 0.6077 - val_acc: 0.6622\n",
      "Epoch 15/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5612 - acc: 0.7235Epoch 00014: val_loss improved from 0.55739 to 0.54529, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5571 - acc: 0.7238 - val_loss: 0.5453 - val_acc: 0.7117\n",
      "Epoch 16/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.7360Epoch 00015: val_loss improved from 0.54529 to 0.53086, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5222 - acc: 0.7386 - val_loss: 0.5309 - val_acc: 0.7162\n",
      "Epoch 17/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.7358Epoch 00016: val_loss improved from 0.53086 to 0.50806, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5208 - acc: 0.7337 - val_loss: 0.5081 - val_acc: 0.7207\n",
      "Epoch 18/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7412Epoch 00017: val_loss improved from 0.50806 to 0.49567, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5169 - acc: 0.7357 - val_loss: 0.4957 - val_acc: 0.7297\n",
      "Epoch 19/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.7178Epoch 00018: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5412 - acc: 0.7142 - val_loss: 0.5209 - val_acc: 0.7297\n",
      "Epoch 20/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.7443Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5034 - acc: 0.7464 - val_loss: 0.5073 - val_acc: 0.7162\n",
      "Epoch 21/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.7259Epoch 00020: val_loss improved from 0.49567 to 0.49508, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5212 - acc: 0.7296 - val_loss: 0.4951 - val_acc: 0.7432\n",
      "Epoch 22/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.7430Epoch 00021: val_loss improved from 0.49508 to 0.49290, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4836 - acc: 0.7500 - val_loss: 0.4929 - val_acc: 0.8063\n",
      "Epoch 23/160\n",
      " 2/10 [====>.........................] - ETA: 3s - loss: 0.4248 - acc: 0.8062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.664002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.332501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.7717Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.4805 - acc: 0.7595 - val_loss: 0.5373 - val_acc: 0.7342\n",
      "Epoch 24/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.7594Epoch 00023: val_loss improved from 0.49290 to 0.46823, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4862 - acc: 0.7547 - val_loss: 0.4682 - val_acc: 0.7297\n",
      "Epoch 25/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4293 - acc: 0.7886Epoch 00024: val_loss improved from 0.46823 to 0.45432, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4356 - acc: 0.7827 - val_loss: 0.4543 - val_acc: 0.7387\n",
      "Epoch 26/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.7074Epoch 00025: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5488 - acc: 0.7064 - val_loss: 0.5411 - val_acc: 0.7252\n",
      "Epoch 27/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.7646Epoch 00026: val_loss improved from 0.45432 to 0.45329, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4803 - acc: 0.7624 - val_loss: 0.4533 - val_acc: 0.7658\n",
      "Epoch 28/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.7438Epoch 00027: val_loss improved from 0.45329 to 0.44342, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4853 - acc: 0.7413 - val_loss: 0.4434 - val_acc: 0.7703\n",
      "Epoch 29/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.7553Epoch 00028: val_loss improved from 0.44342 to 0.43493, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4656 - acc: 0.7483 - val_loss: 0.4349 - val_acc: 0.7658\n",
      "Epoch 30/160\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.4331 - acc: 0.8203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.650001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.325001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.7925Epoch 00029: val_loss improved from 0.43493 to 0.41411, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.4395 - acc: 0.7986 - val_loss: 0.4141 - val_acc: 0.7793\n",
      "Epoch 31/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3980 - acc: 0.7962Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4021 - acc: 0.7943 - val_loss: 0.4412 - val_acc: 0.8063\n",
      "Epoch 32/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8092Epoch 00031: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3944 - acc: 0.8089 - val_loss: 0.4791 - val_acc: 0.7568\n",
      "Epoch 33/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4594 - acc: 0.7623Epoch 00032: val_loss improved from 0.41411 to 0.40422, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4530 - acc: 0.7633 - val_loss: 0.4042 - val_acc: 0.8063\n",
      "Epoch 34/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8019Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4028 - acc: 0.8060 - val_loss: 0.4493 - val_acc: 0.7523\n",
      "Epoch 35/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.7922Epoch 00034: val_loss improved from 0.40422 to 0.36665, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4012 - acc: 0.7906 - val_loss: 0.3667 - val_acc: 0.8288\n",
      "Epoch 36/160\n",
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.3436 - acc: 0.8465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.286002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.265002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.244003). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8141Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.3727 - acc: 0.8212 - val_loss: 0.4798 - val_acc: 0.7613\n",
      "Epoch 37/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.7954Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4026 - acc: 0.7990 - val_loss: 0.3724 - val_acc: 0.8063\n",
      "Epoch 38/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8305Epoch 00037: val_loss improved from 0.36665 to 0.33282, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3567 - acc: 0.8267 - val_loss: 0.3328 - val_acc: 0.8378\n",
      "Epoch 39/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8349Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3442 - acc: 0.8351 - val_loss: 0.3764 - val_acc: 0.8649\n",
      "Epoch 40/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.7935Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4105 - acc: 0.7935 - val_loss: 0.4735 - val_acc: 0.7883\n",
      "Epoch 41/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8204Epoch 00040: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3710 - acc: 0.8234 - val_loss: 0.3554 - val_acc: 0.8108\n",
      "Epoch 42/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8376Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3380 - acc: 0.8368 - val_loss: 0.3455 - val_acc: 0.8649\n",
      "Epoch 43/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3376 - acc: 0.8474Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3397 - acc: 0.8471 - val_loss: 0.4030 - val_acc: 0.7928\n",
      "Epoch 44/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8329Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3423 - acc: 0.8359 - val_loss: 0.3636 - val_acc: 0.8108\n",
      "Epoch 45/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8360Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3348 - acc: 0.8381 - val_loss: 0.3365 - val_acc: 0.8559\n",
      "Epoch 46/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.8644Epoch 00045: val_loss improved from 0.33282 to 0.28622, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3013 - acc: 0.8596 - val_loss: 0.2862 - val_acc: 0.8919\n",
      "Epoch 47/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.8391Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3156 - acc: 0.8425 - val_loss: 0.3263 - val_acc: 0.8604\n",
      "Epoch 48/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.8698Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2791 - acc: 0.8703 - val_loss: 0.3727 - val_acc: 0.8333\n",
      "Epoch 49/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8475Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3534 - acc: 0.8405 - val_loss: 0.3384 - val_acc: 0.8649\n",
      "Epoch 50/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.8589Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2920 - acc: 0.8634 - val_loss: 0.3112 - val_acc: 0.8919\n",
      "Epoch 51/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.8662Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2994 - acc: 0.8625 - val_loss: 0.2917 - val_acc: 0.8829\n",
      "Epoch 52/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.8698Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2808 - acc: 0.8662 - val_loss: 0.2937 - val_acc: 0.8694\n",
      "Epoch 53/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.8628Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2860 - acc: 0.8625 - val_loss: 0.2987 - val_acc: 0.8559\n",
      "Epoch 54/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.8599Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2877 - acc: 0.8656 - val_loss: 0.3359 - val_acc: 0.8333\n",
      "Epoch 55/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.8818Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2500 - acc: 0.8854 - val_loss: 0.3607 - val_acc: 0.8378\n",
      "Epoch 56/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.8521Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3155 - acc: 0.8502 - val_loss: 0.3140 - val_acc: 0.8739\n",
      "Epoch 57/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.8782Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2579 - acc: 0.8800 - val_loss: 0.3472 - val_acc: 0.8198\n",
      "Epoch 58/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3192 - acc: 0.8435Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3124 - acc: 0.8476 - val_loss: 0.4273 - val_acc: 0.7973\n",
      "Epoch 59/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.8597Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3053 - acc: 0.8560 - val_loss: 0.3833 - val_acc: 0.8378\n",
      "Epoch 60/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8472Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3170 - acc: 0.8483 - val_loss: 0.3178 - val_acc: 0.8559\n",
      "Epoch 61/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.8534Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3069 - acc: 0.8539 - val_loss: 0.3156 - val_acc: 0.8739\n",
      "Epoch 62/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8810Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2532 - acc: 0.8827 - val_loss: 0.3641 - val_acc: 0.8333\n",
      "Epoch 63/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.8670Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2848 - acc: 0.8653 - val_loss: 0.3124 - val_acc: 0.8604\n",
      "Epoch 64/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.8946Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2415 - acc: 0.8948 - val_loss: 0.3254 - val_acc: 0.8604\n",
      "Epoch 65/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.8797Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2525 - acc: 0.8823 - val_loss: 0.2956 - val_acc: 0.8784\n",
      "Epoch 66/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.8802Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2577 - acc: 0.8848 - val_loss: 0.5032 - val_acc: 0.7883\n",
      "Epoch 67/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8558Epoch 00066: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3416 - acc: 0.8559 - val_loss: 0.3450 - val_acc: 0.8649\n",
      "Epoch 68/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.8777Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2729 - acc: 0.8767 - val_loss: 0.2890 - val_acc: 0.8694\n",
      "Epoch 69/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.8792Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2480 - acc: 0.8831 - val_loss: 0.3038 - val_acc: 0.8694\n",
      "Epoch 70/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.8966Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2174 - acc: 0.8975 - val_loss: 0.3143 - val_acc: 0.8604\n",
      "Epoch 71/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.8771Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2423 - acc: 0.8813 - val_loss: 0.3011 - val_acc: 0.8649\n",
      "Epoch 72/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.8998Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2362 - acc: 0.8973 - val_loss: 0.3446 - val_acc: 0.8288\n",
      "Epoch 73/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.8847Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2435 - acc: 0.8860 - val_loss: 0.3175 - val_acc: 0.8514\n",
      "Epoch 74/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.8935Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2247 - acc: 0.8961 - val_loss: 0.3116 - val_acc: 0.8784\n",
      "Epoch 75/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9044Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2111 - acc: 0.9002 - val_loss: 0.4041 - val_acc: 0.8468\n",
      "Epoch 76/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.8964Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2125 - acc: 0.8930 - val_loss: 0.3473 - val_acc: 0.8604\n",
      "Epoch 77/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.8912Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2210 - acc: 0.8929 - val_loss: 0.2909 - val_acc: 0.8964\n",
      "Epoch 78/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.8990Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2342 - acc: 0.8989 - val_loss: 0.3239 - val_acc: 0.8514\n",
      "Epoch 79/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.8717Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2616 - acc: 0.8727 - val_loss: 0.5121 - val_acc: 0.8198\n",
      "Epoch 80/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.8737Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2499 - acc: 0.8761 - val_loss: 0.3179 - val_acc: 0.8649\n",
      "Epoch 81/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9037Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2142 - acc: 0.9032 - val_loss: 0.3659 - val_acc: 0.8559\n",
      "Epoch 82/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9047Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2081 - acc: 0.9049 - val_loss: 0.3354 - val_acc: 0.8468\n",
      "Epoch 83/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9034Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2059 - acc: 0.9007 - val_loss: 0.3333 - val_acc: 0.8514\n",
      "Epoch 84/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9141Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1913 - acc: 0.9128 - val_loss: 0.3482 - val_acc: 0.8559\n",
      "Epoch 85/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.8956Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2344 - acc: 0.8938 - val_loss: 0.3348 - val_acc: 0.8604\n",
      "Epoch 86/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.8782Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2461 - acc: 0.8779 - val_loss: 0.3161 - val_acc: 0.8649\n",
      "Epoch 87/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.8857Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2309 - acc: 0.8881 - val_loss: 0.4088 - val_acc: 0.8468\n",
      "Epoch 88/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9044Epoch 00087: val_loss improved from 0.28622 to 0.28376, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2248 - acc: 0.9002 - val_loss: 0.2838 - val_acc: 0.8784\n",
      "Epoch 89/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9055Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2048 - acc: 0.9076 - val_loss: 0.3730 - val_acc: 0.8243\n",
      "Epoch 90/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9011Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2184 - acc: 0.8973 - val_loss: 0.3769 - val_acc: 0.8108\n",
      "Epoch 91/160\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.2191 - acc: 0.8984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.761002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.381001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.8712Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2567 - acc: 0.8725 - val_loss: 0.4503 - val_acc: 0.8288\n",
      "Epoch 92/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.8930Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2296 - acc: 0.8958 - val_loss: 0.3866 - val_acc: 0.8423\n",
      "Epoch 93/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.8938Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2172 - acc: 0.8951 - val_loss: 0.2866 - val_acc: 0.8829\n",
      "Epoch 94/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9185Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1924 - acc: 0.9159 - val_loss: 0.3609 - val_acc: 0.8514\n",
      "Epoch 95/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9180Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1871 - acc: 0.9185 - val_loss: 0.3048 - val_acc: 0.8874\n",
      "Epoch 96/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9172Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1720 - acc: 0.9161 - val_loss: 0.4576 - val_acc: 0.8468\n",
      "Epoch 97/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9128Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1952 - acc: 0.9072 - val_loss: 0.3936 - val_acc: 0.8288\n",
      "Epoch 98/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9138Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1814 - acc: 0.9146 - val_loss: 0.4652 - val_acc: 0.8333\n",
      "Epoch 99/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9128Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1897 - acc: 0.9125 - val_loss: 0.4510 - val_acc: 0.8514\n",
      "Epoch 100/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8831Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2498 - acc: 0.8816 - val_loss: 0.4711 - val_acc: 0.8243\n",
      "Epoch 101/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9013Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2154 - acc: 0.8991 - val_loss: 0.4381 - val_acc: 0.8108\n",
      "Epoch 102/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9203Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1829 - acc: 0.9199 - val_loss: 0.3868 - val_acc: 0.8784\n",
      "Epoch 103/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9167Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1805 - acc: 0.9185 - val_loss: 0.3826 - val_acc: 0.8559\n",
      "Epoch 104/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9092Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1758 - acc: 0.9103 - val_loss: 0.3735 - val_acc: 0.8694\n",
      "Epoch 105/160\n",
      " 3/10 [=======>......................] - ETA: 4s - loss: 0.1680 - acc: 0.9141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.507001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.645501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/10 [=============>................] - ETA: 2s - loss: 0.1761 - acc: 0.9156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.254001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9110Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1887 - acc: 0.9106 - val_loss: 0.5094 - val_acc: 0.8108\n",
      "Epoch 106/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9180Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1760 - acc: 0.9149 - val_loss: 0.5423 - val_acc: 0.8333\n",
      "Epoch 107/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9099Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2018 - acc: 0.9102 - val_loss: 0.4819 - val_acc: 0.8288\n",
      "Epoch 108/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9167Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1778 - acc: 0.9183 - val_loss: 0.4487 - val_acc: 0.8604\n",
      "Epoch 109/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9151Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1689 - acc: 0.9149 - val_loss: 0.3912 - val_acc: 0.8694\n",
      "Epoch 110/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9128Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2026 - acc: 0.9159 - val_loss: 0.4538 - val_acc: 0.8468\n",
      "Epoch 111/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9198Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1848 - acc: 0.9216 - val_loss: 0.4030 - val_acc: 0.8739\n",
      "Epoch 112/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9086Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1866 - acc: 0.9078 - val_loss: 0.3609 - val_acc: 0.8739\n",
      "Epoch 113/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9084Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1793 - acc: 0.9118 - val_loss: 0.5219 - val_acc: 0.8468\n",
      "Epoch 114/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9068Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2163 - acc: 0.9054 - val_loss: 0.3393 - val_acc: 0.8649\n",
      "Epoch 115/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9169Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1927 - acc: 0.9145 - val_loss: 0.4424 - val_acc: 0.8604\n",
      "Epoch 116/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9214Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1723 - acc: 0.9228 - val_loss: 0.3638 - val_acc: 0.8423\n",
      "Epoch 117/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9198Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1693 - acc: 0.9223 - val_loss: 0.3959 - val_acc: 0.8694\n",
      "Epoch 118/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9211Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1588 - acc: 0.9233 - val_loss: 0.4359 - val_acc: 0.8423\n",
      "Epoch 119/160\n",
      " 4/10 [==========>...................] - ETA: 3s - loss: 0.2016 - acc: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.247500). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.493999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.8886Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2488 - acc: 0.8840 - val_loss: 0.3680 - val_acc: 0.8514\n",
      "Epoch 120/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9039Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2318 - acc: 0.8998 - val_loss: 0.3173 - val_acc: 0.8964\n",
      "Epoch 121/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9234Epoch 00120: val_loss improved from 0.28376 to 0.27655, saving model to saved_models/fold0.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.1669 - acc: 0.9239 - val_loss: 0.2766 - val_acc: 0.9099\n",
      "Epoch 122/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9310Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1572 - acc: 0.9266 - val_loss: 0.4051 - val_acc: 0.8649\n",
      "Epoch 123/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9284Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1610 - acc: 0.9256 - val_loss: 0.3917 - val_acc: 0.8514\n",
      "Epoch 124/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9175Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1833 - acc: 0.9150 - val_loss: 0.3724 - val_acc: 0.8784\n",
      "Epoch 125/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1619 - acc: 0.9271Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1610 - acc: 0.9274 - val_loss: 0.3799 - val_acc: 0.8784\n",
      "Epoch 126/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9365Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1383 - acc: 0.9386 - val_loss: 0.4668 - val_acc: 0.8468\n",
      "Epoch 127/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9321Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1560 - acc: 0.9312 - val_loss: 0.4132 - val_acc: 0.8604\n",
      "Epoch 128/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9331Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1540 - acc: 0.9329 - val_loss: 0.3782 - val_acc: 0.9009\n",
      "Epoch 129/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9242Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1697 - acc: 0.9276 - val_loss: 0.6349 - val_acc: 0.8153\n",
      "Epoch 130/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9242Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1676 - acc: 0.9205 - val_loss: 0.4969 - val_acc: 0.8514\n",
      "Epoch 131/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9302Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1563 - acc: 0.9281 - val_loss: 0.4714 - val_acc: 0.8649\n",
      "Epoch 132/160\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.1087 - acc: 0.9570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.806002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.403501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9247Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1661 - acc: 0.9272 - val_loss: 0.3328 - val_acc: 0.8739\n",
      "Epoch 133/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9456Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1323 - acc: 0.9447 - val_loss: 0.4858 - val_acc: 0.8739\n",
      "Epoch 134/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.9401Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1332 - acc: 0.9406 - val_loss: 0.5157 - val_acc: 0.8604\n",
      "Epoch 135/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9073Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1834 - acc: 0.9099 - val_loss: 0.3948 - val_acc: 0.8829\n",
      "Epoch 136/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1477 - acc: 0.9367Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1468 - acc: 0.9376 - val_loss: 0.4975 - val_acc: 0.8378\n",
      "Epoch 137/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9393Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1339 - acc: 0.9371 - val_loss: 0.4042 - val_acc: 0.8604\n",
      "Epoch 138/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9539Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1253 - acc: 0.9510 - val_loss: 0.4160 - val_acc: 0.8649\n",
      "Epoch 139/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9279Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1468 - acc: 0.9297 - val_loss: 0.5627 - val_acc: 0.8649\n",
      "Epoch 140/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9368Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1452 - acc: 0.9333 - val_loss: 0.4287 - val_acc: 0.8694\n",
      "Epoch 141/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9422Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1252 - acc: 0.9431 - val_loss: 0.5758 - val_acc: 0.8378\n",
      "Epoch 142/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9490Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1265 - acc: 0.9480 - val_loss: 0.4855 - val_acc: 0.8468\n",
      "Epoch 143/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9573Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1038 - acc: 0.9540 - val_loss: 0.6146 - val_acc: 0.8514\n",
      "Epoch 144/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9419Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1174 - acc: 0.9448 - val_loss: 0.5944 - val_acc: 0.8784\n",
      "Epoch 145/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9451Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1260 - acc: 0.9430 - val_loss: 0.3985 - val_acc: 0.8694\n",
      "Epoch 146/160\n",
      " 2/10 [====>.........................] - ETA: 4s - loss: 0.1393 - acc: 0.9453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.842001). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.421501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9412Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1323 - acc: 0.9423 - val_loss: 0.6746 - val_acc: 0.8423\n",
      "Epoch 147/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9464Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1215 - acc: 0.9457 - val_loss: 0.5258 - val_acc: 0.8694\n",
      "Epoch 148/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9492Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1246 - acc: 0.9495 - val_loss: 0.5387 - val_acc: 0.8423\n",
      "Epoch 149/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9555Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1183 - acc: 0.9574 - val_loss: 0.5542 - val_acc: 0.8559\n",
      "Epoch 150/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9432Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1268 - acc: 0.9406 - val_loss: 0.5223 - val_acc: 0.8468\n",
      "Epoch 151/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9448Epoch 00150: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1319 - acc: 0.9427 - val_loss: 0.5530 - val_acc: 0.8964\n",
      "Epoch 152/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9521Epoch 00151: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1009 - acc: 0.9537 - val_loss: 0.5401 - val_acc: 0.8694\n",
      "Epoch 153/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9534Epoch 00152: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0958 - acc: 0.9551 - val_loss: 0.5690 - val_acc: 0.8604\n",
      "Epoch 154/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9552Epoch 00153: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1091 - acc: 0.9557 - val_loss: 0.5364 - val_acc: 0.8694\n",
      "Epoch 155/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1657 - acc: 0.9328Epoch 00154: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1628 - acc: 0.9332 - val_loss: 0.6183 - val_acc: 0.8333\n",
      "Epoch 156/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9375Epoch 00155: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1417 - acc: 0.9420 - val_loss: 0.5112 - val_acc: 0.8649\n",
      "Epoch 157/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9628Epoch 00156: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1003 - acc: 0.9612 - val_loss: 0.4726 - val_acc: 0.8604\n",
      "Epoch 158/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9547Epoch 00157: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1091 - acc: 0.9560 - val_loss: 0.4776 - val_acc: 0.8559\n",
      "Epoch 159/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9633Epoch 00158: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0798 - acc: 0.9631 - val_loss: 0.6199 - val_acc: 0.8874\n",
      "Epoch 160/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9586Epoch 00159: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0920 - acc: 0.9604 - val_loss: 0.5052 - val_acc: 0.8739\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.241461829156\n",
      "Test accuracy: 0.905405403794\n",
      "Fold: 1\n",
      "Epoch 1/160\n",
      " 3/10 [=======>......................] - ETA: 5s - loss: 0.7131 - acc: 0.4609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.534501). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.260000). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.4752Epoch 00000: val_loss improved from inf to 0.69277, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 6s - loss: 0.6991 - acc: 0.4795 - val_loss: 0.6928 - val_acc: 0.5204\n",
      "Epoch 2/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5216Epoch 00001: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6927 - acc: 0.5197 - val_loss: 0.6928 - val_acc: 0.5204\n",
      "Epoch 3/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5190Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6904 - acc: 0.5217 - val_loss: 0.7054 - val_acc: 0.5204\n",
      "Epoch 4/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.7068 - acc: 0.4986Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.7056 - acc: 0.5007 - val_loss: 0.6930 - val_acc: 0.5204\n",
      "Epoch 5/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5168Epoch 00004: val_loss improved from 0.69277 to 0.69262, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6929 - acc: 0.5145 - val_loss: 0.6926 - val_acc: 0.5204\n",
      "Epoch 6/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5180Epoch 00005: val_loss improved from 0.69262 to 0.69140, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6930 - acc: 0.5121 - val_loss: 0.6914 - val_acc: 0.5204\n",
      "Epoch 7/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6843 - acc: 0.5261Epoch 00006: val_loss improved from 0.69140 to 0.65549, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6815 - acc: 0.5297 - val_loss: 0.6555 - val_acc: 0.5249\n",
      "Epoch 8/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.6233Epoch 00007: val_loss improved from 0.65549 to 0.62568, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6647 - acc: 0.6222 - val_loss: 0.6257 - val_acc: 0.7149\n",
      "Epoch 9/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.6028Epoch 00008: val_loss improved from 0.62568 to 0.59849, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6378 - acc: 0.6142 - val_loss: 0.5985 - val_acc: 0.7104\n",
      "Epoch 10/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6395 - acc: 0.6393Epoch 00009: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6398 - acc: 0.6451 - val_loss: 0.6116 - val_acc: 0.6652\n",
      "Epoch 11/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.6873Epoch 00010: val_loss improved from 0.59849 to 0.58691, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5825 - acc: 0.6995 - val_loss: 0.5869 - val_acc: 0.6968\n",
      "Epoch 12/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.6664Epoch 00011: val_loss improved from 0.58691 to 0.54094, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6202 - acc: 0.6626 - val_loss: 0.5409 - val_acc: 0.7692\n",
      "Epoch 13/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5832 - acc: 0.7069Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5856 - acc: 0.7057 - val_loss: 0.5434 - val_acc: 0.7285\n",
      "Epoch 14/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.7052Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5839 - acc: 0.6982 - val_loss: 0.5742 - val_acc: 0.6878\n",
      "Epoch 15/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.6996Epoch 00014: val_loss improved from 0.54094 to 0.49491, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5697 - acc: 0.7034 - val_loss: 0.4949 - val_acc: 0.8054\n",
      "Epoch 16/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.7106Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5765 - acc: 0.7134 - val_loss: 0.5031 - val_acc: 0.8009\n",
      "Epoch 17/160\n",
      " 3/10 [=======>......................] - ETA: 3s - loss: 0.5636 - acc: 0.7109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.421001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.7161Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.5566 - acc: 0.7150 - val_loss: 0.5396 - val_acc: 0.7285\n",
      "Epoch 18/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5881 - acc: 0.6957Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5898 - acc: 0.6923 - val_loss: 0.5068 - val_acc: 0.7964\n",
      "Epoch 19/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.7238Epoch 00018: val_loss improved from 0.49491 to 0.48736, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5542 - acc: 0.7275 - val_loss: 0.4874 - val_acc: 0.7919\n",
      "Epoch 20/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.7184Epoch 00019: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5697 - acc: 0.7098 - val_loss: 0.5849 - val_acc: 0.7014\n",
      "Epoch 21/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5837 - acc: 0.7024Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5857 - acc: 0.6955 - val_loss: 0.5513 - val_acc: 0.7240\n",
      "Epoch 22/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5803 - acc: 0.7013Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5749 - acc: 0.7073 - val_loss: 0.4921 - val_acc: 0.7964\n",
      "Epoch 23/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.7251Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5531 - acc: 0.7226 - val_loss: 0.5574 - val_acc: 0.6968\n",
      "Epoch 24/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.7119Epoch 00023: val_loss improved from 0.48736 to 0.47311, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5591 - acc: 0.7160 - val_loss: 0.4731 - val_acc: 0.7873\n",
      "Epoch 25/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.7316Epoch 00024: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5277 - acc: 0.7360 - val_loss: 0.5138 - val_acc: 0.7376\n",
      "Epoch 26/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5632 - acc: 0.6911Epoch 00025: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5596 - acc: 0.6986 - val_loss: 0.5006 - val_acc: 0.7692\n",
      "Epoch 27/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7456Epoch 00026: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5155 - acc: 0.7462 - val_loss: 0.5016 - val_acc: 0.7059\n",
      "Epoch 28/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.7357Epoch 00027: val_loss improved from 0.47311 to 0.44301, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5010 - acc: 0.7408 - val_loss: 0.4430 - val_acc: 0.7828\n",
      "Epoch 29/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.7295Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5323 - acc: 0.7306 - val_loss: 0.4558 - val_acc: 0.7421\n",
      "Epoch 30/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.7032Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5464 - acc: 0.7026 - val_loss: 0.5189 - val_acc: 0.8145\n",
      "Epoch 31/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5680 - acc: 0.6898Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5781 - acc: 0.6775 - val_loss: 0.5264 - val_acc: 0.7014\n",
      "Epoch 32/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5244 - acc: 0.7249Epoch 00031: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5226 - acc: 0.7216 - val_loss: 0.4535 - val_acc: 0.7873\n",
      "Epoch 33/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.7374Epoch 00032: val_loss improved from 0.44301 to 0.41955, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4859 - acc: 0.7392 - val_loss: 0.4195 - val_acc: 0.7783\n",
      "Epoch 34/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.7212Epoch 00033: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4890 - acc: 0.7252 - val_loss: 0.4362 - val_acc: 0.8009\n",
      "Epoch 35/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.7471Epoch 00034: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5017 - acc: 0.7427 - val_loss: 0.4532 - val_acc: 0.7964\n",
      "Epoch 36/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.7312Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4778 - acc: 0.7302 - val_loss: 0.4523 - val_acc: 0.7376\n",
      "Epoch 37/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4791 - acc: 0.7635Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4751 - acc: 0.7660 - val_loss: 0.4230 - val_acc: 0.7557\n",
      "Epoch 38/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7811Epoch 00037: val_loss improved from 0.41955 to 0.39597, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4393 - acc: 0.7819 - val_loss: 0.3960 - val_acc: 0.7828\n",
      "Epoch 39/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8060Epoch 00038: val_loss improved from 0.39597 to 0.37884, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4394 - acc: 0.8036 - val_loss: 0.3788 - val_acc: 0.8235\n",
      "Epoch 40/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5516 - acc: 0.7020Epoch 00039: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5573 - acc: 0.6970 - val_loss: 0.6426 - val_acc: 0.6109\n",
      "Epoch 41/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.7284Epoch 00040: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5365 - acc: 0.7306 - val_loss: 0.4334 - val_acc: 0.8100\n",
      "Epoch 42/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4574 - acc: 0.7602Epoch 00041: val_loss improved from 0.37884 to 0.36892, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4515 - acc: 0.7656 - val_loss: 0.3689 - val_acc: 0.7873\n",
      "Epoch 43/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.7943Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4219 - acc: 0.7944 - val_loss: 0.3736 - val_acc: 0.8145\n",
      "Epoch 44/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.7703Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4647 - acc: 0.7733 - val_loss: 0.4569 - val_acc: 0.7557\n",
      "Epoch 45/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.7770Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4376 - acc: 0.7753 - val_loss: 0.3783 - val_acc: 0.8371\n",
      "Epoch 46/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4038 - acc: 0.8162Epoch 00045: val_loss improved from 0.36892 to 0.34889, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4037 - acc: 0.8166 - val_loss: 0.3489 - val_acc: 0.8371\n",
      "Epoch 47/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8057Epoch 00046: val_loss improved from 0.34889 to 0.32003, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3836 - acc: 0.8091 - val_loss: 0.3200 - val_acc: 0.8507\n",
      "Epoch 48/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.7854Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4263 - acc: 0.7859 - val_loss: 0.4662 - val_acc: 0.7376\n",
      "Epoch 49/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.7306Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4949 - acc: 0.7330 - val_loss: 0.5441 - val_acc: 0.6833\n",
      "Epoch 50/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.7836Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4508 - acc: 0.7839 - val_loss: 0.3961 - val_acc: 0.8145\n",
      "Epoch 51/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8203Epoch 00050: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3763 - acc: 0.8223 - val_loss: 0.3436 - val_acc: 0.8416\n",
      "Epoch 52/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8300Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3655 - acc: 0.8277 - val_loss: 0.3918 - val_acc: 0.8281\n",
      "Epoch 53/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8071Epoch 00052: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4177 - acc: 0.7949 - val_loss: 0.3898 - val_acc: 0.8145\n",
      "Epoch 54/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.7917Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4172 - acc: 0.7897 - val_loss: 0.3345 - val_acc: 0.8416\n",
      "Epoch 55/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8372Epoch 00054: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3527 - acc: 0.8412 - val_loss: 0.3328 - val_acc: 0.8371\n",
      "Epoch 56/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8424Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3429 - acc: 0.8432 - val_loss: 0.3469 - val_acc: 0.8507\n",
      "Epoch 57/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8438Epoch 00056: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3280 - acc: 0.8467 - val_loss: 0.3561 - val_acc: 0.8190\n",
      "Epoch 58/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8304Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3515 - acc: 0.8301 - val_loss: 0.3643 - val_acc: 0.8235\n",
      "Epoch 59/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.8410Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3589 - acc: 0.8411 - val_loss: 0.4130 - val_acc: 0.8054\n",
      "Epoch 60/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8546Epoch 00059: val_loss improved from 0.32003 to 0.30660, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3259 - acc: 0.8558 - val_loss: 0.3066 - val_acc: 0.8462\n",
      "Epoch 61/160\n",
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.3167 - acc: 0.8672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.363007). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.182005). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8521Epoch 00060: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.3219 - acc: 0.8528 - val_loss: 0.3662 - val_acc: 0.8281\n",
      "Epoch 62/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8526Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3192 - acc: 0.8491 - val_loss: 0.3416 - val_acc: 0.8552\n",
      "Epoch 63/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.8559Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3108 - acc: 0.8584 - val_loss: 0.3235 - val_acc: 0.8371\n",
      "Epoch 64/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8585Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3147 - acc: 0.8556 - val_loss: 0.3390 - val_acc: 0.8507\n",
      "Epoch 65/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.8611Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2949 - acc: 0.8637 - val_loss: 0.3275 - val_acc: 0.8371\n",
      "Epoch 66/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.8599Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3182 - acc: 0.8571 - val_loss: 0.3075 - val_acc: 0.8507\n",
      "Epoch 67/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8323Epoch 00066: val_loss improved from 0.30660 to 0.30243, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3599 - acc: 0.8347 - val_loss: 0.3024 - val_acc: 0.8507\n",
      "Epoch 68/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2986 - acc: 0.8704Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2955 - acc: 0.8714 - val_loss: 0.3913 - val_acc: 0.8416\n",
      "Epoch 69/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8607Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3010 - acc: 0.8542 - val_loss: 0.4151 - val_acc: 0.8100\n",
      "Epoch 70/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.8669Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2850 - acc: 0.8640 - val_loss: 0.3368 - val_acc: 0.8552\n",
      "Epoch 71/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.8585Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3040 - acc: 0.8563 - val_loss: 0.3415 - val_acc: 0.8371\n",
      "Epoch 72/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8627Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2828 - acc: 0.8681 - val_loss: 0.3847 - val_acc: 0.8190\n",
      "Epoch 73/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.8678Epoch 00072: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2776 - acc: 0.8740 - val_loss: 0.3239 - val_acc: 0.8869\n",
      "Epoch 74/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.8607Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2870 - acc: 0.8642 - val_loss: 0.4122 - val_acc: 0.8190\n",
      "Epoch 75/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.8620Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2801 - acc: 0.8641 - val_loss: 0.4206 - val_acc: 0.8507\n",
      "Epoch 76/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8666Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3241 - acc: 0.8616 - val_loss: 0.3786 - val_acc: 0.8552\n",
      "Epoch 77/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.8559Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3068 - acc: 0.8585 - val_loss: 0.3515 - val_acc: 0.8552\n",
      "Epoch 78/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.8785Epoch 00077: val_loss improved from 0.30243 to 0.29700, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2859 - acc: 0.8767 - val_loss: 0.2970 - val_acc: 0.8824\n",
      "Epoch 79/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.8862Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2697 - acc: 0.8846 - val_loss: 0.3309 - val_acc: 0.8643\n",
      "Epoch 80/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.8755Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2680 - acc: 0.8804 - val_loss: 0.3600 - val_acc: 0.8235\n",
      "Epoch 81/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.8836Epoch 00080: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2666 - acc: 0.8769 - val_loss: 0.4045 - val_acc: 0.8281\n",
      "Epoch 82/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.8659Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2778 - acc: 0.8653 - val_loss: 0.3431 - val_acc: 0.8552\n",
      "Epoch 83/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.8788Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2694 - acc: 0.8770 - val_loss: 0.3334 - val_acc: 0.8643\n",
      "Epoch 84/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.8839Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2630 - acc: 0.8805 - val_loss: 0.3270 - val_acc: 0.8597\n",
      "Epoch 85/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.8809Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2733 - acc: 0.8769 - val_loss: 0.3585 - val_acc: 0.8371\n",
      "Epoch 86/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.8831Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2569 - acc: 0.8863 - val_loss: 0.3713 - val_acc: 0.8145\n",
      "Epoch 87/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.8587Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2855 - acc: 0.8554 - val_loss: 0.3670 - val_acc: 0.8688\n",
      "Epoch 88/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.8900Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2574 - acc: 0.8837 - val_loss: 0.3682 - val_acc: 0.8552\n",
      "Epoch 89/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.8831Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2540 - acc: 0.8817 - val_loss: 0.3439 - val_acc: 0.8778\n",
      "Epoch 90/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.8842Epoch 00089: val_loss improved from 0.29700 to 0.28390, saving model to saved_models/fold1.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2513 - acc: 0.8830 - val_loss: 0.2839 - val_acc: 0.8824\n",
      "Epoch 91/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.8806Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2402 - acc: 0.8830 - val_loss: 0.3667 - val_acc: 0.8507\n",
      "Epoch 92/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.8893Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2423 - acc: 0.8917 - val_loss: 0.3061 - val_acc: 0.8733\n",
      "Epoch 93/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.8872Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2380 - acc: 0.8875 - val_loss: 0.2976 - val_acc: 0.8824\n",
      "Epoch 94/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9007Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2418 - acc: 0.9005 - val_loss: 0.3396 - val_acc: 0.8824\n",
      "Epoch 95/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.8915Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2474 - acc: 0.8930 - val_loss: 0.2938 - val_acc: 0.8778\n",
      "Epoch 96/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.8929Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2334 - acc: 0.8921 - val_loss: 0.3568 - val_acc: 0.8733\n",
      "Epoch 97/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.8817Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2781 - acc: 0.8753 - val_loss: 0.3537 - val_acc: 0.8688\n",
      "Epoch 98/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2526 - acc: 0.8874Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2525 - acc: 0.8891 - val_loss: 0.3800 - val_acc: 0.8688\n",
      "Epoch 99/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.8831Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2520 - acc: 0.8856 - val_loss: 0.3600 - val_acc: 0.8145\n",
      "Epoch 100/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.8799Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2696 - acc: 0.8808 - val_loss: 0.3909 - val_acc: 0.8416\n",
      "Epoch 101/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.8667Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2635 - acc: 0.8684 - val_loss: 0.3937 - val_acc: 0.8281\n",
      "Epoch 102/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.8955Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2302 - acc: 0.8965 - val_loss: 0.3318 - val_acc: 0.8552\n",
      "Epoch 103/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.8851Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2431 - acc: 0.8834 - val_loss: 0.3160 - val_acc: 0.8507\n",
      "Epoch 104/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.8782Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2647 - acc: 0.8814 - val_loss: 0.3865 - val_acc: 0.8552\n",
      "Epoch 105/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.8929Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2299 - acc: 0.8942 - val_loss: 0.3169 - val_acc: 0.8914\n",
      "Epoch 106/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.8827Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2549 - acc: 0.8848 - val_loss: 0.3991 - val_acc: 0.8688\n",
      "Epoch 107/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.8971Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2318 - acc: 0.8979 - val_loss: 0.4663 - val_acc: 0.8643\n",
      "Epoch 108/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.8866Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2305 - acc: 0.8921 - val_loss: 0.3008 - val_acc: 0.8733\n",
      "Epoch 109/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.8812Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2427 - acc: 0.8850 - val_loss: 0.3989 - val_acc: 0.8235\n",
      "Epoch 110/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8492Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3083 - acc: 0.8501 - val_loss: 0.4260 - val_acc: 0.8281\n",
      "Epoch 111/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.8846Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2446 - acc: 0.8894 - val_loss: 0.3825 - val_acc: 0.8688\n",
      "Epoch 112/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.8913Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2219 - acc: 0.8948 - val_loss: 0.3709 - val_acc: 0.8824\n",
      "Epoch 113/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9110Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 4s - loss: 0.1965 - acc: 0.9129 - val_loss: 0.4004 - val_acc: 0.8824\n",
      "Epoch 114/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9083Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2107 - acc: 0.9061 - val_loss: 0.3727 - val_acc: 0.8597\n",
      "Epoch 115/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9087Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2340 - acc: 0.9020 - val_loss: 0.3873 - val_acc: 0.8824\n",
      "Epoch 116/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.8718Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2549 - acc: 0.8764 - val_loss: 0.3283 - val_acc: 0.8959\n",
      "Epoch 117/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9072Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2256 - acc: 0.9064 - val_loss: 0.3414 - val_acc: 0.8824\n",
      "Epoch 118/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9072Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2137 - acc: 0.9093 - val_loss: 0.3741 - val_acc: 0.8326\n",
      "Epoch 119/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.8986Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2296 - acc: 0.8999 - val_loss: 0.3060 - val_acc: 0.8824\n",
      "Epoch 120/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9103Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1975 - acc: 0.9084 - val_loss: 0.3223 - val_acc: 0.8733\n",
      "Epoch 121/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9077Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2042 - acc: 0.9105 - val_loss: 0.4656 - val_acc: 0.8507\n",
      "Epoch 122/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9036Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2125 - acc: 0.9067 - val_loss: 0.3194 - val_acc: 0.8778\n",
      "Epoch 123/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.8978Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2064 - acc: 0.8985 - val_loss: 0.4878 - val_acc: 0.8416\n",
      "Epoch 124/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.8947Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2287 - acc: 0.8944 - val_loss: 0.4163 - val_acc: 0.8462\n",
      "Epoch 125/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9059Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2044 - acc: 0.9030 - val_loss: 0.3134 - val_acc: 0.8959\n",
      "Epoch 126/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9217Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1803 - acc: 0.9216 - val_loss: 0.3525 - val_acc: 0.8914\n",
      "Epoch 127/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9139Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1777 - acc: 0.9110 - val_loss: 0.4261 - val_acc: 0.8643\n",
      "Epoch 128/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1872 - acc: 0.9186Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1844 - acc: 0.9176 - val_loss: 0.3909 - val_acc: 0.8462\n",
      "Epoch 129/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9046Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2469 - acc: 0.9019 - val_loss: 0.3730 - val_acc: 0.8778\n",
      "Epoch 130/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9160Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1941 - acc: 0.9180 - val_loss: 0.3811 - val_acc: 0.8552\n",
      "Epoch 131/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.8962Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2309 - acc: 0.8950 - val_loss: 0.3518 - val_acc: 0.8733\n",
      "Epoch 132/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.8927Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2238 - acc: 0.8884 - val_loss: 0.3511 - val_acc: 0.8869\n",
      "Epoch 133/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9133Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2014 - acc: 0.9133 - val_loss: 0.3628 - val_acc: 0.8869\n",
      "Epoch 134/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9210Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1819 - acc: 0.9190 - val_loss: 0.4383 - val_acc: 0.8643\n",
      "Epoch 135/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9215Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1762 - acc: 0.9251 - val_loss: 0.4045 - val_acc: 0.8643\n",
      "Epoch 136/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9265Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1908 - acc: 0.9219 - val_loss: 0.4185 - val_acc: 0.8688\n",
      "Epoch 137/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9129Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1951 - acc: 0.9159 - val_loss: 0.5512 - val_acc: 0.8507\n",
      "Epoch 138/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9297Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1687 - acc: 0.9282 - val_loss: 0.3838 - val_acc: 0.8733\n",
      "Epoch 139/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9140Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1860 - acc: 0.9162 - val_loss: 0.4130 - val_acc: 0.8643\n",
      "Epoch 140/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9240Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1893 - acc: 0.9210 - val_loss: 0.3890 - val_acc: 0.8688\n",
      "Epoch 141/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9140Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1845 - acc: 0.9162 - val_loss: 0.6190 - val_acc: 0.8326\n",
      "Epoch 142/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.8857Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2968 - acc: 0.8843 - val_loss: 0.4745 - val_acc: 0.7828\n",
      "Epoch 143/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.8859Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2395 - acc: 0.8857 - val_loss: 0.3296 - val_acc: 0.8824\n",
      "Epoch 144/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9043Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1978 - acc: 0.9071 - val_loss: 0.4773 - val_acc: 0.8281\n",
      "Epoch 145/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9077Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2066 - acc: 0.9097 - val_loss: 0.3499 - val_acc: 0.8643\n",
      "Epoch 146/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9088Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1867 - acc: 0.9052 - val_loss: 0.3612 - val_acc: 0.8824\n",
      "Epoch 147/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9106Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1905 - acc: 0.9137 - val_loss: 0.3307 - val_acc: 0.8778\n",
      "Epoch 148/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1804 - acc: 0.9069Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1770 - acc: 0.9105 - val_loss: 0.4250 - val_acc: 0.8733\n",
      "Epoch 149/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9301Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1616 - acc: 0.9265 - val_loss: 0.4043 - val_acc: 0.8643\n",
      "Epoch 150/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9308Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1692 - acc: 0.9314 - val_loss: 0.4595 - val_acc: 0.8643\n",
      "Epoch 151/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9205Epoch 00150: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1784 - acc: 0.9165 - val_loss: 0.4297 - val_acc: 0.8688\n",
      "Epoch 152/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9311Epoch 00151: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1576 - acc: 0.9325 - val_loss: 0.5783 - val_acc: 0.8416\n",
      "Epoch 153/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.8884Epoch 00152: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2374 - acc: 0.8891 - val_loss: 0.4025 - val_acc: 0.8597\n",
      "Epoch 154/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9065Epoch 00153: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2026 - acc: 0.9081 - val_loss: 0.3695 - val_acc: 0.8643\n",
      "Epoch 155/160\n",
      " 3/10 [=======>......................] - ETA: 3s - loss: 0.1549 - acc: 0.9453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.724002). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.464502). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.205002). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9277Epoch 00154: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1667 - acc: 0.9307 - val_loss: 0.4042 - val_acc: 0.8914\n",
      "Epoch 156/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9296Epoch 00155: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1533 - acc: 0.9304 - val_loss: 0.3970 - val_acc: 0.8824\n",
      "Epoch 157/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9169Epoch 00156: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1801 - acc: 0.9175 - val_loss: 0.3233 - val_acc: 0.8959\n",
      "Epoch 158/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9275Epoch 00157: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1648 - acc: 0.9305 - val_loss: 0.4527 - val_acc: 0.8824\n",
      "Epoch 159/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9189Epoch 00158: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1759 - acc: 0.9206 - val_loss: 0.4346 - val_acc: 0.8778\n",
      "Epoch 160/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9303Epoch 00159: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1614 - acc: 0.9302 - val_loss: 0.4018 - val_acc: 0.8824\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.283425152705\n",
      "Test accuracy: 0.878378375157\n",
      "Fold: 2\n",
      "Epoch 1/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6989 - acc: 0.4739Epoch 00000: val_loss improved from inf to 0.69324, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 7s - loss: 0.6984 - acc: 0.4741 - val_loss: 0.6932 - val_acc: 0.4818\n",
      "Epoch 2/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6938 - acc: 0.5216Epoch 00001: val_loss improved from 0.69324 to 0.68709, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6922 - acc: 0.5239 - val_loss: 0.6871 - val_acc: 0.5182\n",
      "Epoch 3/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4751Epoch 00002: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6931 - acc: 0.4816 - val_loss: 0.6929 - val_acc: 0.5182\n",
      "Epoch 4/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6961 - acc: 0.5136Epoch 00003: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6957 - acc: 0.5173 - val_loss: 0.6926 - val_acc: 0.5182\n",
      "Epoch 5/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5154Epoch 00004: val_loss improved from 0.68709 to 0.68500, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6918 - acc: 0.5185 - val_loss: 0.6850 - val_acc: 0.5182\n",
      "Epoch 6/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6808 - acc: 0.5211Epoch 00005: val_loss improved from 0.68500 to 0.66213, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6776 - acc: 0.5249 - val_loss: 0.6621 - val_acc: 0.7455\n",
      "Epoch 7/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6547 - acc: 0.6716Epoch 00006: val_loss improved from 0.66213 to 0.60028, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6501 - acc: 0.6756 - val_loss: 0.6003 - val_acc: 0.6773\n",
      "Epoch 8/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.6716Epoch 00007: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.6386 - acc: 0.6721 - val_loss: 0.6347 - val_acc: 0.6955\n",
      "Epoch 9/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.6126 - acc: 0.7042Epoch 00008: val_loss improved from 0.60028 to 0.54942, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.6092 - acc: 0.7068 - val_loss: 0.5494 - val_acc: 0.7364\n",
      "Epoch 10/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.7043Epoch 00009: val_loss improved from 0.54942 to 0.54326, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.5801 - acc: 0.7050 - val_loss: 0.5433 - val_acc: 0.7318\n",
      "Epoch 11/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.6974Epoch 00010: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5760 - acc: 0.6995 - val_loss: 0.6553 - val_acc: 0.6727\n",
      "Epoch 12/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.7323Epoch 00011: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5698 - acc: 0.7354 - val_loss: 0.5442 - val_acc: 0.7364\n",
      "Epoch 13/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.7290Epoch 00012: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5476 - acc: 0.7328 - val_loss: 0.5449 - val_acc: 0.7318\n",
      "Epoch 14/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.7195Epoch 00013: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5506 - acc: 0.7250 - val_loss: 0.5596 - val_acc: 0.7091\n",
      "Epoch 15/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.7262Epoch 00014: val_loss improved from 0.54326 to 0.52193, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5475 - acc: 0.7255 - val_loss: 0.5219 - val_acc: 0.7455\n",
      "Epoch 16/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.7203Epoch 00015: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5563 - acc: 0.7165 - val_loss: 0.5315 - val_acc: 0.7364\n",
      "Epoch 17/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.7106Epoch 00016: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5598 - acc: 0.7162 - val_loss: 0.5343 - val_acc: 0.7455\n",
      "Epoch 18/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.7295Epoch 00017: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5452 - acc: 0.7218 - val_loss: 0.5234 - val_acc: 0.7500\n",
      "Epoch 19/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.7369Epoch 00018: val_loss improved from 0.52193 to 0.51278, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.5216 - acc: 0.7398 - val_loss: 0.5128 - val_acc: 0.7455\n",
      "Epoch 20/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.7336Epoch 00019: val_loss improved from 0.51278 to 0.49101, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 6s - loss: 0.5036 - acc: 0.7329 - val_loss: 0.4910 - val_acc: 0.7409\n",
      "Epoch 21/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.7231Epoch 00020: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5138 - acc: 0.7266 - val_loss: 0.5054 - val_acc: 0.7318\n",
      "Epoch 22/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4818 - acc: 0.7375Epoch 00021: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4773 - acc: 0.7408 - val_loss: 0.4985 - val_acc: 0.7318\n",
      "Epoch 23/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.7085Epoch 00022: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5295 - acc: 0.7166 - val_loss: 0.5613 - val_acc: 0.7136\n",
      "Epoch 24/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.7189Epoch 00023: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.5119 - acc: 0.7245 - val_loss: 0.4975 - val_acc: 0.7273\n",
      "Epoch 25/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.7559Epoch 00024: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4722 - acc: 0.7570 - val_loss: 0.5263 - val_acc: 0.6955\n",
      "Epoch 26/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.7751Epoch 00025: val_loss improved from 0.49101 to 0.47198, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4710 - acc: 0.7770 - val_loss: 0.4720 - val_acc: 0.7455\n",
      "Epoch 27/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.7859Epoch 00026: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4657 - acc: 0.7804 - val_loss: 0.4760 - val_acc: 0.7455\n",
      "Epoch 28/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.7929Epoch 00027: val_loss improved from 0.47198 to 0.44977, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4554 - acc: 0.7946 - val_loss: 0.4498 - val_acc: 0.7545\n",
      "Epoch 29/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.7970Epoch 00028: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.4350 - acc: 0.7940 - val_loss: 0.5164 - val_acc: 0.7227\n",
      "Epoch 30/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.7972Epoch 00029: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4288 - acc: 0.7979 - val_loss: 0.4784 - val_acc: 0.7500\n",
      "Epoch 31/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.7745Epoch 00030: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4439 - acc: 0.7742 - val_loss: 0.4681 - val_acc: 0.7545\n",
      "Epoch 32/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.7958Epoch 00031: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4224 - acc: 0.7989 - val_loss: 0.4530 - val_acc: 0.7864\n",
      "Epoch 33/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.7914Epoch 00032: val_loss improved from 0.44977 to 0.41421, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4299 - acc: 0.7871 - val_loss: 0.4142 - val_acc: 0.7909\n",
      "Epoch 34/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4138 - acc: 0.8103Epoch 00033: val_loss improved from 0.41421 to 0.40199, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.4104 - acc: 0.8114 - val_loss: 0.4020 - val_acc: 0.8000\n",
      "Epoch 35/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8176Epoch 00034: val_loss improved from 0.40199 to 0.40048, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3690 - acc: 0.8185 - val_loss: 0.4005 - val_acc: 0.8000\n",
      "Epoch 36/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8174Epoch 00035: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3998 - acc: 0.8056 - val_loss: 0.4952 - val_acc: 0.7409\n",
      "Epoch 37/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.7905Epoch 00036: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4185 - acc: 0.7954 - val_loss: 0.4070 - val_acc: 0.8045\n",
      "Epoch 38/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8275Epoch 00037: val_loss improved from 0.40048 to 0.39253, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3685 - acc: 0.8296 - val_loss: 0.3925 - val_acc: 0.8091\n",
      "Epoch 39/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3648 - acc: 0.8197Epoch 00038: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3603 - acc: 0.8232 - val_loss: 0.3929 - val_acc: 0.8091\n",
      "Epoch 40/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8189Epoch 00039: val_loss improved from 0.39253 to 0.39205, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3595 - acc: 0.8274 - val_loss: 0.3920 - val_acc: 0.7955\n",
      "Epoch 41/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3416 - acc: 0.8363Epoch 00040: val_loss improved from 0.39205 to 0.36841, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3353 - acc: 0.8405 - val_loss: 0.3684 - val_acc: 0.8273\n",
      "Epoch 42/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8407Epoch 00041: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3438 - acc: 0.8454 - val_loss: 0.3807 - val_acc: 0.8136\n",
      "Epoch 43/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8268Epoch 00042: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3713 - acc: 0.8248 - val_loss: 0.4097 - val_acc: 0.8136\n",
      "Epoch 44/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.7989Epoch 00043: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.4020 - acc: 0.7985 - val_loss: 0.4892 - val_acc: 0.7455\n",
      "Epoch 45/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8373Epoch 00044: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3537 - acc: 0.8417 - val_loss: 0.4383 - val_acc: 0.7864\n",
      "Epoch 46/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.8462Epoch 00045: val_loss improved from 0.36841 to 0.35273, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3320 - acc: 0.8460 - val_loss: 0.3527 - val_acc: 0.8273\n",
      "Epoch 47/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2936 - acc: 0.8653Epoch 00046: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.3038 - acc: 0.8598 - val_loss: 0.3831 - val_acc: 0.8136\n",
      "Epoch 48/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3575 - acc: 0.8311Epoch 00047: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.3508 - acc: 0.8335 - val_loss: 0.4071 - val_acc: 0.7773\n",
      "Epoch 49/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8349Epoch 00048: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3549 - acc: 0.8348 - val_loss: 0.3770 - val_acc: 0.8273\n",
      "Epoch 50/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8316Epoch 00049: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3650 - acc: 0.8249 - val_loss: 0.3529 - val_acc: 0.8364\n",
      "Epoch 51/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8611Epoch 00050: val_loss improved from 0.35273 to 0.33898, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3091 - acc: 0.8623 - val_loss: 0.3390 - val_acc: 0.8409\n",
      "Epoch 52/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8597Epoch 00051: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3086 - acc: 0.8617 - val_loss: 0.3671 - val_acc: 0.8364\n",
      "Epoch 53/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.8516Epoch 00052: val_loss improved from 0.33898 to 0.32879, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.3143 - acc: 0.8546 - val_loss: 0.3288 - val_acc: 0.8364\n",
      "Epoch 54/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2897 - acc: 0.8672Epoch 00053: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2867 - acc: 0.8665 - val_loss: 0.3322 - val_acc: 0.8364\n",
      "Epoch 55/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.8641Epoch 00054: val_loss improved from 0.32879 to 0.31736, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [===============================] - 4s - loss: 0.3019 - acc: 0.8636 - val_loss: 0.3174 - val_acc: 0.8455\n",
      "Epoch 56/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.8551Epoch 00055: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2932 - acc: 0.8570 - val_loss: 0.3546 - val_acc: 0.8136\n",
      "Epoch 57/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8358Epoch 00056: val_loss improved from 0.31736 to 0.31635, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.3314 - acc: 0.8346 - val_loss: 0.3164 - val_acc: 0.8409\n",
      "Epoch 58/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.8715Epoch 00057: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2769 - acc: 0.8712 - val_loss: 0.3215 - val_acc: 0.8500\n",
      "Epoch 59/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.8623Epoch 00058: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2827 - acc: 0.8624 - val_loss: 0.3198 - val_acc: 0.8500\n",
      "Epoch 60/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.8628Epoch 00059: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3000 - acc: 0.8610 - val_loss: 0.4479 - val_acc: 0.8000\n",
      "Epoch 61/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.8571Epoch 00060: val_loss improved from 0.31635 to 0.29570, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2973 - acc: 0.8579 - val_loss: 0.2957 - val_acc: 0.8455\n",
      "Epoch 62/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.8536Epoch 00061: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2946 - acc: 0.8541 - val_loss: 0.3546 - val_acc: 0.8318\n",
      "Epoch 63/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.8525Epoch 00062: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.3166 - acc: 0.8442 - val_loss: 0.2974 - val_acc: 0.8409\n",
      "Epoch 64/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8570Epoch 00063: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2957 - acc: 0.8530 - val_loss: 0.3349 - val_acc: 0.8364\n",
      "Epoch 65/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.8626Epoch 00064: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2798 - acc: 0.8638 - val_loss: 0.3508 - val_acc: 0.8500\n",
      "Epoch 66/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.8668Epoch 00065: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2923 - acc: 0.8669 - val_loss: 0.3744 - val_acc: 0.7955\n",
      "Epoch 67/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.8601Epoch 00066: val_loss improved from 0.29570 to 0.28730, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2890 - acc: 0.8573 - val_loss: 0.2873 - val_acc: 0.8455\n",
      "Epoch 68/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.8817Epoch 00067: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2545 - acc: 0.8833 - val_loss: 0.2883 - val_acc: 0.8591\n",
      "Epoch 69/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.8867Epoch 00068: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2456 - acc: 0.8828 - val_loss: 0.3051 - val_acc: 0.8455\n",
      "Epoch 70/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.8904Epoch 00069: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2380 - acc: 0.8875 - val_loss: 0.3052 - val_acc: 0.8591\n",
      "Epoch 71/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.8845Epoch 00070: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2483 - acc: 0.8891 - val_loss: 0.3031 - val_acc: 0.8682\n",
      "Epoch 72/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.8832Epoch 00071: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2470 - acc: 0.8780 - val_loss: 0.3257 - val_acc: 0.8545\n",
      "Epoch 73/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.8763Epoch 00072: val_loss improved from 0.28730 to 0.28405, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2627 - acc: 0.8739 - val_loss: 0.2841 - val_acc: 0.8682\n",
      "Epoch 74/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.8848Epoch 00073: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2432 - acc: 0.8869 - val_loss: 0.2997 - val_acc: 0.8773\n",
      "Epoch 75/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.8842Epoch 00074: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2392 - acc: 0.8834 - val_loss: 0.3938 - val_acc: 0.8545\n",
      "Epoch 76/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.8781Epoch 00075: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2475 - acc: 0.8793 - val_loss: 0.2891 - val_acc: 0.8636\n",
      "Epoch 77/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.8727Epoch 00076: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2627 - acc: 0.8717 - val_loss: 0.3803 - val_acc: 0.8500\n",
      "Epoch 78/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.8705Epoch 00077: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2727 - acc: 0.8659 - val_loss: 0.2859 - val_acc: 0.8682\n",
      "Epoch 79/160\n",
      " 3/10 [=======>......................] - ETA: 2s - loss: 0.2695 - acc: 0.8464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.325501). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.2729 - acc: 0.8657Epoch 00078: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2721 - acc: 0.8664 - val_loss: 0.3581 - val_acc: 0.8591\n",
      "Epoch 80/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.8800Epoch 00079: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2488 - acc: 0.8803 - val_loss: 0.2965 - val_acc: 0.8682\n",
      "Epoch 81/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.8927Epoch 00080: val_loss improved from 0.28405 to 0.26089, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 4s - loss: 0.2371 - acc: 0.8909 - val_loss: 0.2609 - val_acc: 0.8636\n",
      "Epoch 82/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.8888Epoch 00081: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2235 - acc: 0.8882 - val_loss: 0.2722 - val_acc: 0.8682\n",
      "Epoch 83/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9053Epoch 00082: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2119 - acc: 0.9055 - val_loss: 0.2963 - val_acc: 0.8500\n",
      "Epoch 84/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2890 - acc: 0.8709Epoch 00083: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2862 - acc: 0.8684 - val_loss: 0.3177 - val_acc: 0.8545\n",
      "Epoch 85/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2669 - acc: 0.8741Epoch 00084: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2654 - acc: 0.8756 - val_loss: 0.2987 - val_acc: 0.8455\n",
      "Epoch 86/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.8956Epoch 00085: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2225 - acc: 0.8957 - val_loss: 0.2780 - val_acc: 0.8682\n",
      "Epoch 87/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.8988Epoch 00086: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2134 - acc: 0.9015 - val_loss: 0.2776 - val_acc: 0.8682\n",
      "Epoch 88/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9095Epoch 00087: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1929 - acc: 0.9090 - val_loss: 0.3209 - val_acc: 0.8636\n",
      "Epoch 89/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.8888Epoch 00088: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2325 - acc: 0.8882 - val_loss: 0.2996 - val_acc: 0.8636\n",
      "Epoch 90/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9006Epoch 00089: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1928 - acc: 0.9028 - val_loss: 0.3213 - val_acc: 0.8636\n",
      "Epoch 91/160\n",
      " 4/10 [==========>...................] - ETA: 3s - loss: 0.1822 - acc: 0.9102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.255499). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.509999). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9053Epoch 00090: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.2000 - acc: 0.9019 - val_loss: 0.2931 - val_acc: 0.8682\n",
      "Epoch 92/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9032Epoch 00091: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1919 - acc: 0.9042 - val_loss: 0.2756 - val_acc: 0.8773\n",
      "Epoch 93/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.8877Epoch 00092: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2199 - acc: 0.8914 - val_loss: 0.3457 - val_acc: 0.8773\n",
      "Epoch 94/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.8979Epoch 00093: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2161 - acc: 0.8957 - val_loss: 0.3311 - val_acc: 0.8591\n",
      "Epoch 95/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.8984Epoch 00094: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.2183 - acc: 0.8993 - val_loss: 0.3745 - val_acc: 0.8500\n",
      "Epoch 96/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.8825Epoch 00095: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2597 - acc: 0.8826 - val_loss: 0.3560 - val_acc: 0.8136\n",
      "Epoch 97/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.8931Epoch 00096: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2387 - acc: 0.8923 - val_loss: 0.3178 - val_acc: 0.8455\n",
      "Epoch 98/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.8736Epoch 00097: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2524 - acc: 0.8802 - val_loss: 0.2675 - val_acc: 0.8591\n",
      "Epoch 99/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.8981Epoch 00098: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2222 - acc: 0.8968 - val_loss: 0.2660 - val_acc: 0.8773\n",
      "Epoch 100/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9042Epoch 00099: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1978 - acc: 0.9038 - val_loss: 0.3217 - val_acc: 0.8636\n",
      "Epoch 101/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9202Epoch 00100: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1761 - acc: 0.9175 - val_loss: 0.3387 - val_acc: 0.8591\n",
      "Epoch 102/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1752 - acc: 0.9117Epoch 00101: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1739 - acc: 0.9127 - val_loss: 0.3007 - val_acc: 0.8773\n",
      "Epoch 103/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.8910Epoch 00102: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2268 - acc: 0.8867 - val_loss: 0.3041 - val_acc: 0.8636\n",
      "Epoch 104/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9154Epoch 00103: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1837 - acc: 0.9136 - val_loss: 0.3199 - val_acc: 0.8818\n",
      "Epoch 105/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9030Epoch 00104: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1992 - acc: 0.9061 - val_loss: 0.3020 - val_acc: 0.8682\n",
      "Epoch 106/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9024Epoch 00105: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1949 - acc: 0.9077 - val_loss: 0.2859 - val_acc: 0.8773\n",
      "Epoch 107/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9039Epoch 00106: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1944 - acc: 0.9099 - val_loss: 0.3315 - val_acc: 0.8591\n",
      "Epoch 108/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9122Epoch 00107: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1873 - acc: 0.9099 - val_loss: 0.3004 - val_acc: 0.8591\n",
      "Epoch 109/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9155Epoch 00108: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1749 - acc: 0.9146 - val_loss: 0.3067 - val_acc: 0.8727\n",
      "Epoch 110/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9154Epoch 00109: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1761 - acc: 0.9195 - val_loss: 0.2768 - val_acc: 0.8636\n",
      "Epoch 111/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9187Epoch 00110: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1831 - acc: 0.9146 - val_loss: 0.2755 - val_acc: 0.8682\n",
      "Epoch 112/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9132Epoch 00111: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1946 - acc: 0.9132 - val_loss: 0.4634 - val_acc: 0.8409\n",
      "Epoch 113/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.8994Epoch 00112: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2059 - acc: 0.9009 - val_loss: 0.2979 - val_acc: 0.8636\n",
      "Epoch 114/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9330Epoch 00113: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1608 - acc: 0.9299 - val_loss: 0.2645 - val_acc: 0.8773\n",
      "Epoch 115/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9081Epoch 00114: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1770 - acc: 0.9116 - val_loss: 0.2842 - val_acc: 0.8727\n",
      "Epoch 116/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9206Epoch 00115: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1677 - acc: 0.9223 - val_loss: 0.2994 - val_acc: 0.8773\n",
      "Epoch 117/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9307Epoch 00116: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1676 - acc: 0.9268 - val_loss: 0.2784 - val_acc: 0.8727\n",
      "Epoch 118/160\n",
      " 4/10 [==========>...................] - ETA: 3s - loss: 0.1620 - acc: 0.9355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.188000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.376001). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9320Epoch 00117: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1516 - acc: 0.9324 - val_loss: 0.3215 - val_acc: 0.8727\n",
      "Epoch 119/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9305Epoch 00118: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1516 - acc: 0.9291 - val_loss: 0.3170 - val_acc: 0.8636\n",
      "Epoch 120/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9342Epoch 00119: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1450 - acc: 0.9344 - val_loss: 0.3421 - val_acc: 0.8727\n",
      "Epoch 121/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9268Epoch 00120: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1515 - acc: 0.9256 - val_loss: 0.3174 - val_acc: 0.8682\n",
      "Epoch 122/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9192Epoch 00121: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1634 - acc: 0.9187 - val_loss: 0.3633 - val_acc: 0.8864\n",
      "Epoch 123/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9183Epoch 00122: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1795 - acc: 0.9166 - val_loss: 0.3219 - val_acc: 0.8682\n",
      "Epoch 124/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9112Epoch 00123: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1955 - acc: 0.9100 - val_loss: 0.3189 - val_acc: 0.8591\n",
      "Epoch 125/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9251Epoch 00124: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1633 - acc: 0.9283 - val_loss: 0.4301 - val_acc: 0.8591\n",
      "Epoch 126/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9216Epoch 00125: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1694 - acc: 0.9207 - val_loss: 0.3225 - val_acc: 0.8636\n",
      "Epoch 127/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9313Epoch 00126: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1627 - acc: 0.9284 - val_loss: 0.3249 - val_acc: 0.8864\n",
      "Epoch 128/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9151Epoch 00127: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1936 - acc: 0.9107 - val_loss: 0.3479 - val_acc: 0.8591\n",
      "Epoch 129/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9223Epoch 00128: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1798 - acc: 0.9271 - val_loss: 0.3095 - val_acc: 0.8864\n",
      "Epoch 130/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9226Epoch 00129: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1697 - acc: 0.9232 - val_loss: 0.3328 - val_acc: 0.8682\n",
      "Epoch 131/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9360Epoch 00130: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1575 - acc: 0.9367 - val_loss: 0.3098 - val_acc: 0.8636\n",
      "Epoch 132/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9327Epoch 00131: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1560 - acc: 0.9303 - val_loss: 0.3313 - val_acc: 0.8636\n",
      "Epoch 133/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9376Epoch 00132: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1406 - acc: 0.9368 - val_loss: 0.3328 - val_acc: 0.8818\n",
      "Epoch 134/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9351Epoch 00133: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1396 - acc: 0.9367 - val_loss: 0.3238 - val_acc: 0.8818\n",
      "Epoch 135/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9235Epoch 00134: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1703 - acc: 0.9202 - val_loss: 0.3176 - val_acc: 0.8818\n",
      "Epoch 136/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1615 - acc: 0.9257Epoch 00135: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1624 - acc: 0.9239 - val_loss: 0.3288 - val_acc: 0.8727\n",
      "Epoch 137/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9313Epoch 00136: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1385 - acc: 0.9319 - val_loss: 0.2938 - val_acc: 0.8909\n",
      "Epoch 138/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9465Epoch 00137: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1154 - acc: 0.9492 - val_loss: 0.3582 - val_acc: 0.8545\n",
      "Epoch 139/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9558Epoch 00138: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1110 - acc: 0.9554 - val_loss: 0.3351 - val_acc: 0.8773\n",
      "Epoch 140/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9498Epoch 00139: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1103 - acc: 0.9502 - val_loss: 0.2900 - val_acc: 0.8955\n",
      "Epoch 141/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9313Epoch 00140: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1393 - acc: 0.9319 - val_loss: 0.2877 - val_acc: 0.8909\n",
      "Epoch 142/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9210Epoch 00141: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1827 - acc: 0.9225 - val_loss: 0.4749 - val_acc: 0.8545\n",
      "Epoch 143/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9154Epoch 00142: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1923 - acc: 0.9153 - val_loss: 0.3502 - val_acc: 0.8682\n",
      "Epoch 144/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.8864Epoch 00143: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.2358 - acc: 0.8909 - val_loss: 0.5464 - val_acc: 0.7864\n",
      "Epoch 145/160\n",
      " 4/10 [==========>...................] - ETA: 2s - loss: 0.3274 - acc: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.185000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.368998). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.8649Epoch 00144: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.3057 - acc: 0.8643 - val_loss: 0.2845 - val_acc: 0.8818\n",
      "Epoch 146/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9218Epoch 00145: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1782 - acc: 0.9246 - val_loss: 0.2711 - val_acc: 0.8909\n",
      "Epoch 147/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9327Epoch 00146: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1536 - acc: 0.9317 - val_loss: 0.2901 - val_acc: 0.9000\n",
      "Epoch 148/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9434Epoch 00147: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1247 - acc: 0.9421 - val_loss: 0.3310 - val_acc: 0.8682\n",
      "Epoch 149/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9441Epoch 00148: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1197 - acc: 0.9463 - val_loss: 0.3422 - val_acc: 0.8591\n",
      "Epoch 150/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9504Epoch 00149: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1006 - acc: 0.9499 - val_loss: 0.3643 - val_acc: 0.8909\n",
      "Epoch 151/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9482Epoch 00150: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1147 - acc: 0.9486 - val_loss: 0.2724 - val_acc: 0.8955\n",
      "Epoch 152/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9585Epoch 00151: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.0998 - acc: 0.9574 - val_loss: 0.3499 - val_acc: 0.9091\n",
      "Epoch 153/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9257Epoch 00152: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1560 - acc: 0.9300 - val_loss: 0.3376 - val_acc: 0.8591\n",
      "Epoch 154/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9327Epoch 00153: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1321 - acc: 0.9367 - val_loss: 0.3303 - val_acc: 0.9000\n",
      "Epoch 155/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9447Epoch 00154: val_loss improved from 0.26089 to 0.26044, saving model to saved_models/fold2.weights.best.from_scratch.hdf5\n",
      "11/10 [===============================] - 5s - loss: 0.1334 - acc: 0.9456 - val_loss: 0.2604 - val_acc: 0.9000\n",
      "Epoch 156/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9517Epoch 00155: val_loss did not improve\n",
      "11/10 [===============================] - 2s - loss: 0.1124 - acc: 0.9503 - val_loss: 0.3012 - val_acc: 0.9136\n",
      "Epoch 157/160\n",
      " 3/10 [=======>......................] - ETA: 3s - loss: 0.1545 - acc: 0.9336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.189000). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.504502). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9418Epoch 00156: val_loss did not improve\n",
      "11/10 [===============================] - 3s - loss: 0.1242 - acc: 0.9442 - val_loss: 0.2688 - val_acc: 0.8818\n",
      "Epoch 158/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9536Epoch 00157: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1036 - acc: 0.9564 - val_loss: 0.4865 - val_acc: 0.8364\n",
      "Epoch 159/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9433Epoch 00158: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1370 - acc: 0.9408 - val_loss: 0.4154 - val_acc: 0.8818\n",
      "Epoch 160/160\n",
      "10/10 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9475Epoch 00159: val_loss did not improve\n",
      "11/10 [===============================] - 1s - loss: 0.1089 - acc: 0.9493 - val_loss: 0.3111 - val_acc: 0.8955\n",
      "128/148 [========================>.....] - ETA: 0sTest loss: 0.329148532168\n",
      "Test accuracy: 0.851351348129\n",
      "Fold: 3\n",
      "Epoch 1/160\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,128,38,38]\n\t [[Node: conv2d_100/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](activation_99/Relu, conv2d_100/kernel/read)]]\n\t [[Node: mul_1366/_3379 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1749_mul_1366\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_100/convolution', defined at:\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-c3ce7f0d2779>\", line 32, in <module>\n    model, checkpointer = init_model(getModel11(3), file_name)\n  File \"<ipython-input-30-38453e75c55d>\", line 47, in getModel11\n    kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 466, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3095, in conv2d\n    data_format='NHWC')\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 639, in convolution\n    op=op)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 308, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 631, in op\n    name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,128,38,38]\n\t [[Node: conv2d_100/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](activation_99/Relu, conv2d_100/kernel/read)]]\n\t [[Node: mul_1366/_3379 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1749_mul_1366\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,128,38,38]\n\t [[Node: conv2d_100/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](activation_99/Relu, conv2d_100/kernel/read)]]\n\t [[Node: mul_1366/_3379 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1749_mul_1366\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-c3ce7f0d2779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,128,38,38]\n\t [[Node: conv2d_100/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](activation_99/Relu, conv2d_100/kernel/read)]]\n\t [[Node: mul_1366/_3379 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1749_mul_1366\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'conv2d_100/convolution', defined at:\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-c3ce7f0d2779>\", line 32, in <module>\n    model, checkpointer = init_model(getModel11(3), file_name)\n  File \"<ipython-input-30-38453e75c55d>\", line 47, in getModel11\n    kernel_initializer = RandomNormal(mean=0.0, stddev=0.02, seed=None) ))\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 466, in add\n    output_tensor = layer(self.outputs[0])\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3095, in conv2d\n    data_format='NHWC')\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 639, in convolution\n    op=op)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 308, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 631, in op\n    name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 129, in _non_atrous_convolution\n    name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 396, in conv2d\n    data_format=data_format, name=name)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\ProgramData\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,128,38,38]\n\t [[Node: conv2d_100/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](activation_99/Relu, conv2d_100/kernel/read)]]\n\t [[Node: mul_1366/_3379 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1749_mul_1366\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "#X_train_initial2 = X_train_initial[:,:,:, 1:3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_initial, y_train_initial, test_size=0.1, random_state=598)\n",
    "\n",
    "print('Final training dataset size: {}'.format(len(X_train)))\n",
    "print('Final test dataset size: {}'.format(len(X_test)))\n",
    "\n",
    "K = 6\n",
    "folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=98).split(X_train, y_train))\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=0, width_shift_range = 0, height_shift_range = 0, \n",
    "                             horizontal_flip = True, vertical_flip = True)\n",
    "\n",
    "#res = model.fit(train_tensors, train_targets, \n",
    "#          validation_data = (X_valid, y_valid),\n",
    "#          epochs=epochs, batch_size=64, callbacks=[checkpointer], verbose=1)\n",
    "mean_loss = 0\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "    print(\"Fold: {}\".format(i))\n",
    "    file_name = 'saved_models/fold{}.weights.best.from_scratch.hdf5'.format(i)\n",
    "\n",
    "    model, checkpointer = init_model(getModel11(3), file_name)\n",
    "    \n",
    "    X_train_ = X_train[train_idx]\n",
    "    y_train_ = y_train[train_idx]\n",
    "    X_valid_ = X_train[valid_idx]\n",
    "    y_valid_ =  y_train[valid_idx]\n",
    "\n",
    "    datagen.fit(X_train_)\n",
    "\n",
    "    res = model.fit_generator(datagen.flow(X_train_, y_train_, batch_size=batch_size),  steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid_, y_valid_), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    \n",
    "    prediction_model = load_model(file_name )\n",
    "    \n",
    "    test_score = prediction_model.evaluate(X_test, y_test)\n",
    " \n",
    "    print('Test loss:', test_score[0])\n",
    "    print('Test accuracy:', test_score[1])\n",
    "    \n",
    "    mean_loss += test_score[0] / K\n",
    "        \n",
    "#res = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "#                    steps_per_epoch=len(X_train) / batch_size, epochs=epochs, validation_data = (X_valid, y_valid), verbose=1, callbacks=[checkpointer])\n",
    "\n",
    "print('Mean loss:',   mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graphs = [res]\n",
    "\n",
    "fig = plt.figure(figsize = (15, 15))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, hist in enumerate(graphs):\n",
    "    ax1 = fig.add_subplot(110 + i + 1)\n",
    "    #plt.setp([ax1], xticks=[], yticks=[])\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_ylabel('loss')\n",
    "    #ax1.set_xlabel('epoch')\n",
    "    ax1.plot(hist.history['loss'])\n",
    "    ax1.plot(hist.history['val_loss'])\n",
    "    ax1.xaxis.set_ticks(np.arange(0, epochs, epochs // 10))\n",
    "    ax1.yaxis.set_ticks(np.arange(0, 1, 0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/test.json'\n",
    "test_data = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test, y_test, ids = prepare_dataset_ignore_angle(test_data, global_min, global_max)\n",
    "X_test, y_test, ids = prepare_dataset_with_angles(test_data, ptocessing_lambda, ptocessing_angle_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "folder = 'saved_models/last/' \n",
    "\n",
    "n = 7\n",
    "\n",
    "prediction_models = []\n",
    "\n",
    "for i in range(n):\n",
    "    prediction_model = load_model(folder + 'fold' + str(i) + '.weights.best.from_scratch.hdf5')\n",
    "    prediction_models.append(prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train__, X_test__, y_train__, y_test__ = train_test_split(X_train_initial, y_train_initial, test_size=0.1, random_state=147)\n",
    "\n",
    "res = []\n",
    "resv = []\n",
    "\n",
    "for model in prediction_models:\n",
    "    res.append(model.predict(X_train__))\n",
    "    resv.append(model.predict(X_test__))\n",
    "\n",
    "#res1 = prediction_model_0.predict(X_train__)\n",
    "#res2 = prediction_model_1.predict(X_train__)\n",
    "#res3 = prediction_model_2.predict(X_train__)\n",
    "\n",
    "\n",
    "#res1v = prediction_model_0.predict(X_test__)\n",
    "#res2v = prediction_model_1.predict(X_test__)\n",
    "#res3v = prediction_model_2.predict(X_test__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.40550570e-05]\n"
     ]
    }
   ],
   "source": [
    "resa = np.array(res)\n",
    "resav = np.array(resv)\n",
    "print(resa[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1323, 1)\n",
      "0.154932051846\n",
      "0.156666048837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "\n",
    "#res_f = np.concatenate([res1, res2], axis = 1)\n",
    "#res_f_v = np.concatenate([res1v, res2v], axis = 1)\\\n",
    "\n",
    "res_f_mean = np.mean(resa, axis=0)\n",
    "res_f_v_mean = np.mean(resav, axis=0)\n",
    "print(res_f_mean.shape)\n",
    "\n",
    "print(log_loss(y_train__, res_f_mean))\n",
    "print(log_loss(y_test__, res_f_v_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_f_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bb824310639e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_f_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'quicksort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_f_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'quicksort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_f_v' is not defined"
     ]
    }
   ],
   "source": [
    "d = np.sort(res_f_v, axis=1, kind='quicksort', order=None)\n",
    "d1 = np.sort(res_f_v, axis=1, kind='quicksort', order=None)\n",
    "\n",
    "rr = []\n",
    "for item in d1:\n",
    "    if np.mean(item) > 0.5:\n",
    "        rr.append(np.max(item))\n",
    "    else:\n",
    "        rr.append(np.min(item))\n",
    "\n",
    "print(y_test__.shape)\n",
    "d = np.concatenate([d, np.expand_dims(y_test__, axis=1)], axis = 1)\n",
    "\n",
    "#d = d[:,3:4]\n",
    "print(log_loss(y_test__, rr))\n",
    "\n",
    "for i in range(132):\n",
    "    print(i, d[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f170fe1748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nANMkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f170fe1780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_channel, hv_channel = get_image_channels(X_test__, 6)\n",
    "display_image(hh_channel)\n",
    "display_image(hv_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lrc = AdaBoostClassifier(random_state = 3)\n",
    "#lrc = SVC(random_state = 3)\n",
    "\n",
    "results = {}\n",
    "    \n",
    "start = time() # Get start time\n",
    "lrc.fit(res_f, y_train__)\n",
    "end = time() # Get end time\n",
    "results['train_time'] = end - start\n",
    "\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = lrc.predict(res_f_v)\n",
    "predictions_train = lrc.predict(res_f)\n",
    "end = time() # Get end time\n",
    "    \n",
    "results['pred_time'] = end - start\n",
    "results['acc_train'] = accuracy_score(y_train__, predictions_train)\n",
    "results['log_loss_train'] = log_loss(y_train__, predictions_train)\n",
    "results['acc_test'] = accuracy_score(y_test__, predictions_test)\n",
    "results['log_loss_test'] = log_loss(y_test__, predictions_test)\n",
    "       \n",
    "print (\"{} trained on {} samples.\".format(lrc.__class__.__name__, len(res_f)))\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 8424, 1)\n"
     ]
    }
   ],
   "source": [
    "rest = []\n",
    "\n",
    "for model in prediction_models:\n",
    "    rest.append(model.predict(X_test))\n",
    "    \n",
    "res_t = np.array(rest)\n",
    "\n",
    "print(res_t.shape)\n",
    "res = np.mean(res_t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 1)\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "headers = ['id', 'is_iceberg']\n",
    "\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(headers)\n",
    "\n",
    "    for prediction, item_id in zip(res, ids):\n",
    "        csv_writer.writerow([item_id, np.asscalar(prediction)])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
